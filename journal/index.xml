<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Theo Pomies</title>
<link>https://theopomies.com/journal/</link>
<atom:link href="https://theopomies.com/journal/index.xml" rel="self" type="application/rss+xml"/>
<description>Daily journal, accountability management and learning tracking.</description>
<generator>quarto-1.8.25</generator>
<lastBuildDate>Wed, 08 Oct 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Blogs and videos</title>
  <link>https://theopomies.com/journal/2025-10-08.html</link>
  <description><![CDATA[ 





<p>I’m feeling both very hyped for Kivala and very down regarding my lack of time for ML these last weeks…</p>
<p>This entry is gonna be short, I’m trying to get the ball back rolling for writing daily, these moments kinda suck because I want to reflect, I want to build a habit of working on this side quest and document it, but being CTO/lead at a startup with an innovative product but still in a quite saturated market (intercom/digicodes) requires 110% of my time and attention and I’ve been trying to be hitting the gym 5/7 and sleep min 9h/nights so that’s tough.</p>
<p>I spend a lot of time in trains, moving throughout france seeing family, friends, going to the Kivala HQ etc., internet quality sucks in the train so I use this time to read mostly</p>
<section id="kivala" class="level3">
<h3 class="anchored" data-anchor-id="kivala">Kivala</h3>
<p>Wrote some scripts, got a new hardware prototype, ran the app on it, fixed some hardware issues, updated our current hardware with a release allowing Light themeing for the bright days + QoL fixes.</p>
</section>
<section id="ml" class="level3">
<h3 class="anchored" data-anchor-id="ml">ML</h3>
<p>Read some blogs articles and essays, 2 from John Schulman (KL Divergence estimator and Research career advice), 1 from thinking machines on their Tinker framework, downladed and skimmed some papers (tiny MoEs, diffusion papers, world models and RL). Started reading a book about information theory, I’ll write more about it once I understand enough to.</p>
<p>Overall I’m still proud of myself and I feel like I’ve got my priorities straight, my current startup is my income AND capital upside could be huge, and my health is primordial, I still feel in peak form, shape and energy.</p>


</section>

 ]]></description>
  <guid>https://theopomies.com/journal/2025-10-08.html</guid>
  <pubDate>Wed, 08 Oct 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>NOT ML, building my first Yocto image from scratch on MacOS</title>
  <link>https://theopomies.com/journal/2025-10-02.html</link>
  <description><![CDATA[ 





<p>The timeline for the release of our new product is narrowing, so I still don’t have a lot of time for ML. I still take some time to read daily papers, blog posts and X/Twitter threads, but I can’t afford to code for ML right now.</p>
<section id="yocto-linux" class="level3">
<h3 class="anchored" data-anchor-id="yocto-linux">Yocto Linux</h3>
<p>The Yocto Project is an embedded linux distribution linux distribution maintained by the Linux Fundation with OpenEmbedded. It allows to create a custom distribution with every lib and dependency you want and no mas. It’s meant to be build on GNU/Linux but with a few shenanigans it runs fine on MacOS.</p>
<p>I learned a lot, fast, with the help of my good friend gpt-5, about metas, recipes, BSPs, bitbake and such, and now after a few tweaks I have managed to run and flash our brand new image on the prototype, and run one V2 app, so all is well and going according to plan.</p>
<p>I think learning about distributed training will help a little in managed a cluster of embedded linux devices all around the world, we’ll see!</p>
<p>I still hope to have more free time soon to get back to the ML grind, but the startup grind is still thrilling.</p>


</section>

 ]]></description>
  <category>kivala</category>
  <category>embedded</category>
  <guid>https://theopomies.com/journal/2025-10-02.html</guid>
  <pubDate>Thu, 02 Oct 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Maximum Likelihood and LoRA on Connectionism</title>
  <link>https://theopomies.com/journal/2025-09-30.html</link>
  <description><![CDATA[ 





<p>Big days for Kivala, our V2 product’s development is coming along very well and I’ve been hard working on the software side to make this release a success and expand Kivala Worldwide. It’s taking a lot of my time, so today I read about ML in the train to Paris</p>
<section id="maximum-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="maximum-likelihood">Maximum Likelihood</h3>
<p>I wrote about Maximum Likelihood and negative log likelihood today in my <a href="../notes/probabilities.qmd">probabilities</a> cheatsheet. Basically we want to maximize the likelihood of our model parameters based on a set of examples. For numerical stability and to transform this objective in a minimizing problem, we can use the negative log likelihood.</p>
</section>
<section id="lora-on-connectionism" class="level3">
<h3 class="anchored" data-anchor-id="lora-on-connectionism">LoRA on Connectionism</h3>
<p>I read <a href="https://thinkingmachines.ai/blog/lora/" target="_blank">Thinking Machine’s new Connectionism article</a> by <a href="https://x.com/johnschulman2" target="_blank">John Schulman</a>. They discuss sort of scaling laws, rank choices and learning rates for LoRAs. Still important to note that <a href="https://x.com/Tim_Dettmers" target="_blank">Tim Dettmers</a> did similar experiments and document them 2 years ago.</p>


</section>

 ]]></description>
  <category>mle</category>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-09-30.html</guid>
  <pubDate>Tue, 30 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Catching up on classes, DiLoCo, Decentralized Training and Expert Parallelism</title>
  <link>https://theopomies.com/journal/2025-09-25.html</link>
  <description><![CDATA[ 





<p>The last 2 days have been really busy, so I couldn’t even attend to the 3 classes that took place then. I caught up all of them this saturday, but it’s fine, I was already familiar with the topics</p>
<section id="diloco" class="level3">
<h3 class="anchored" data-anchor-id="diloco">DiLoCo</h3>
<p>The first guest lecture was DiLoCo by Zach Charles from Google. DiLoCo is a distributed, internet-scale training strategy, very similar to federated training. Basically, on local nodes you implement regular DDP / other paralellism. Then, after H timesteps, you exchange parameters, and perform an outer-gradient step. The idea is that naively averaging parameters degrades performance, but trying to find the overall gradient over the H timesteps, and combining them, allows to find the distributed overall gradient, which is more pertinent. Overlapping some of the training examples allows to introduce a larger overlap of distributions (and gradients).</p>
<p>To further optimize this decentralized techine, researchers introduced Streaming DiLoCo, a variant, once again, overlapping computations and communications</p>
</section>
<section id="decentralized-training" class="level3">
<h3 class="anchored" data-anchor-id="decentralized-training">Decentralized Training</h3>
<p>Then, <a href="https://x.com/samsja19" target="_blank">Sami Jaghouar</a> from <a href="https://www.primeintellect.ai" target="_blank">Prime Intellect</a> discussed Decentralized Training overall, with DiLoCo, OpenDiLoCo (their version) and Intellect-1, their 10B model trained across 3 continents (!!!). He also discussed applying such decentralized patterns to RL, which is even easier, with one cluster/node performing inference, another the reward model, and yet another the training models. He insisted on the importance of fault tolerance, as the risk of having GPU failures grows with the number of GPU, and becomes daily occurences with frontier scale clusters.</p>
</section>
<section id="expert-parallelism" class="level3">
<h3 class="anchored" data-anchor-id="expert-parallelism">Expert Parallelism</h3>
<p>Finaly, <a href="https://x.com/m_sirovatka" target="_blank">Matej Sirovatka</a> from GPU MODE explained in more details the math behind MoE and Expert Parellism, a form of tensor parallelism where the experts of a MoE are scattered across ranks.</p>


</section>

 ]]></description>
  <category>mle</category>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-09-25.html</guid>
  <pubDate>Sat, 27 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Async Tensor Parallelism with Less Wright + Efficient Strategies for Distributed Inference with Marc Sun</title>
  <link>https://theopomies.com/journal/2025-09-24.html</link>
  <description><![CDATA[ 





<section id="async-tp" class="level3">
<h3 class="anchored" data-anchor-id="async-tp">Async TP</h3>
<p>Yesterday we learned a bit more about TP, and today thanks to <a href="https://x.com/lessw2020" target="_blank">Less Wright</a> we learned about Async TP. The idea is to decompose our operations (comm and computations) into more finegrained operations (say instead of a big matmul that would require receiving the full tensor, we do smaller matmuls and receive a sharded input slice by slice). We also have 2 streams, a computation stream performing matmuls (and other kernels) and a communication stream, that way we can do both in parallel and not waste cycles.</p>
<p>However there is a “quantization wave” (see <a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html" target="_blank">this article</a>) at the end of matmuls, and because we split the work to interleave compute and comms, we have alot of matmuls. A solution is to swap the roles of the streams at the end of the first computations, having the compute stream become the comms stream and vice-versa.</p>
</section>
<section id="efficient-strategies-for-distributed-inference" class="level3">
<h3 class="anchored" data-anchor-id="efficient-strategies-for-distributed-inference">Efficient Strategies for Distributed Inference</h3>
<p>Then <a href="https://x.com/_marcsun" target="_blank">Marc Sun</a> gave us a very complete talk on optimizing inference. This <a href="https://www.aleksagordic.com/blog/vllm" target="_blank">inside vLLM article</a> and this <a href="https://pytorch.org/blog/accelerating-generative-ai-2/" target="_blank">Accelerating PyTorch inference article</a> should cover most of the topics. We discussed topics such as prefill vs decode phase, KV Caching, PagedAttention, <code>torch.compile</code>, Quantization, speculative decoding, continuous batching, prefix caching, TP/PP/DP.</p>


</section>

 ]]></description>
  <category>mle</category>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-09-24.html</guid>
  <pubDate>Wed, 24 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>PP and TP with Scrach to Scale</title>
  <link>https://theopomies.com/journal/2025-09-23.html</link>
  <description><![CDATA[ 





<section id="pipeline-parallelism" class="level3">
<h3 class="anchored" data-anchor-id="pipeline-parallelism">Pipeline Parallelism</h3>
<p>Pipeline Parallelism is a parallelism strategy that consists in distributing the model along layers on different ranks, and have the forward pass go sequentialy through the ranks. Think of it as having 1+ layers on each GPU, input in the first, get the activations, put them through the second etc.</p>
<p>The trick now is orchestration. A naive implementation would be to process the full forward, and then the full backward (GPipe). But you incur a lot of idle time (Bubble).</p>
<p>One solution is “1 forward 1 backward” (1f1b), where you interleave the forward and backward passes. This reminded me of DeepSeek’s <em>Open Source Week</em> where they released <a href="https://github.com/deepseek-ai/DualPipe" target="_blank">DualPipe</a></p>
</section>
<section id="tensor-parallelism" class="level3">
<h3 class="anchored" data-anchor-id="tensor-parallelism">Tensor Parallelism</h3>
<p>TP is an intra-layer parallelism strategy that consists of sharding a layer/tensor across different ranks. This makes sense when you think of matmul, you can split a tensor, perform two matmuls (split_n @ input) and concatenate (for column parallel) or add (for row parallel) the results to recover the original output .</p>
<p>TP is useful when the model is pretty wide AND you have NVLink. Without NVLink you get drowned in communications and the gains vanish.</p>


</section>

 ]]></description>
  <category>mle</category>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-09-23.html</guid>
  <pubDate>Tue, 23 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Regular job + slight progress on ZeRO-2/3</title>
  <link>https://theopomies.com/journal/2025-09-22.html</link>
  <description><![CDATA[ 





<section id="regular-job" class="level3">
<h3 class="anchored" data-anchor-id="regular-job">Regular Job</h3>
<p>We’ve had some issues with our SIM provider resulting in intermittent loss of network on our edge devices, so I’m working on issues to mitigate and enhance our resilience infra.</p>
<p>As a result of developping a new product from scratch in-house, we’ve worked on a design overhaul of the entire intercom system, which I’ve already fully implemented. But we’ve decided to ship this update to our existing system. That’s running on a completely different tech stack, so I have to duplicate the work and effort, leading to little time for ML.</p>
</section>
<section id="zero-23" class="level3">
<h3 class="anchored" data-anchor-id="zero-23">ZeRO-2/3</h3>
<p>Not much to add. I’ve begun writing my toy FSDP implementation, not much done yet. I did hit a wall trying to run the same script on my macbook pro and on Modal. torchrun would fail with cryptic issues about IPv6, here’s the line to run the script working:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode sh code-overflow-wrap code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">GLOO_SOCKET_FAMILY</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>inet <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">PYTORCH_ENABLE_MPS_FALLBACK</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>1 <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">uv</span> run <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-m</span> torch.distributed.run <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--nnodes</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>1 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--node_rank</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>0 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--master_addr</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>127.0.0.1 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--master_port</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>29500 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--nproc_per_node</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span>script.py<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span></span></code></pre></div></div>


</section>

 ]]></description>
  <category>mle</category>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-09-22.html</guid>
  <pubDate>Mon, 22 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>DataLoader Workshop and some more ZeRO research</title>
  <link>https://theopomies.com/journal/2025-09-19.html</link>
  <description><![CDATA[ 





<p>Light work today on the ML side, big day at regular job, lots to do, it do be like that sometimes, and it’s important to keep my priorities straight and aligned. I have the remember the opportunity that I have beign CTO and partner at <a href="https://kivala.fr" target="_blank">Kivala</a>, I’ve cried in gratitude, I’ve prayed for this opportunity, Theo from 2 years wanted to be where I am, so even though I feel shiny object syndrom about AI/ML and the field’s pillars are super interesting, I should not forget that I’m still a value-creation machine and I should first and foremost build Kivala, and sell it for a hefty price.</p>
<section id="dataloader-workshop" class="level3">
<h3 class="anchored" data-anchor-id="dataloader-workshop">DataLoader Workshop</h3>
<p>Still, in the evening I went through the newly uploaded DataLoader workshop, once again as part of Scratch To Scale.</p>
<p>Fairly interesting but I didn’t learn much, I already had a strong understanding on the inner workings of a DataLoader and how to write a distributed one. Basically we just pull data index by rank so each rank gets its own mini-batch and there’s no intersection.</p>
</section>
<section id="zero" class="level3">
<h3 class="anchored" data-anchor-id="zero">ZeRO</h3>
<p>Finally still on the ZeRO side, I’ve explored better ways to implement it, better ways to wrap the model. I still feel like the gap between ease of toy implementation and production implementation are huge and I’m not sure I’ll be able to build a production-ready, generci and flexible, ZeRO implementation. But I can say for sure that now I think I really do understand the algorithm much better than I did a few days ago.</p>


</section>

 ]]></description>
  <category>mle</category>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-09-19.html</guid>
  <pubDate>Fri, 19 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>ZeRO-2 + Arctic Sequence Length Training presentation by Tunji Ruwase from Snowflake</title>
  <link>https://theopomies.com/journal/2025-09-18.html</link>
  <description><![CDATA[ 





<section id="zero-1---zero-2" class="level3">
<h3 class="anchored" data-anchor-id="zero-1---zero-2">ZeRo-1 -&gt; ZeRO-2</h3>
<p>Well, today I realized my toy implementation of ZeRO-1 was not very scalable, I flattened the optimizer states and split them so that each rank owned complete state tensors. This is no-bueno because very unbalanced, for 2 GPUs, rank 0 owned 99% of the total size, for 3 optimizer state tensors, and rank 1 owned 1% of the total size, for 3 state tensors too! Furthermore, it is a poor abstraction for my implementation of ZeRO-2. Now I’m flattening the weights, and sharding the tensors themselves.</p>
</section>
<section id="aslt-by-tunji-ruwase" class="level3">
<h3 class="anchored" data-anchor-id="aslt-by-tunji-ruwase">ASLT by Tunji Ruwase</h3>
<p>Once again, as part of Scratch To Scale, we’ve had another wonderful lecture by yet another industry expert, <a href="https://www.snowflake.com/en/blog/authors/olatunji--tunji--ruwase/" target="_blank">Tunji Ruwase</a> from <a href="https://www.snowflake.com/" target="_blank">Snowflake</a>, previously from <a href="https://www.deepspeed.ai" target="_blank">Microsoft DeepSpeed</a>! This technique allows the tiling of sequences to expand training to much longer sequences that would otherwise OOM. I’m not gonna lie, the level of these lectures is slightly out of my current knowledge frontier, but it’s great because it forces me to <em>pull</em> knowledge on-demande and learn very fast.</p>


</section>

 ]]></description>
  <category>mle</category>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-09-18.html</guid>
  <pubDate>Thu, 18 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Implementing ZeRO-1 + Lectures on DTensor/DeviceMesh and Parallel Processing</title>
  <link>https://theopomies.com/journal/2025-09-17.html</link>
  <description><![CDATA[ 





<p>Like I said <a href="../journal/2025-09-16.html">yesterday</a>, I had been a bit passive during the course (Scratch To Scale), not missing a class, going through the material, but not implementing it from scratch. This was for multiple reasons, including overload (doing a lot lately) but that’s no excuse. (Reading <a href="https://x.com/justinskycak/status/1967657669713596786" target="_blank">this tweet</a> was a much needed kick in the butt)</p>
<section id="implementing-dp-and-zero-1-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="implementing-dp-and-zero-1-from-scratch">Implementing DP and ZeRO-1 from scratch</h3>
<p>So I took the class replays, the <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook" target="_blank">UltraScale Playbook</a>, the paper, blogs etc. and tried to really understand the precise interweaving of computations and communications. At some point it just <strong>clicked</strong>, like really, not the first level like “ok I get it”, but the last “ok, I get it <strong>now</strong>”. But then you write the PyTorch code, and the little details bite you. I had the general picture, now I had to do the grunt work. I made a DP wrapper for a model, handling gradient sync, then I wrote a ZeRO-1 Optimizer wrapper. I did what seemed obvious from what I understood: try to shard the optimizer states. For that I wanted to <code>del useless_state_on_this_rank</code>. Except states are lazy. So I just removed the optimizers pointers to some of the model params, as a result the optimizer simply doesn’t create states for them since it ignores their existence.</p>
<p>I’ll link a notebook once I’ve done ZeRO-2 and 3</p>
</section>
<section id="dtensor-and-devicemesh" class="level3">
<h3 class="anchored" data-anchor-id="dtensor-and-devicemesh">DTensor and DeviceMesh</h3>
<p>Mostly 🤯. <a href="https://x.com/wanchao_" target="_blank">Wanchao Liang</a> from Thinking Machines, <strong>author</strong> of PyTorch’s DTensor and TorchTitan, gave us a lecture on DTensor. It was dense, mindblowing, and intense since I follow this cohort from France, so lessons are in the evening.</p>
<p>Basically, if I had to explain what I understood, DTensors are a syntactic sugar over Distributed Tensors and paralllel operations on them. Using specs, we can explain how we want to distribute a tensor, sharding it, replicating it, or representing it as a partial-tensor, pending a reduction.</p>
</section>
<section id="parallel-processing-on-modal" class="level3">
<h3 class="anchored" data-anchor-id="parallel-processing-on-modal">Parallel Processing on Modal</h3>
<p>Finally we had a great lecture/demo by <a href="https://x.com/charles_irl" target="_blank">Charles Frye</a> on the history of computation and parallel programming, followed by demonstrations of how to run distributed/parallel programs on Modal.</p>
<p>I’ve been using Modal to run my notebooks so far, so it’s great to see how we can run scripts/jobs on it too!</p>


</section>

 ]]></description>
  <category>mle</category>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-09-17.html</guid>
  <pubDate>Wed, 17 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Some more probability &amp; statistics and a ZeRO3 + HSDP lesson (S2S)</title>
  <link>https://theopomies.com/journal/2025-09-16.html</link>
  <description><![CDATA[ 





<section id="covariance-random-vectors-outer-product" class="level3">
<h3 class="anchored" data-anchor-id="covariance-random-vectors-outer-product">Covariance + Random Vectors + outer product</h3>
<p>Today I tried to reaaaally understand what <a href="../notes/probability.html#sec-covariance">covariance</a> is, and I think I did. This led me to covariane matrices, which led me to the expectation and variance for random vectors — basically component-wise expectation and (co)variance. This in turn led me to <a href="../notes/linear-algebra.html#sec-outer-product">outer products</a> for a quick digression.</p>
<p>That’s it, it’s not much but it’s honest work. I tried to really understand what they were and why the way it’s computed makes sense.</p>
</section>
<section id="zero3-hsdp" class="level3">
<h3 class="anchored" data-anchor-id="zero3-hsdp">ZeRO3 + HSDP</h3>
<p>Then in the evening we had a lesson on ZeRO3 (ZeRO with sharded optimizer states, gradients <strong>and</strong> model params) and HSDP (FSDP with Hybrid Shards). Not gonna lie, it was a bit dense, and it’s only gonna increase in complexity. I’ve been naughty and did not do too much homework on these lessons, tomorrow I’ll make up for it and grind hard on ZeRO{1,2,3}/{F,H}SDP, to fully grok it.</p>
<p>Juggling regular job, gym, learning/writing math for DL and S2S <em>is</em> intense, but I know that’s the zone where I thrive and reach my potential.</p>


</section>

 ]]></description>
  <category>math</category>
  <category>mle</category>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-09-16.html</guid>
  <pubDate>Tue, 16 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Reviewing Daniel Han’s guest lecture + Backprop Ninja</title>
  <link>https://theopomies.com/journal/2025-09-15.html</link>
  <description><![CDATA[ 





<section id="daniels-triton-kernels-lecture" class="level3">
<h3 class="anchored" data-anchor-id="daniels-triton-kernels-lecture">Daniel’s Triton Kernels lecture</h3>
<p>I wanted to briefly review Daniel’s lecture from last week on Triton kernels from first principles, going through simplifying derivatives for LLM blocks and using calculus tricks to write custom backprop kernels in triton rather than letting the autograd to its magic.</p>
</section>
<section id="backprop-ninja" class="level3">
<h3 class="anchored" data-anchor-id="backprop-ninja">Backprop ninja</h3>
<p>This reminded me of <a href="https://www.youtube.com/watch?v=q8SA3rM6ckI" target="_blank">this lecture</a> by <a href="https://x.com/karpathy" target="_blank">Andrej Karpaty</a> in his <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" target="_blank">NN Zero To Hero</a> series on youtube. So of course, just for practice and to get a stronger grasp on backprop, I did the exercises, implemented the entire backprop by hand (and Apple Pen + iPad). As a first principled kind of guy, there’s nothing better than going this low, understanding where the <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BW%7D%5E%5Ctop"> comes from in the derivatives by writing all the dot products and derivating the loss wrt the inputs/weights and <strong>seeing</strong> the pattern emerge.</p>
<p>I went through the playlist 2 years ago already, in my first wave of interest for ML, and it’s been good getting back in the game. I’m feeling nostalgic, I miss the good parts about living in New York and being locked in.</p>
</section>
<section id="misc" class="level3">
<h3 class="anchored" data-anchor-id="misc">Misc</h3>
<p>I saw the leaks for the incoming Meta Connect keynote, about the new Meta x {RayBan, Oakley}, and I can’t wait for us to enter in the era of <strong>good</strong> AR glasses and embedded AI assistants! This reminds me of a theory I saw on X that the iPhone 17 Air serves many very interesting purposes to Apple, the first being for “fashion” and taking this customer base away from the Pros, but also this is a crash test on miniaturization, and it will finance the progress and experiments towards including this kind of tech in Apple’s own AR glasses, or even using the thinness for foldable phones.</p>
<p>Good time to be optimist about tech.</p>


</section>

 ]]></description>
  <category>math</category>
  <category>mle</category>
  <guid>https://theopomies.com/journal/2025-09-15.html</guid>
  <pubDate>Mon, 15 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Connectionism and Cursor articles</title>
  <link>https://theopomies.com/journal/2025-09-13.html</link>
  <description><![CDATA[ 





<p>Today is the Lord’s day so I try not to do heavy work, and only personal stuff, nothing regular job related. That was the perfect occasion to read two articles from my reading list.</p>
<section id="connectionism---defeating-nondeterminism-in-llm-inference" class="level3">
<h3 class="anchored" data-anchor-id="connectionism---defeating-nondeterminism-in-llm-inference">Connectionism - Defeating Nondeterminism in LLM Inference</h3>
<p>Thinking Machines, <a href="https://x.com/miramurati" target="_blank">Mira Muramati</a>’s very well funded superintelligence lab started their blog with <a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/" target="_blank">their first article</a> by <a href="https://x.com/cHHillee" target="_blank">Horace He</a> tackling (non-)determinism — ie. reproducibility — in current LLMs. The root cause is the non-associative nature of floating-point arithmetic — eg. <img src="https://latex.codecogs.com/png.latex?(x%20+%20y)%20+%20z%20%5Cneq%20x%20+%20(y%20+%20z)">.</p>
<p>This issue arises because of two main components of our inference systems:</p>
<ul>
<li><strong>Concurrency</strong>: the order in which threads finish has repercussions on the order of operations. They note that this is minimal and steps to avoid it are known and good eng. implement them</li>
<li><strong>Non batch-invariant kernels</strong>: some kernels implementations vary depending on batch size, and at inference we the users have no control over the actual batch size being fed to the LLM, we are pooled with other users. This can be solved by implementing batch-invariant kernels, notably for RMSNorm, Matmul and Attention, but requires engineering efforts.</li>
</ul>
</section>
<section id="cursor-tab-online-rl-article" class="level3">
<h3 class="anchored" data-anchor-id="cursor-tab-online-rl-article">Cursor Tab online RL article</h3>
<p>Cursor recently released an interesting <a href="https://cursor.com/blog/tab-rl" target="_blank">article</a> where they announce the release of their updated tab completion model, Cursor Tab, with improved acceptance rate <strong>and</strong> suggestion rate — sometimes not suggesting anything is the right suggestion.</p>
<p>They explain the way they perform online on-policy RL, with rolling releases of the model every 1.5-2h — this is required because in order to have on-policy RL, the rewards collected must come from the current policy, the most recently updated one. They are aiming for even faster release/training cycle. Is this a way to have continual learning?</p>


</section>

 ]]></description>
  <category>math</category>
  <category>mle</category>
  <guid>https://theopomies.com/journal/2025-09-13.html</guid>
  <pubDate>Sat, 13 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Finished eigendecomposition notes + Jacobian</title>
  <link>https://theopomies.com/journal/2025-09-12.html</link>
  <description><![CDATA[ 





<section id="eigendecomposition" class="level3">
<h3 class="anchored" data-anchor-id="eigendecomposition">Eigendecomposition</h3>
<p>I had learned about diagonalization/decomposition using <a href="../notes/linear-algebra.html#sec-eigenv">eigenvectors and eigenvalues</a> in The Manga Guide to Linear Algebra. Now reading more about it in the appendix of D2L, I felt ready to write my notes about them, and after doing a little more research that’s what I did, and I feel I’ve got a strong grasp on the subject.</p>
</section>
<section id="jacobian" class="level3">
<h3 class="anchored" data-anchor-id="jacobian">Jacobian</h3>
<p>With the little free time I still had in this (once again) busy day, I wrote my notes on the <a href="../notes/calculus.html#sec-jacobian">Jacobian</a>, a generalization of the <a href="../notes/calculus.html#sec-gradient">gradient</a> to functions with vector inputs <strong>and</strong> outputs.</p>


</section>

 ]]></description>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-09-12.html</guid>
  <pubDate>Fri, 12 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Writing triton kernels with Daniel Han from Unsloth + linear algebra</title>
  <link>https://theopomies.com/journal/2025-09-11.html</link>
  <description><![CDATA[ 





<section id="linear-algebra-notes" class="level3">
<h3 class="anchored" data-anchor-id="linear-algebra-notes">Linear algebra notes</h3>
<p>Today I wrote some more linear algebra notes, on <a href="../notes/linear-algebra.html#sec-linear-dependence">linear dependence</a>, <a href="../notes/linear-algebra.html#sec-rank">rank</a>, <a href="../notes/linear-algebra.html#sec-basis">basis</a> and <a href="../notes/linear-algebra.html#sec-dimension">dimension</a>. In the process I also wrote about <a href="../notes/linear-algebra.html#sec-vector-space">vector spaces</a>. I really begin to feel the value of these notes, both as an external memory to refer to later, but also as a learning process allowing me to really understand to summarize and re-explain.</p>
</section>
<section id="daniel-han" class="level3">
<h3 class="anchored" data-anchor-id="daniel-han">Daniel Han</h3>
<p>As part of S2S we had a guest lecture by <a href="https://x.com/danielhanchen" target="_blank">Daniel Han</a> from <a href="https://unsloth.ai" target="_blank">Unsloth</a> on <a href="https://triton-lang.org/main/index.html" target="_blank">Triton</a> and custom kernels. It was super interesting because it was working from first principles, and that’s the way I work. Instead of diving into triton’s DSL etc, he took a pen and a stack of paper and wrote neural network graphs of computations, deriving them by hand and explaining how custom training kernels were written as a way to speed-up backprop by using derivation tricks, using our knowledge of calculus, caching computations and results to achieve a better result than a <code>torch.compile</code> or autograd.</p>
</section>
<section id="light-mode" class="level3">
<h3 class="anchored" data-anchor-id="light-mode">Light mode</h3>
<p>I also took way too much time to get an OK-tier light mode for this website.</p>


</section>

 ]]></description>
  <category>mle</category>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-09-11.html</guid>
  <pubDate>Thu, 11 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>FP8 Training with Phuc Nguyen (HF)</title>
  <link>https://theopomies.com/journal/2025-09-10.html</link>
  <description><![CDATA[ 





<p>Big day at work today, so I only got the time for the evening call and some time to re-read yesterday’s lecture (ZeRO/FSDP)</p>
<section id="fp8" class="level3">
<h3 class="anchored" data-anchor-id="fp8">FP8</h3>
<p>Phuc gave us a nice presentation on how and why to train FP8 precision models. Why do it? To speed up training. Issues: the model diverges suuuuuper fast under full FP8 regime, need to be careful. Solution: mixed precision training. We then saw a quick overview of how frontier labs do it (DeepSeek, Meta etc) and frameworks for it (torch/oa, torchtitan)</p>


</section>

 ]]></description>
  <category>mle</category>
  <guid>https://theopomies.com/journal/2025-09-10.html</guid>
  <pubDate>Wed, 10 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>ZeRO / FSDP with Sylvain Gugger and Scott Mueller</title>
  <link>https://theopomies.com/journal/2025-09-09.html</link>
  <description><![CDATA[ 





<section id="zero-fsdp" class="level3">
<h3 class="anchored" data-anchor-id="zero-fsdp">ZeRO / FSDP</h3>
<p>Tonight we had a superb lesson — very dense — by <a href="https://x.com/GuggerSylvain">Sylvain Gugger</a>{_target=blank} on ZeRO, followed by a code dive-in with Scott. Overall takeaway (more detailed in <a href="../notes/distributed-training.html#">my notes</a>): * adam is stateful: has states, so ~4x model size in total to store - ZeRO: <strong>Ze</strong>ro <strong>R</strong>edudency <strong>O</strong>ptimizer -&gt; sharding optimizer state: each gpu updates a subset of the models params then they share it all together all_gather - ZeRO2: Also sharding gradients - ZeRO3 == FSDP (PyTorch version): also sharding the model!</p>
<p>ZeRO is NOT A PARALLELISM strategy, it’s a modeling one. Think parallelism = more throughput, modeling strategy <img src="https://latex.codecogs.com/png.latex?%5Capprox"> memory optimizations</p>


</section>

 ]]></description>
  <category>python</category>
  <category>mle</category>
  <guid>https://theopomies.com/journal/2025-09-09.html</guid>
  <pubDate>Tue, 09 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Marimo Workshop + Integrals refresher</title>
  <link>https://theopomies.com/journal/2025-09-08.html</link>
  <description><![CDATA[ 





<section id="binge-watched-essence-of-calculus" class="level3">
<h3 class="anchored" data-anchor-id="binge-watched-essence-of-calculus">Binge watched Essence of Calculus</h3>
<p>After digging back into probabilities, I realized how important it was that I not only know <a href="../notes/calculus.html#integrals">integrals</a> and formulas, but also that I completely <em>grok</em> and am able to derive these formulas from first principles. So I re-watched the entire <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr" target="_blank">Essence of Calculus</a> series by the incredible <a href="https://www.youtube.com/@3blue1brown" target="_blank">3b1b / Grant Sanderson</a>.</p>
<p>And grokking I did, between the videos and my old high-school and college lessons somewhere hidden deep inside my mind, I feel a renewed understanding of integration, so I wrote notes for the future me that will obviously forget again.</p>
</section>
<section id="some-more-linear-algebra" class="level3">
<h3 class="anchored" data-anchor-id="some-more-linear-algebra">Some more Linear Algebra</h3>
<p>Some research into the basics — first principles always wins — led me to learn about <a href="../notes/linear-algebra.html#sec-elementary-matrix">Elementary Matrices</a>. I’d never given more thought to Gaussian Elimination that just using the row operations to achieve my goal. But thinking about elementary matrices and their properties, made things like <a href="../notes/linear-algebra.html#lu-decomposition-or-lu-factorization">LU decomposition</a> click even more.</p>
</section>
<section id="marimo-workshop" class="level3">
<h3 class="anchored" data-anchor-id="marimo-workshop">Marimo Workshop</h3>
<p>To finish off this awesome day of learning, we had a workshop with <a href="https://koaning.io/" target="_blank">Vincent Warmerdam</a> from the <a href="https://marimo.io" target="_blank">Marimo</a> team. I love marimo and I loved learning more about it’s capabilities and ways to hack (with / at) it.</p>


</section>

 ]]></description>
  <category>python</category>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-09-08.html</guid>
  <pubDate>Mon, 08 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Ray / Anyscale workshop with S2S</title>
  <link>https://theopomies.com/journal/2025-09-05.html</link>
  <description><![CDATA[ 





<section id="going-down-the-ray-rabbit-hole" class="level3">
<h3 class="anchored" data-anchor-id="going-down-the-ray-rabbit-hole">Going down the ray rabbit hole</h3>
<p>Today we had a workshop on <a href="https://ray.io" target="_blank">Ray</a> x <a href="https://anyscale.com" target="_blank">Anyscale</a> by <a href="https://x.com/robertnishihara" target="_blank">Robert Nishihara</a> — co-founder and CEO of Anyscale, co-creator of Ray — himself!</p>
<p>At first I thought this was just gonna be a cloud ad. Like “look this is our platform, here’s how to use it, please do” (lol). I didn’t know Ray, since I don’t know much — yet — about distributed workloads.</p>
<p>The workshop was nice, we covered Ray Data and Ray Train, their data and training libraries. These looked like powerful stuff, but I was still cautious about it, they looked like some frameworks that were too high level, too much abstraction for me, à la fast.ai (it’s good, but too high level for me sometimes, I like the <em>from scratch</em> feel of some other stuff, the tweakability).</p>
<p>During the workshop someone asked about using Ray + vLLM (LLM inference engine). I thought “one’s for training, the other for inference, I don’t see the intersection here”. Oh boy was I wrong. After seing the Anyscale employes answer, I realized I didn’t fully grasp what Ray was and what it offered. So, naturally, I started digging. Know I know that Ray is fully OSS, it’s a library for distributed pythonic applications. Ray Core is <em>just</em> that, and that’s already a lot, it allows us to create remote Tasks and Actors, very good parallel / distributed primitives. Then they built higher-level utilities above that, with Ray {Data, Train, Tune and Serve}.</p>
<p>I had badly misjuged it, I feel like we’re gonna become very close friends.</p>
<p>Before I use a lib I like to know what it does and how it does it, but once I’ll have built a minimal toy version, it’s gonna be you and me buddy.</p>
</section>
<section id="lapace-expansion" class="level3">
<h3 class="anchored" data-anchor-id="lapace-expansion">Lapace Expansion</h3>
<p>To finish the day, I wrote a quick Laplace Expansion function in DeepML.</p>


</section>

 ]]></description>
  <category>python</category>
  <category>mle</category>
  <guid>https://theopomies.com/journal/2025-09-05.html</guid>
  <pubDate>Fri, 05 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Going through DDP with S2S</title>
  <link>https://theopomies.com/journal/2025-09-04.html</link>
  <description><![CDATA[ 





<section id="finishing-the-preliminaries-for-d2l" class="level3">
<h3 class="anchored" data-anchor-id="finishing-the-preliminaries-for-d2l">Finishing the preliminaries for D2L</h3>
<p>Well, finally I’m done with the “catching up” for D2L, with a good refresher on Probabilities. Since I studied stats and probabilities at university, I didn’t <em>learn</em> anything new here, but it’s always good to refresh some knowledge, and re-derive formulas, to have them in mind and <em>grok</em> or <em>accept</em> them as true. Like the classic <a href="../notes/probability.html#bayes-theorem">Bayes Theorem</a>, I always accepted the formula as a way to flip conditional probabilities, but rarely as a way of updating a prior.</p>
</section>
<section id="evening-study-session-on-ddp" class="level3">
<h3 class="anchored" data-anchor-id="evening-study-session-on-ddp">Evening study session on DDP</h3>
<p>We discussed the <a href="../notes/distributed-training.html#algorithms-techniques">different types of parallelism</a> (for ML distributed training):</p>
<ul>
<li>(Distributed) Data Parallelism (<strong>DDP</strong>): Each <em>rank</em> loads a copy — replica — of the model, after each optimizer step they always all have the same parameters, they are <strong>replicants</strong>. Each rank then trains on a <em>different</em> mini-batch (hence the importance of <a href="../notes/distributed-training.html#data-sharding">data sharding</a>). We then average the gradients, perform a step of gradient descent, rinse and repeat. If we <em>can</em> use this, we should, it has the least amount of overhead, but it requires that the model + optimizer states all fit in the device’s VRAM.</li>
<li>Pipeline Parallelism (<strong>PP</strong>): We split the model across different ranks without splitting the layers (so we split <em>along the layers</em>). That is <em>inter-layer</em> parallelism. An exaggeration would be having a model with 2 hidden layers and 1 output layer split across 3 GPUs.</li>
<li>Tensor Parallelism (<strong>TP</strong>): We split the layers of the model across different ranks, that’s <em>intra-layer</em> parallelism. This could be useful if some layers are so large they don’t even fit in a single device. To reduce overhead, it’s advisable that even if a layer is split across different <em>ranks</em>, all parts remain on the same <em>node</em>. (See <a href="../notes/distributed-training.html#terminology">terminology</a>)</li>
</ul>
<p>There’s also</p>
<ul>
<li>Expert Parallelism (<strong>EP</strong>): For Mixture-of-Experts (<strong>MoE</strong>) we can split the experts on different devices.</li>
</ul>
<p>We didn’t discuss much this last one but I knew about it already and searched a bit to know more about it.</p>
<p>Last but not least we discussed the issue of DataLoaders / Samplers, and the importance of data sharding. They are a mean of “splitting” efficiently our dataset so that each rank sees a different mini-batch of data. Also, each process only loads its <em>shard</em> so it’s more memory/communication efficient.</p>


</section>

 ]]></description>
  <category>python</category>
  <category>mle</category>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-09-04.html</guid>
  <pubDate>Thu, 04 Sep 2025 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
