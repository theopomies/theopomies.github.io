<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Theo Pomies</title>
<link>https://theopomies.com/journal/</link>
<atom:link href="https://theopomies.com/journal/index.xml" rel="self" type="application/rss+xml"/>
<description>Daily journal, accountability management and learning tracking.</description>
<generator>quarto-1.8.24</generator>
<lastBuildDate>Sat, 13 Sep 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Connectionism and Cursor articles</title>
  <link>https://theopomies.com/journal/2025-09-13.html</link>
  <description><![CDATA[ 





<p>Today is the Lord’s day so I try not to do heavy work, and only personal stuff, nothing regular job related. That was the perfect occasion to read two articles from my reading list.</p>
<section id="connectionism---defeating-nondeterminism-in-llm-inference" class="level3">
<h3 class="anchored" data-anchor-id="connectionism---defeating-nondeterminism-in-llm-inference">Connectionism - Defeating Nondeterminism in LLM Inference</h3>
<p>Thinking Machines, <a href="https://x.com/miramurati" target="_blank">Mira Muramati</a>’s very well funded superintelligence lab started their blog with <a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/" target="_blank">their first article</a> by <a href="https://x.com/cHHillee" target="_blank">Horace He</a> tackling (non-)determinism — ie. reproducibility — in current LLMs. The root cause is the non-associative nature of floating-point arithmetic — eg. <img src="https://latex.codecogs.com/png.latex?(x%20+%20y)%20+%20z%20%5Cneq%20x%20+%20(y%20+%20z)">.</p>
<p>This issue arises because of two main components of our inference systems:</p>
<ul>
<li><strong>Concurrency</strong>: the order in which threads finish has repercussions on the order of operations. They note that this is minimal and steps to avoid it are known and good eng. implement them</li>
<li><strong>Non batch-invariant kernels</strong>: some kernels implementations vary depending on batch size, and at inference we the users have no control over the actual batch size being fed to the LLM, we are pooled with other users. This can be solved by implementing batch-invariant kernels, notably for RMSNorm, Matmul and Attention, but requires engineering efforts.</li>
</ul>
</section>
<section id="cursor-tab-online-rl-article" class="level3">
<h3 class="anchored" data-anchor-id="cursor-tab-online-rl-article">Cursor Tab online RL article</h3>
<p>Cursor recently released an interesting <a href="https://cursor.com/blog/tab-rl" target="_blank">article</a> where they announce the release of their updated tab completion model, Cursor Tab, with improved acceptance rate <strong>and</strong> suggestion rate — sometimes not suggesting anything is the right suggestion.</p>
<p>They explain the way they perform online on-policy RL, with rolling releases of the model every 1.5-2h — this is required because in order to have on-policy RL, the rewards collected must come from the current policy, the most recently updated one. They are aiming for even faster release/training cycle. Is this a way to have continual learning?</p>


</section>

 ]]></description>
  <category>math</category>
  <category>mle</category>
  <guid>https://theopomies.com/journal/2025-09-13.html</guid>
  <pubDate>Sat, 13 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Finished eigendecomposition notes + Jacobian</title>
  <link>https://theopomies.com/journal/2025-09-12.html</link>
  <description><![CDATA[ 





<section id="eigendecomposition" class="level3">
<h3 class="anchored" data-anchor-id="eigendecomposition">Eigendecomposition</h3>
<p>I had learned about diagonalization/decomposition using <a href="../notes/linear-algebra.html#sec-eigenv">eigenvectors and eigenvalues</a> in The Manga Guide to Linear Algebra. Now reading more about it in the appendix of D2L, I felt ready to write my notes about them, and after doing a little more research that’s what I did, and I feel I’ve got a strong grasp on the subject.</p>
</section>
<section id="jacobian" class="level3">
<h3 class="anchored" data-anchor-id="jacobian">Jacobian</h3>
<p>With the little free time I still had in this (once again) busy day, I wrote my notes on the <a href="../notes/calculus.html#sec-jacobian">Jacobian</a>, a generalization of the <a href="../notes/calculus.html#sec-gradient">gradient</a> to functions with vector inputs <strong>and</strong> outputs.</p>


</section>

 ]]></description>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-09-12.html</guid>
  <pubDate>Fri, 12 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Writing triton kernels with Daniel Han from Unsloth + linear algebra</title>
  <link>https://theopomies.com/journal/2025-09-11.html</link>
  <description><![CDATA[ 





<section id="linear-algebra-notes" class="level3">
<h3 class="anchored" data-anchor-id="linear-algebra-notes">Linear algebra notes</h3>
<p>Today I wrote some more linear algebra notes, on <a href="../notes/linear-algebra.html#sec-linear-dependence">linear dependence</a>, <a href="../notes/linear-algebra.html#sec-rank">rank</a>, <a href="../notes/linear-algebra.html#sec-basis">basis</a> and <a href="../notes/linear-algebra.html#sec-dimension">dimension</a>. In the process I also wrote about <a href="../notes/linear-algebra.html#sec-vector-space">vector spaces</a>. I really begin to feel the value of these notes, both as an external memory to refer to later, but also as a learning process allowing me to really understand to summarize and re-explain.</p>
</section>
<section id="daniel-han" class="level3">
<h3 class="anchored" data-anchor-id="daniel-han">Daniel Han</h3>
<p>As part of S2S we had a guest lecture by <a href="https://x.com/danielhanchen" target="_blank">Daniel Han</a> from <a href="https://unsloth.ai" target="_blank">Unsloth</a> on <a href="https://triton-lang.org/main/index.html" target="_blank">Triton</a> and custom kernels. It was super interesting because it was working from first principles, and that’s the way I work. Instead of diving into triton’s DSL etc, he took a pen and a stack of paper and wrote neural network graphs of computations, deriving them by hand and explaining how custom training kernels were written as a way to speed-up backprop by using derivation tricks, using our knowledge of calculus, caching computations and results to achieve a better result than a <code>torch.compile</code> or autograd.</p>
</section>
<section id="light-mode" class="level3">
<h3 class="anchored" data-anchor-id="light-mode">Light mode</h3>
<p>I also took way too much time to get an OK-tier light mode for this website.</p>


</section>

 ]]></description>
  <category>mle</category>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-09-11.html</guid>
  <pubDate>Thu, 11 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>FP8 Training with Phuc Nguyen (HF)</title>
  <link>https://theopomies.com/journal/2025-09-10.html</link>
  <description><![CDATA[ 





<p>Big day at work today, so I only got the time for the evening call and some time to re-read yesterday’s lecture (ZeRO/FSDP)</p>
<section id="fp8" class="level3">
<h3 class="anchored" data-anchor-id="fp8">FP8</h3>
<p>Phuc gave us a nice presentation on how and why to train FP8 precision models. Why do it? To speed up training. Issues: the model diverges suuuuuper fast under full FP8 regime, need to be careful. Solution: mixed precision training. We then saw a quick overview of how frontier labs do it (DeepSeek, Meta etc) and frameworks for it (torch/oa, torchtitan)</p>


</section>

 ]]></description>
  <category>mle</category>
  <guid>https://theopomies.com/journal/2025-09-10.html</guid>
  <pubDate>Wed, 10 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>ZeRO / FSDP with Sylvain Gugger and Scott Mueller</title>
  <link>https://theopomies.com/journal/2025-09-09.html</link>
  <description><![CDATA[ 





<section id="zero-fsdp" class="level3">
<h3 class="anchored" data-anchor-id="zero-fsdp">ZeRO / FSDP</h3>
<p>Tonight we had a superb lesson — very dense — by <a href="https://x.com/GuggerSylvain">Sylvain Gugger</a>{_target=blank} on ZeRO, followed by a code dive-in with Scott. Overall takeaway (more detailed in <a href="../notes/distributed-training.html#">my notes</a>): * adam is stateful: has states, so ~4x model size in total to store - ZeRO: <strong>Ze</strong>ro <strong>R</strong>edudency <strong>O</strong>ptimizer -&gt; sharding optimizer state: each gpu updates a subset of the models params then they share it all together all_gather - ZeRO2: Also sharding gradients - ZeRO3 == FSDP (PyTorch version): also sharding the model!</p>
<p>ZeRO is NOT A PARALLELISM strategy, it’s a modeling one. Think parallelism = more throughput, modeling strategy <img src="https://latex.codecogs.com/png.latex?%5Capprox"> memory optimizations</p>


</section>

 ]]></description>
  <category>python</category>
  <category>mle</category>
  <guid>https://theopomies.com/journal/2025-09-09.html</guid>
  <pubDate>Tue, 09 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Marimo Workshop + Integrals refresher</title>
  <link>https://theopomies.com/journal/2025-09-08.html</link>
  <description><![CDATA[ 





<section id="binge-watched-essence-of-calculus" class="level3">
<h3 class="anchored" data-anchor-id="binge-watched-essence-of-calculus">Binge watched Essence of Calculus</h3>
<p>After digging back into probabilities, I realized how important it was that I not only know <a href="../notes/calculus.html#integrals">integrals</a> and formulas, but also that I completely <em>grok</em> and am able to derive these formulas from first principles. So I re-watched the entire <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr" target="_blank">Essence of Calculus</a> series by the incredible <a href="https://www.youtube.com/@3blue1brown" target="_blank">3b1b / Grant Sanderson</a>.</p>
<p>And grokking I did, between the videos and my old high-school and college lessons somewhere hidden deep inside my mind, I feel a renewed understanding of integration, so I wrote notes for the future me that will obviously forget again.</p>
</section>
<section id="some-more-linear-algebra" class="level3">
<h3 class="anchored" data-anchor-id="some-more-linear-algebra">Some more Linear Algebra</h3>
<p>Some research into the basics — first principles always wins — led me to learn about <a href="../notes/linear-algebra.html#sec-elementary-matrix">Elementary Matrices</a>. I’d never given more thought to Gaussian Elimination that just using the row operations to achieve my goal. But thinking about elementary matrices and their properties, made things like <a href="../notes/linear-algebra.html#lu-decomposition-or-lu-factorization">LU decomposition</a> click even more.</p>
</section>
<section id="marimo-workshop" class="level3">
<h3 class="anchored" data-anchor-id="marimo-workshop">Marimo Workshop</h3>
<p>To finish off this awesome day of learning, we had a workshop with <a href="https://koaning.io/" target="_blank">Vincent Warmerdam</a> from the <a href="https://marimo.io" target="_blank">Marimo</a> team. I love marimo and I loved learning more about it’s capabilities and ways to hack (with / at) it.</p>


</section>

 ]]></description>
  <category>python</category>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-09-08.html</guid>
  <pubDate>Mon, 08 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Ray / Anyscale workshop with S2S</title>
  <link>https://theopomies.com/journal/2025-09-05.html</link>
  <description><![CDATA[ 





<section id="going-down-the-ray-rabbit-hole" class="level3">
<h3 class="anchored" data-anchor-id="going-down-the-ray-rabbit-hole">Going down the ray rabbit hole</h3>
<p>Today we had a workshop on <a href="https://ray.io" target="_blank">Ray</a> x <a href="https://anyscale.com" target="_blank">Anyscale</a> by <a href="https://x.com/robertnishihara" target="_blank">Robert Nishihara</a> — co-founder and CEO of Anyscale, co-creator of Ray — himself!</p>
<p>At first I thought this was just gonna be a cloud ad. Like “look this is our platform, here’s how to use it, please do” (lol). I didn’t know Ray, since I don’t know much — yet — about distributed workloads.</p>
<p>The workshop was nice, we covered Ray Data and Ray Train, their data and training libraries. These looked like powerful stuff, but I was still cautious about it, they looked like some frameworks that were too high level, too much abstraction for me, à la fast.ai (it’s good, but too high level for me sometimes, I like the <em>from scratch</em> feel of some other stuff, the tweakability).</p>
<p>During the workshop someone asked about using Ray + vLLM (LLM inference engine). I thought “one’s for training, the other for inference, I don’t see the intersection here”. Oh boy was I wrong. After seing the Anyscale employes answer, I realized I didn’t fully grasp what Ray was and what it offered. So, naturally, I started digging. Know I know that Ray is fully OSS, it’s a library for distributed pythonic applications. Ray Core is <em>just</em> that, and that’s already a lot, it allows us to create remote Tasks and Actors, very good parallel / distributed primitives. Then they built higher-level utilities above that, with Ray {Data, Train, Tune and Serve}.</p>
<p>I had badly misjuged it, I feel like we’re gonna become very close friends.</p>
<p>Before I use a lib I like to know what it does and how it does it, but once I’ll have built a minimal toy version, it’s gonna be you and me buddy.</p>
</section>
<section id="lapace-expansion" class="level3">
<h3 class="anchored" data-anchor-id="lapace-expansion">Lapace Expansion</h3>
<p>To finish the day, I wrote a quick Laplace Expansion function in DeepML.</p>


</section>

 ]]></description>
  <category>python</category>
  <category>mle</category>
  <guid>https://theopomies.com/journal/2025-09-05.html</guid>
  <pubDate>Fri, 05 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Going through DDP with S2S</title>
  <link>https://theopomies.com/journal/2025-09-04.html</link>
  <description><![CDATA[ 





<section id="finishing-the-preliminaries-for-d2l" class="level3">
<h3 class="anchored" data-anchor-id="finishing-the-preliminaries-for-d2l">Finishing the preliminaries for D2L</h3>
<p>Well, finally I’m done with the “catching up” for D2L, with a good refresher on Probabilities. Since I studied stats and probabilities at university, I didn’t <em>learn</em> anything new here, but it’s always good to refresh some knowledge, and re-derive formulas, to have them in mind and <em>grok</em> or <em>accept</em> them as true. Like the classic <a href="../notes/probability.html#bayes-theorem">Bayes Theorem</a>, I always accepted the formula as a way to flip conditional probabilities, but rarely as a way of updating a prior.</p>
</section>
<section id="evening-study-session-on-ddp" class="level3">
<h3 class="anchored" data-anchor-id="evening-study-session-on-ddp">Evening study session on DDP</h3>
<p>We discussed the <a href="../notes/distributed-training.html#algorithms-techniques">different types of parallelism</a> (for ML distributed training):</p>
<ul>
<li>(Distributed) Data Parallelism (<strong>DDP</strong>): Each <em>rank</em> loads a copy — replica — of the model, after each optimizer step they always all have the same parameters, they are <strong>replicants</strong>. Each rank then trains on a <em>different</em> mini-batch (hence the importance of <a href="../notes/distributed-training.html#data-sharding">data sharding</a>). We then average the gradients, perform a step of gradient descent, rinse and repeat. If we <em>can</em> use this, we should, it has the least amount of overhead, but it requires that the model + optimizer states all fit in the device’s VRAM.</li>
<li>Pipeline Parallelism (<strong>PP</strong>): We split the model across different ranks without splitting the layers (so we split <em>along the layers</em>). That is <em>inter-layer</em> parallelism. An exaggeration would be having a model with 2 hidden layers and 1 output layer split across 3 GPUs.</li>
<li>Tensor Parallelism (<strong>TP</strong>): We split the layers of the model across different ranks, that’s <em>intra-layer</em> parallelism. This could be useful if some layers are so large they don’t even fit in a single device. To reduce overhead, it’s advisable that even if a layer is split across different <em>ranks</em>, all parts remain on the same <em>node</em>. (See <a href="../notes/distributed-training.html#terminology">terminology</a>)</li>
</ul>
<p>There’s also</p>
<ul>
<li>Expert Parallelism (<strong>EP</strong>): For Mixture-of-Experts (<strong>MoE</strong>) we can split the experts on different devices.</li>
</ul>
<p>We didn’t discuss much this last one but I knew about it already and searched a bit to know more about it.</p>
<p>Last but not least we discussed the issue of DataLoaders / Samplers, and the importance of data sharding. They are a mean of “splitting” efficiently our dataset so that each rank sees a different mini-batch of data. Also, each process only loads its <em>shard</em> so it’s more memory/communication efficient.</p>


</section>

 ]]></description>
  <category>python</category>
  <category>mle</category>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-09-04.html</guid>
  <pubDate>Thu, 04 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Fireside chat QA with Yuxiang Wei from Meta FAIR with S2S</title>
  <link>https://theopomies.com/journal/2025-09-03.html</link>
  <description><![CDATA[ 





<p>Well today I woke up at 5am to go on-site at my startup in Paris (I live 500km away, in Bordeaux) and got home at around 7pm, so not much free time to study, so I just read a bit in the train and ML related stuff was just the evening fireside chat.</p>
<section id="fireside-chat-with-yuxiang-wei-from-meta-fair-s2s" class="level3">
<h3 class="anchored" data-anchor-id="fireside-chat-with-yuxiang-wei-from-meta-fair-s2s">Fireside chat with Yuxiang Wei from Meta FAIR (S2S)</h3>
<p>As part of the course (conference) Scratch To Scale, we have a lot of very qualified engineers and researchers from top labs / companies dedicating time for guest lectures or QA sessions.</p>
<p>Today we had the chance to listen to Yuxiang Wei — of Meta FAIR, of the SWE-RL paper — and ask him loads of questions. We discussed topics such as {pre, mid, post}-training, training infrastructure, the advent of RL and its impacts on said infrastructure and more.</p>


</section>

 ]]></description>
  <category>mle</category>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-09-03.html</guid>
  <pubDate>Wed, 03 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Starting Scratch To Scale (S2S) and Dive into Deep Learning (D2L)</title>
  <link>https://theopomies.com/journal/2025-09-02.html</link>
  <description><![CDATA[ 





<p>Today I revised the basics of calculus on D2L, and ended the day with the first “lesson” on Distributed Training (S2S) by <a href="https://www.x.com/thezachmueller">Zach Mueller</a>.</p>
<section id="calculus" class="level3">
<h3 class="anchored" data-anchor-id="calculus">Calculus</h3>
<p>I worked a bit on calculus, and there’s always something to learn, even we you go as far back as the high-school level stuff. Small example, it’s just today that I realized that <img src="https://latex.codecogs.com/png.latex?%5Cdfrac%7Bdx%7D%7Bdy%7D%20=%20%5Cdfrac%7B1%7D%7B%5Cfrac%7Bdy%7D%7Bdx%7D%7D"> (cf.&nbsp;the definition of derivative as a limit).</p>
<p>I also got my hands back into multivariate calculus and learning useful identities.</p>
</section>
<section id="distributed-training-s2s" class="level3">
<h3 class="anchored" data-anchor-id="distributed-training-s2s">Distributed Training (S2S)</h3>
<p>Finally, I finished the day learning the basics of distributed/parallel processing/training on GPUs (using <code>torch.distributed</code>, we’re not yet at the triton or CUDA level, but someday we’ll be there, just watch).</p>
<p>We went from the <a href="../notes/distributed-training.html#basics-point-point">primitives</a> — (i)send and (i)recv — to the <a href="../notes/distributed-training.html#collective-operations">collective operations</a> — reduce, all_reduce, scatter, reduce_scatter, broadcast, barrier, all2all, gather, all_gather. I can now much more easily conceive how distributed training algorithms work.</p>
<p>I learned a few distributed training concepts, such as the <em>rank</em>.</p>
<p>I concluded the day by running my first notebooks accelerated by more than 1 GPU on <a href="https://www.modal.com" target="_blank">Modal</a>. I’d done some lightly GPU accelerated stuff on <a href="https://www.kaggle.com" target="_blank">Kaggle</a>, but now I could grasp how to do stuff with multiple GPUs.</p>


</section>

 ]]></description>
  <category>math</category>
  <category>python</category>
  <category>mle</category>
  <guid>https://theopomies.com/journal/2025-09-02.html</guid>
  <pubDate>Tue, 02 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Finished reading The Manga Guide to Linear Algebra</title>
  <link>https://theopomies.com/journal/2025-09-01.html</link>
  <description><![CDATA[ 





<p>I had a lot of work to do for my day job, we are trying to build and commercialize V2 of <a href="https://www.kivala.fr" target="_blank">our intercom system</a>, and I was faced with technical difficulties that left me very little time to work on math / ML. I still found the time to finish The Manga Guide to Linear Algebra, and learned a few important concepts and techniques!</p>
<section id="still-i-did-a-few-things" class="level3">
<h3 class="anchored" data-anchor-id="still-i-did-a-few-things">Still, I did a <em>few</em> things</h3>
<p>TIL about [subspaces], [linear spans], the [rank] of a matrix and a technique to find it via <a href="../notes/linear-algebra.html#sec-gaussian-elimination">Gaussian Eliminination</a> — as usual, linear algebra appears to be the art of applying Gaussian Elimination correctly —, [Eigenvalues] and [Eigenvectors]! That was still packed!</p>
<p>I skimmed through the materials for <a href="https://maven.com/walk-with-code/scratch-to-scale" target="_blank">Scratch To Scale</a>, the course I’m following by <a href="https://x.com/thezachmueller" target="_blank">Scott Mueller</a>, beginning tomorrow.</p>


</section>

 ]]></description>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-09-01.html</guid>
  <pubDate>Mon, 01 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Started reading The Manga Guide to Linear Algebra</title>
  <link>https://theopomies.com/journal/2025-08-29.html</link>
  <description><![CDATA[ 





<section id="the-manga-guide-to-linear-algebra" class="level2">
<h2 class="anchored" data-anchor-id="the-manga-guide-to-linear-algebra">The Manga Guide to Linear Algebra</h2>
<section id="why" class="level3">
<h3 class="anchored" data-anchor-id="why">Why?</h3>
<p>Yes, this might sound weird if you don’t know the concept, but hear me out. After seing it praised on Twitter/X by an anon, I got myself a copy of <a href="https://www.amazon.com/Manga-Guide-Linear-Algebra/dp/1593274130" target="_blank">The Manga Guide to Linear Algebra</a>.</p>
<p>It’s pretty good if you want to quickly get up to speed, develop intuition, but does not go too deep into formal proofs.</p>
<p>You might not know me, but before going to a Software Engineering School, I studied math for 2 years in Uni, and dropped out because it was too “theorical”, not concrete enough for me, I needed to <em>grok</em> the formulas, to visualize, feel the proofs naturally and see the applications. Sadly, partly due to how Uni works — partly due to playing League of Legends too much — I did not get the level of understanding required for my attention to stick.</p>
<p>Now that I’ve become a SWE, I’ve seen the practical uses of such Math, and I now find myself <em>needing</em> more of it! So, step by step I’ll try to go from intuition and basics (this book) -&gt; implementing existing algorithms -&gt; proofs and more advanced formulas and theorem (some other textbook like <a href="https://linear.axler.net/LADR4e.pdf" target="_blank">Linear Algebra Done Right</a>) -&gt; understanding ML architecture choices -&gt; being able to invent/discover new arch and techniques?</p>
</section>
<section id="what-i-learned-so-far" class="level3">
<h3 class="anchored" data-anchor-id="what-i-learned-so-far">What I learned so far</h3>
<p>Being somewhat familiar with Linear Algebra and computer science, I already knew a few things, so I read pretty fast through the first bits.</p>
<p>The first thing I learned were the techniques for computing the determinant for matrices of dimensions 4 and above. There’s a technique based on the generalisation of the formula <img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BA%7D)%20=%20a_%7B11%7Da_%7B22%7D%20-%20a_%7B12%7Da_%7B21%7D">.</p>
<p>Along with the interpretation/intuition for the determinant (consolidated by 3b1b).</p>
<p>But there’s also <a href="../notes/linear-algebra.html#lu-decomposition-or-lu-factorization">an algorithm (LU decomposition)</a> used in computer science based on a set of rules for determinant and gaussian reduction.</p>


</section>
</section>

 ]]></description>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-08-29.html</guid>
  <pubDate>Fri, 29 Aug 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Setting up this blog!</title>
  <link>https://theopomies.com/journal/2025-08-28.html</link>
  <description><![CDATA[ 





<section id="this-blog" class="level2">
<h2 class="anchored" data-anchor-id="this-blog">This “Blog”</h2>
<p>Today’s the day!</p>
<p>I’ve decided to document my learning process to “transition” from SWE / using AI <img src="https://latex.codecogs.com/png.latex?%5Clongrightarrow"> MLE/Applied Research/ building and training AI.</p>
<p>After experimenting with <a href="https://marimo.io" target="_blank">marimo</a>, loving it and <a href="https://github.com/marimo-team/marimo/issues/6172" target="_blank">struggling</a> to make a blog out of marimo notebooks and experiments, I’ve stumbled upon <a href="https://www.fast.ai/posts/2020-01-20-nb2md.html" target="_blank">this article</a> by <a href="https://x.com/jeremyphoward" target="_blank">Jeremy Howard</a> encouraging me to write my blog via notebooks + <a href="https://quarto.org" target="_blank">Quarto</a>.</p>
<p>It also happens that I had just purchased the <a href="https://maven.com/walk-with-code/scratch-to-scale" target="_blank">Scratch To Scale course/conference</a> by the brilliant <a href="https://x.com/thezachmueller" target="_blank">Zach Mueller</a> and <a href="https://muellerzr.github.io" target="_blank">his blog</a> was also using Quarto — which I would soon learn was no coincidence, Zach and Jeremy were closed and Zach contributed to Jeremy’s framework, <a href="https://www.fast.ai" target="_blank">fast.ai</a>.</p>
<p>So there it is, I intend to document my learning process, render notebooks of experiments, store knowledge, formulas I’ve finally understood even if I’m the last person on earth to do so, bookmarks, reading notes etc.</p>


</section>

 ]]></description>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-08-28.html</guid>
  <pubDate>Thu, 28 Aug 2025 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
