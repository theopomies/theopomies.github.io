<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Theo Pomies</title>
<link>https://theopomies.com/journal/</link>
<atom:link href="https://theopomies.com/journal/index.xml" rel="self" type="application/rss+xml"/>
<description>Daily journal, accountability management and learning tracking.</description>
<generator>quarto-1.7.34</generator>
<lastBuildDate>Thu, 04 Sep 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Going through DDP with S2S</title>
  <link>https://theopomies.com/journal/2025-09-04.html</link>
  <description><![CDATA[ 




<p>DDP -&gt; parallelizing data</p>
<p>Types of parallelism DDP = Running the same model on multiple ranks, sending different mini-batch to different ranks, averaging gradients (all_reduce with sum then division by worldsize)</p>
<p>Pipeline P -&gt; layers on gpus so they run on a chain</p>
<p>Tensor P (mostly intra-node) -&gt; distributes a single matrix / splits a single layer</p>
<section id="ddp" class="level1">
<h1>DDP</h1>
<p>n gpus = n replicants of the model</p>
<p>_ notebook _</p>
<p>Try to understand data sharding / sharded sampling (see torch / accelerate)</p>


</section>

 ]]></description>
  <category>python</category>
  <category>mle</category>
  <guid>https://theopomies.com/journal/2025-09-04.html</guid>
  <pubDate>Thu, 04 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Some D2L and a fireside chat QA with Yuxiang Wei from Meta FAIR with S2S</title>
  <link>https://theopomies.com/journal/2025-09-03.html</link>
  <description><![CDATA[ 




<p>Some notes and answers by Yuxiang Wei here Probabilities on D2L</p>



 ]]></description>
  <category>math</category>
  <category>mle</category>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-09-03.html</guid>
  <pubDate>Wed, 03 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Starting Scratch To Scale (S2S) and Dive into Deep Learning (D2L)</title>
  <link>https://theopomies.com/journal/2025-09-02.html</link>
  <description><![CDATA[ 




<p>Today I revised the basics of calculus on D2L, and ended the day with the first “lesson” on Distributed Training (S2S) by <a href="https://www.x.com/thezachmueller">Zach Mueller</a>.</p>
<section id="calculus" class="level3">
<h3 class="anchored" data-anchor-id="calculus">Calculus</h3>
<p>I worked a bit on calculus, and there’s always something to learn, even we you go as far back as the high-school level stuff. Small example, it’s just today that I realized that <img src="https://latex.codecogs.com/png.latex?%5Cdfrac%7Bdx%7D%7Bdy%7D%20=%20%5Cdfrac%7B1%7D%7B%5Cfrac%7Bdy%7D%7Bdx%7D%7D"> (cf.&nbsp;the definition of derivative as a limit).</p>
<p>I also got my hands back into multivariate calculus and learning useful identities.</p>
</section>
<section id="distributed-training-s2s" class="level3">
<h3 class="anchored" data-anchor-id="distributed-training-s2s">Distributed Training (S2S)</h3>
<p>Finally, I finished the day learning the basics of distributed/parallel processing/training on GPUs (using <code>torch.distributed</code>, we’re not yet at the triton or CUDA level, but someday we’ll be there, just watch).</p>
<p>We went from the <a href="../notes/distributed-training.html#basics-point-point">primitives</a> — (i)send and (i)recv — to the <a href="../notes/distributed-training.html#collective-operations">collective operations</a> — reduce, all_reduce, scatter, reduce_scatter, broadcast, barrier, all2all, gather, all_gather. I can now much more easily conceive how distributed training algorithms work.</p>
<p>I learned a few distributed training concepts, such as the <em>rank</em>.</p>
<p>I concluded the day by running my first notebooks accelerated by more than 1 GPU on <a href="https://www.modal.com" target="_blank">Modal</a>. I’d done some lightly GPU accelerated stuff on <a href="https://www.kaggle.com" target="_blank">Kaggle</a>, but now I could grasp how to do stuff with multiple GPUs.</p>


</section>

 ]]></description>
  <category>math</category>
  <category>python</category>
  <category>mle</category>
  <guid>https://theopomies.com/journal/2025-09-02.html</guid>
  <pubDate>Tue, 02 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Finished reading The Manga Guide to Linear Algebra</title>
  <link>https://theopomies.com/journal/2025-09-01.html</link>
  <description><![CDATA[ 




<p>I had a lot of work to do for my day job, we are trying to build and commercialize V2 of <a href="https://www.kivala.fr" target="_blank">our intercom system</a>, and I was faced with technical difficulties that left me very little time to work on math / ML. I still found the time to finish The Manga Guide to Linear Algebra, and learned a few important concepts and techniques!</p>
<section id="still-i-did-a-few-things" class="level3">
<h3 class="anchored" data-anchor-id="still-i-did-a-few-things">Still, I did a <em>few</em> things</h3>
<p>TIL about [subspaces], [linear spans], the [rank] of a matrix and a technique to find it via <a href="../notes/linear-algebra.html#gaussian-elimination">Gaussian Eliminination</a> — as usual, linear algebra appears to be the art of applying Gaussian Elimination correctly —, [Eigenvalues] and [Eigenvectors]! That was still packed!</p>
<p>I skimmed through the materials for <a href="https://maven.com/walk-with-code/scratch-to-scale" target="_blank">Scratch To Scale</a>, the course I’m following by <a href="https://x.com/thezachmueller" target="_blank">Scott Mueller</a>, beginning tomorrow.</p>


</section>

 ]]></description>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-09-01.html</guid>
  <pubDate>Mon, 01 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Started reading The Manga Guide to Linear Algebra</title>
  <link>https://theopomies.com/journal/2025-08-29.html</link>
  <description><![CDATA[ 




<section id="the-manga-guide-to-linear-algebra" class="level2">
<h2 class="anchored" data-anchor-id="the-manga-guide-to-linear-algebra">The Manga Guide to Linear Algebra</h2>
<section id="why" class="level3">
<h3 class="anchored" data-anchor-id="why">Why?</h3>
<p>Yes, this might sound weird if you don’t know the concept, but hear me out. After seing it praised on Twitter/X by an anon, I got myself a copy of <a href="https://www.amazon.com/Manga-Guide-Linear-Algebra/dp/1593274130" target="_blank">The Manga Guide to Linear Algebra</a>.</p>
<p>It’s pretty good if you want to quickly get up to speed, develop intuition, but does not go too deep into formal proofs.</p>
<p>You might not know me, but before going to a Software Engineering School, I studied math for 2 years in Uni, and dropped out because it was too “theorical”, not concrete enough for me, I needed to <em>grok</em> the formulas, to visualize, feel the proofs naturally and see the applications. Sadly, partly due to how Uni works — partly due to playing League of Legends too much — I did not get the level of understanding required for my attention to stick.</p>
<p>Now that I’ve become a SWE, I’ve seen the practical uses of such Math, and I now find myself <em>needing</em> more of it! So, step by step I’ll try to go from intuition and basics (this book) -&gt; implementing existing algorithms -&gt; proofs and more advanced formulas and theorem (some other textbook like <a href="https://linear.axler.net/LADR4e.pdf" target="_blank">Linear Algebra Done Right</a>) -&gt; understanding ML architecture choices -&gt; being able to invent/discover new arch and techniques?</p>
</section>
<section id="what-i-learned-so-far" class="level3">
<h3 class="anchored" data-anchor-id="what-i-learned-so-far">What I learned so far</h3>
<p>Being somewhat familiar with Linear Algebra and computer science, I already knew a few things, so I read pretty fast through the first bits.</p>
<p>The first thing I learned were the techniques for computing the determinant for matrices of dimensions 4 and above. There’s a technique based on the generalisation of the formula <img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BA%7D)%20=%20a_%7B11%7Da_%7B22%7D%20-%20a_%7B12%7Da_%7B21%7D">.</p>
<p>Along with the interpretation/intuition for the determinant (consolidated by 3b1b).</p>
<p>But there’s also <a href="../notes/linear-algebra.html#lu-decomposition-or-lu-factorization">an algorithm (LU decomposition)</a> used in computer science based on a set of rules for determinant and gaussian reduction.</p>


</section>
</section>

 ]]></description>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-08-29.html</guid>
  <pubDate>Fri, 29 Aug 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Setting up this blog!</title>
  <link>https://theopomies.com/journal/2025-08-28.html</link>
  <description><![CDATA[ 




<section id="this-blog" class="level2">
<h2 class="anchored" data-anchor-id="this-blog">This “Blog”</h2>
<p>Today’s the day!</p>
<p>I’ve decided to document my learning process to “transition” from SWE / using AI <img src="https://latex.codecogs.com/png.latex?%5Clongrightarrow"> MLE/Applied Research/ building and training AI.</p>
<p>After experimenting with <a href="https://marimo.io" target="_blank">marimo</a>, loving it and <a href="https://github.com/marimo-team/marimo/issues/6172" target="_blank">struggling</a> to make a blog out of marimo notebooks and experiments, I’ve stumbled upon <a href="https://www.fast.ai/posts/2020-01-20-nb2md.html" target="_blank">this article</a> by <a href="https://x.com/jeremyphoward" target="_blank">Jeremy Howard</a> encouraging me to write my blog via notebooks + <a href="https://quarto.org" target="_blank">Quarto</a>.</p>
<p>It also happens that I had just purchased the <a href="https://maven.com/walk-with-code/scratch-to-scale" target="_blank">Scratch To Scale course/conference</a> by the brilliant <a href="https://x.com/thezachmueller" target="_blank">Zach Mueller</a> and <a href="https://muellerzr.github.io" target="_blank">his blog</a> was also using Quarto — which I would soon learn was no coincidence, Zach and Jeremy were closed and Zach contributed to Jeremy’s framework, <a href="https://www.fast.ai" target="_blank">fast.ai</a>.</p>
<p>So there it is, I intend to document my learning process, render notebooks of experiments, store knowledge, formulas I’ve finally understood even if I’m the last person on earth to do so, bookmarks, reading notes etc.</p>


</section>

 ]]></description>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-08-28.html</guid>
  <pubDate>Thu, 28 Aug 2025 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
