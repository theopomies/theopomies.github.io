<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Theo Pomies</title>
<link>https://theopomies.com/journal/</link>
<atom:link href="https://theopomies.com/journal/index.xml" rel="self" type="application/rss+xml"/>
<description>Daily journal, accountability management and learning tracking.</description>
<generator>quarto-1.7.34</generator>
<lastBuildDate>Mon, 08 Sep 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Marimo Workshop + Integrals refresher</title>
  <link>https://theopomies.com/journal/2025-09-08.html</link>
  <description><![CDATA[ 




<section id="binge-watched-essence-of-calculus" class="level3">
<h3 class="anchored" data-anchor-id="binge-watched-essence-of-calculus">Binge watched Essence of Calculus</h3>
<p>After digging back into probabilities, I realized how important it was that I not only know <a href="../notes/calculus.html#integrals">integrals</a> and formulas, but also that I completely <em>grok</em> and am able to derive these formulas from first principles. So I re-watched the entire <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr" target="_blank">Essence of Calculus</a> series by the incredible <a href="https://www.youtube.com/@3blue1brown" target="_blank">3b1b / Grant Sanderson</a>.</p>
<p>And grokking I did, between the videos and my old high-school and college lessons somewhere hidden deep inside my mind, I feel a renewed understanding of integration, so I wrote notes for the future me that will obviously forget again.</p>
</section>
<section id="some-more-linear-algebra" class="level3">
<h3 class="anchored" data-anchor-id="some-more-linear-algebra">Some more Linear Algebra</h3>
<p>Some research into the basics — first principles always wins — led me to learn about <a href="../notes/linear-algebra.html#elementary-matrix">Elementary Matrices</a>. I’d never given more thought to Gaussian Elimination that just using the row operations to achieve my goal. But thinking about elementary matrices and their properties, made things like <a href="../notes/linear-algebra.html#lu-decomposition-or-lu-factorization">LU decomposition</a> click even more.</p>
</section>
<section id="marimo-workshop" class="level3">
<h3 class="anchored" data-anchor-id="marimo-workshop">Marimo Workshop</h3>
<p>To finish off this awesome day of learning, we had a workshop with <a href="https://koaning.io/" target="_blank">Vincent Warmerdam</a> from the <a href="https://marimo.io" target="_blank">Marimo</a> team. I love marimo and I loved learning more about it’s capabilities and ways to hack (with / at) it.</p>


</section>

 ]]></description>
  <category>python</category>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-09-08.html</guid>
  <pubDate>Mon, 08 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Ray / Anyscale workshop with S2S</title>
  <link>https://theopomies.com/journal/2025-09-05.html</link>
  <description><![CDATA[ 




<section id="going-down-the-ray-rabbit-hole" class="level3">
<h3 class="anchored" data-anchor-id="going-down-the-ray-rabbit-hole">Going down the ray rabbit hole</h3>
<p>Today we had a workshop on <a href="https://ray.io" target="_blank">Ray</a> x <a href="https://anyscale.com" target="_blank">Anyscale</a> by <a href="https://x.com/robertnishihara" target="_blank">Robert Nishihara</a> — co-founder and CEO of Anyscale, co-creator of Ray — himself!</p>
<p>At first I thought this was just gonna be a cloud ad. Like “look this is our platform, here’s how to use it, please do” (lol). I didn’t know Ray, since I don’t know much — yet — about distributed workloads.</p>
<p>The workshop was nice, we covered Ray Data and Ray Train, their data and training libraries. These looked like powerful stuff, but I was still cautious about it, they looked like some frameworks that were too high level, too much abstraction for me, à la fast.ai (it’s good, but too high level for me sometimes, I like the <em>from scratch</em> feel of some other stuff, the tweakability).</p>
<p>During the workshop someone asked about using Ray + vLLM (LLM inference engine). I thought “one’s for training, the other for inference, I don’t see the intersection here”. Oh boy was I wrong. After seing the Anyscale employes answer, I realized I didn’t fully grasp what Ray was and what it offered. So, naturally, I started digging. Know I know that Ray is fully OSS, it’s a library for distributed pythonic applications. Ray Core is <em>just</em> that, and that’s already a lot, it allows us to create remote Tasks and Actors, very good parallel / distributed primitives. Then they built higher-level utilities above that, with Ray {Data, Train, Tune and Serve}.</p>
<p>I had badly misjuged it, I feel like we’re gonna become very close friends.</p>
<p>Before I use a lib I like to know what it does and how it does it, but once I’ll have built a minimal toy version, it’s gonna be you and me buddy.</p>
</section>
<section id="lapace-expansion" class="level3">
<h3 class="anchored" data-anchor-id="lapace-expansion">Lapace Expansion</h3>
<p>To finish the day, I wrote a quick Laplace Expansion function in DeepML.</p>


</section>

 ]]></description>
  <category>python</category>
  <category>mle</category>
  <guid>https://theopomies.com/journal/2025-09-05.html</guid>
  <pubDate>Fri, 05 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Going through DDP with S2S</title>
  <link>https://theopomies.com/journal/2025-09-04.html</link>
  <description><![CDATA[ 




<section id="finishing-the-preliminaries-for-d2l" class="level3">
<h3 class="anchored" data-anchor-id="finishing-the-preliminaries-for-d2l">Finishing the preliminaries for D2L</h3>
<p>Well, finally I’m done with the “catching up” for D2L, with a good refresher on Probabilities. Since I studied stats and probabilities at university, I didn’t <em>learn</em> anything new here, but it’s always good to refresh some knowledge, and re-derive formulas, to have them in mind and <em>grok</em> or <em>accept</em> them as true. Like the classic <a href="../notes/probability.html#bayes-theorem">Bayes Theorem</a>, I always accepted the formula as a way to flip conditional probabilities, but rarely as a way of updating a prior.</p>
</section>
<section id="evening-study-session-on-ddp" class="level3">
<h3 class="anchored" data-anchor-id="evening-study-session-on-ddp">Evening study session on DDP</h3>
<p>We discussed the <a href="../notes/distributed-training.html#algorithms-techniques">different types of parallelism</a> (for ML distributed training):</p>
<ul>
<li>(Distributed) Data Parallelism (<strong>DDP</strong>): Each <em>rank</em> loads a copy — replica — of the model, after each optimizer step they always all have the same parameters, they are <strong>replicants</strong>. Each rank then trains on a <em>different</em> mini-batch (hence the importance of <a href="../notes/distributed-training.html#data-sharding">data sharding</a>). We then average the gradients, perform a step of gradient descent, rinse and repeat. If we <em>can</em> use this, we should, it has the least amount of overhead, but it requires that the model + optimizer states all fit in the device’s VRAM.</li>
<li>Pipeline Parallelism (<strong>PP</strong>): We split the model across different ranks without splitting the layers (so we split <em>along the layers</em>). That is <em>inter-layer</em> parallelism. An exaggeration would be having a model with 2 hidden layers and 1 output layer split across 3 GPUs.</li>
<li>Tensor Parallelism (<strong>TP</strong>): We split the layers of the model across different ranks, that’s <em>intra-layer</em> parallelism. This could be useful if some layers are so large they don’t even fit in a single device. To reduce overhead, it’s advisable that even if a layer is split across different <em>ranks</em>, all parts remain on the same <em>node</em>. (See <a href="../notes/distributed-training.html#terminology">terminology</a>)</li>
</ul>
<p>There’s also</p>
<ul>
<li>Expert Parallelism (<strong>EP</strong>): For Mixture-of-Experts (<strong>MoE</strong>) we can split the experts on different devices.</li>
</ul>
<p>We didn’t discuss much this last one but I knew about it already and searched a bit to know more about it.</p>
<p>Last but not least we discussed the issue of DataLoaders / Samplers, and the importance of data sharding. They are a mean of “splitting” efficiently our dataset so that each rank sees a different mini-batch of data. Also, each process only loads its <em>shard</em> so it’s more memory/communication efficient.</p>


</section>

 ]]></description>
  <category>python</category>
  <category>mle</category>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-09-04.html</guid>
  <pubDate>Thu, 04 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Fireside chat QA with Yuxiang Wei from Meta FAIR with S2S</title>
  <link>https://theopomies.com/journal/2025-09-03.html</link>
  <description><![CDATA[ 




<p>Well today I woke up at 5am to go on-site at my startup in Paris (I live 500km away, in Bordeaux) and got home at around 7pm, so not much free time to study, so I just read a bit in the train and ML related stuff was just the evening fireside chat.</p>
<section id="fireside-chat-with-yuxiang-wei-from-meta-fair-s2s" class="level3">
<h3 class="anchored" data-anchor-id="fireside-chat-with-yuxiang-wei-from-meta-fair-s2s">Fireside chat with Yuxiang Wei from Meta FAIR (S2S)</h3>
<p>As part of the course (conference) Scratch To Scale, we have a lot of very qualified engineers and researchers from top labs / companies dedicating time for guest lectures or QA sessions.</p>
<p>Today we had the chance to listen to Yuxiang Wei — of Meta FAIR, of the SWE-RL paper — and ask him loads of questions. We discussed topics such as {pre, mid, post}-training, training infrastructure, the advent of RL and its impacts on said infrastructure and more.</p>


</section>

 ]]></description>
  <category>mle</category>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-09-03.html</guid>
  <pubDate>Wed, 03 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Starting Scratch To Scale (S2S) and Dive into Deep Learning (D2L)</title>
  <link>https://theopomies.com/journal/2025-09-02.html</link>
  <description><![CDATA[ 




<p>Today I revised the basics of calculus on D2L, and ended the day with the first “lesson” on Distributed Training (S2S) by <a href="https://www.x.com/thezachmueller">Zach Mueller</a>.</p>
<section id="calculus" class="level3">
<h3 class="anchored" data-anchor-id="calculus">Calculus</h3>
<p>I worked a bit on calculus, and there’s always something to learn, even we you go as far back as the high-school level stuff. Small example, it’s just today that I realized that <img src="https://latex.codecogs.com/png.latex?%5Cdfrac%7Bdx%7D%7Bdy%7D%20=%20%5Cdfrac%7B1%7D%7B%5Cfrac%7Bdy%7D%7Bdx%7D%7D"> (cf.&nbsp;the definition of derivative as a limit).</p>
<p>I also got my hands back into multivariate calculus and learning useful identities.</p>
</section>
<section id="distributed-training-s2s" class="level3">
<h3 class="anchored" data-anchor-id="distributed-training-s2s">Distributed Training (S2S)</h3>
<p>Finally, I finished the day learning the basics of distributed/parallel processing/training on GPUs (using <code>torch.distributed</code>, we’re not yet at the triton or CUDA level, but someday we’ll be there, just watch).</p>
<p>We went from the <a href="../notes/distributed-training.html#basics-point-point">primitives</a> — (i)send and (i)recv — to the <a href="../notes/distributed-training.html#collective-operations">collective operations</a> — reduce, all_reduce, scatter, reduce_scatter, broadcast, barrier, all2all, gather, all_gather. I can now much more easily conceive how distributed training algorithms work.</p>
<p>I learned a few distributed training concepts, such as the <em>rank</em>.</p>
<p>I concluded the day by running my first notebooks accelerated by more than 1 GPU on <a href="https://www.modal.com" target="_blank">Modal</a>. I’d done some lightly GPU accelerated stuff on <a href="https://www.kaggle.com" target="_blank">Kaggle</a>, but now I could grasp how to do stuff with multiple GPUs.</p>


</section>

 ]]></description>
  <category>math</category>
  <category>python</category>
  <category>mle</category>
  <guid>https://theopomies.com/journal/2025-09-02.html</guid>
  <pubDate>Tue, 02 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Finished reading The Manga Guide to Linear Algebra</title>
  <link>https://theopomies.com/journal/2025-09-01.html</link>
  <description><![CDATA[ 




<p>I had a lot of work to do for my day job, we are trying to build and commercialize V2 of <a href="https://www.kivala.fr" target="_blank">our intercom system</a>, and I was faced with technical difficulties that left me very little time to work on math / ML. I still found the time to finish The Manga Guide to Linear Algebra, and learned a few important concepts and techniques!</p>
<section id="still-i-did-a-few-things" class="level3">
<h3 class="anchored" data-anchor-id="still-i-did-a-few-things">Still, I did a <em>few</em> things</h3>
<p>TIL about [subspaces], [linear spans], the [rank] of a matrix and a technique to find it via <a href="../notes/linear-algebra.html#gaussian-elimination">Gaussian Eliminination</a> — as usual, linear algebra appears to be the art of applying Gaussian Elimination correctly —, [Eigenvalues] and [Eigenvectors]! That was still packed!</p>
<p>I skimmed through the materials for <a href="https://maven.com/walk-with-code/scratch-to-scale" target="_blank">Scratch To Scale</a>, the course I’m following by <a href="https://x.com/thezachmueller" target="_blank">Scott Mueller</a>, beginning tomorrow.</p>


</section>

 ]]></description>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-09-01.html</guid>
  <pubDate>Mon, 01 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Started reading The Manga Guide to Linear Algebra</title>
  <link>https://theopomies.com/journal/2025-08-29.html</link>
  <description><![CDATA[ 




<section id="the-manga-guide-to-linear-algebra" class="level2">
<h2 class="anchored" data-anchor-id="the-manga-guide-to-linear-algebra">The Manga Guide to Linear Algebra</h2>
<section id="why" class="level3">
<h3 class="anchored" data-anchor-id="why">Why?</h3>
<p>Yes, this might sound weird if you don’t know the concept, but hear me out. After seing it praised on Twitter/X by an anon, I got myself a copy of <a href="https://www.amazon.com/Manga-Guide-Linear-Algebra/dp/1593274130" target="_blank">The Manga Guide to Linear Algebra</a>.</p>
<p>It’s pretty good if you want to quickly get up to speed, develop intuition, but does not go too deep into formal proofs.</p>
<p>You might not know me, but before going to a Software Engineering School, I studied math for 2 years in Uni, and dropped out because it was too “theorical”, not concrete enough for me, I needed to <em>grok</em> the formulas, to visualize, feel the proofs naturally and see the applications. Sadly, partly due to how Uni works — partly due to playing League of Legends too much — I did not get the level of understanding required for my attention to stick.</p>
<p>Now that I’ve become a SWE, I’ve seen the practical uses of such Math, and I now find myself <em>needing</em> more of it! So, step by step I’ll try to go from intuition and basics (this book) -&gt; implementing existing algorithms -&gt; proofs and more advanced formulas and theorem (some other textbook like <a href="https://linear.axler.net/LADR4e.pdf" target="_blank">Linear Algebra Done Right</a>) -&gt; understanding ML architecture choices -&gt; being able to invent/discover new arch and techniques?</p>
</section>
<section id="what-i-learned-so-far" class="level3">
<h3 class="anchored" data-anchor-id="what-i-learned-so-far">What I learned so far</h3>
<p>Being somewhat familiar with Linear Algebra and computer science, I already knew a few things, so I read pretty fast through the first bits.</p>
<p>The first thing I learned were the techniques for computing the determinant for matrices of dimensions 4 and above. There’s a technique based on the generalisation of the formula <img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BA%7D)%20=%20a_%7B11%7Da_%7B22%7D%20-%20a_%7B12%7Da_%7B21%7D">.</p>
<p>Along with the interpretation/intuition for the determinant (consolidated by 3b1b).</p>
<p>But there’s also <a href="../notes/linear-algebra.html#lu-decomposition-or-lu-factorization">an algorithm (LU decomposition)</a> used in computer science based on a set of rules for determinant and gaussian reduction.</p>


</section>
</section>

 ]]></description>
  <category>math</category>
  <guid>https://theopomies.com/journal/2025-08-29.html</guid>
  <pubDate>Fri, 29 Aug 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Setting up this blog!</title>
  <link>https://theopomies.com/journal/2025-08-28.html</link>
  <description><![CDATA[ 




<section id="this-blog" class="level2">
<h2 class="anchored" data-anchor-id="this-blog">This “Blog”</h2>
<p>Today’s the day!</p>
<p>I’ve decided to document my learning process to “transition” from SWE / using AI <img src="https://latex.codecogs.com/png.latex?%5Clongrightarrow"> MLE/Applied Research/ building and training AI.</p>
<p>After experimenting with <a href="https://marimo.io" target="_blank">marimo</a>, loving it and <a href="https://github.com/marimo-team/marimo/issues/6172" target="_blank">struggling</a> to make a blog out of marimo notebooks and experiments, I’ve stumbled upon <a href="https://www.fast.ai/posts/2020-01-20-nb2md.html" target="_blank">this article</a> by <a href="https://x.com/jeremyphoward" target="_blank">Jeremy Howard</a> encouraging me to write my blog via notebooks + <a href="https://quarto.org" target="_blank">Quarto</a>.</p>
<p>It also happens that I had just purchased the <a href="https://maven.com/walk-with-code/scratch-to-scale" target="_blank">Scratch To Scale course/conference</a> by the brilliant <a href="https://x.com/thezachmueller" target="_blank">Zach Mueller</a> and <a href="https://muellerzr.github.io" target="_blank">his blog</a> was also using Quarto — which I would soon learn was no coincidence, Zach and Jeremy were closed and Zach contributed to Jeremy’s framework, <a href="https://www.fast.ai" target="_blank">fast.ai</a>.</p>
<p>So there it is, I intend to document my learning process, render notebooks of experiments, store knowledge, formulas I’ve finally understood even if I’m the last person on earth to do so, bookmarks, reading notes etc.</p>


</section>

 ]]></description>
  <category>python</category>
  <guid>https://theopomies.com/journal/2025-08-28.html</guid>
  <pubDate>Thu, 28 Aug 2025 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
