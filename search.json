[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Theo POMIES",
    "section": "",
    "text": "I’m Théo, a graduate of EPITECH and currently CTO of a tech startup, Kivala. My background is in AI, large language models, backend development, and data engineering. Over the past years, I’ve worked on building robust backends with Python and FastAPI, designing automation pipelines, and deploying AI systems ranging from custom LLMs to retrieval-augmented generation (RAG) architectures.\nMy technical work usually revolves around:"
  },
  {
    "objectID": "index.html#transition-to-mle-applied-research",
    "href": "index.html#transition-to-mle-applied-research",
    "title": "Theo POMIES",
    "section": "Transition to MLE & Applied Research",
    "text": "Transition to MLE & Applied Research\nI’m now in the process of transitioning towards machine learning engineering and applied research. This blog is where I’ll share my journey — notes from my learning process, experiments, and articles about the concepts and techniques I find valuable along the way.\nIt’s part notebook, part knowledge base, and part exploration. My goal is to document both the practice of building systems and the research side of AI that makes them possible."
  },
  {
    "objectID": "notes/typescript.html",
    "href": "notes/typescript.html",
    "title": "On Typescript",
    "section": "",
    "text": "Let your types be flexible\nLet your constraints be rigid"
  },
  {
    "objectID": "notes/typescript.html#first-principles",
    "href": "notes/typescript.html#first-principles",
    "title": "On Typescript",
    "section": "",
    "text": "Let your types be flexible\nLet your constraints be rigid"
  },
  {
    "objectID": "notes/typescript.html#tldr-rules",
    "href": "notes/typescript.html#tldr-rules",
    "title": "On Typescript",
    "section": "TL;DR Rules",
    "text": "TL;DR Rules\n\nUse unknown instead of any, then use type narrowing to get the correct type.\nUse type over interface, unless you actually need to reach for an interface or need to express objects/class inheritance.\nAvoid using as to assert types, most of the time you actually want to narrow the type with checks (if/else).\nUse array.at(index) instead of array[index] unless array is a tuple (fixed size array).\nNEVER use TS specifics (enum, private in constructor, etc.)."
  },
  {
    "objectID": "notes/typescript.html#recommandations",
    "href": "notes/typescript.html#recommandations",
    "title": "On Typescript",
    "section": "Recommandations",
    "text": "Recommandations\n\nUse satisfies to check if an object fits a type but not erase the type.\nUse as const whenever possible. (Immutable data, enum-like objects, etc.)\nDefine (and export) types where they are consumed, and import them from other files if needed."
  },
  {
    "objectID": "notes/typescript.html#explanations",
    "href": "notes/typescript.html#explanations",
    "title": "On Typescript",
    "section": "Explanations",
    "text": "Explanations\n\nNarrowing over using as\nSuppose you have a function that takes a number\nfunction double(a: number) {\n    return a * 2;\n}\nAnd you have a variable that could be a number or a string\nfunction getNumberOrString(): number | string {\n    return Math.random() &gt; 0.5 ? 1 : \"1\";\n}\nconst a: number | string = getNumberOrString();\nTypescript will allow you to use as to assert the variable to a number (this is one of the ways that TypeScript is not sound)\nconst result: number = double(a as number);\nBut this not correct/sound at runtime!\nThe correct way to do this is to narrow the type with a check (if/else/early return).\nif (typeof a === \"number\") {\n    const result = double(a);\n}"
  },
  {
    "objectID": "notes/calculus.html",
    "href": "notes/calculus.html",
    "title": "Calculus",
    "section": "",
    "text": "As the name states, we analyze differences in the output(s) of a function based on differences in the input(s) \\[ \\dfrac{dy}{dx} = \\lim_{h \\rightarrow 0} \\dfrac{f(x + h) - f(x)}{h} \\]\nA function is said to be differentiable at \\(x\\) if this limit exists, and differentiable on an interval if it exists at any \\(x\\) in this interval.\n\n\nFrom that we can get common derivatives\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}C & = 0 && \\text{for any constant C} \\\\\n    \\dfrac{d}{dx}x^n & = nx^{n - 1} && \\text{for n} \\neq 0 \\\\\n    \\dfrac{d}{dx}e^x & = e^x \\\\\n    \\dfrac{d}{dx}\\ln x & = x^{-1} \\\\\n    \\dfrac{d}{dx}a^x & = \\ln(a)a^x \\\\\n    \\dfrac{d}{dx}\\cos x & = -\\sin x \\\\\n    \\dfrac{d}{dx}\\sin x & = \\cos x \\\\\n\\end{aligned} \\]\nFrom these it becomes trivial to derive \\(\\tan\\), \\(\\sec\\), \\(\\csc\\) and \\(\\cot\\).\n\n\n\nFinally a composition of differentiable functions is also differentiable, so we have the following rules that allow us to derive almost any function:\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}Cf(x) & = C\\dfrac{d}{dx}f(x) && \\text{Constant multiple rule} \\\\\n    \\dfrac{d}{dx}[f(x) + g(x)] & = \\dfrac{d}{dx}f(x) + \\dfrac{d}{dx}g(x) && \\text{Sum rule} \\\\\n    \\dfrac{d}{dx}[f(x)g(x)] & = \\dfrac{d}{dx}f(x)g(x) + f(x)\\dfrac{d}{dx}g(x) && \\text{Product rule} \\\\\n    \\dfrac{dy}{dx} & = \\dfrac{dy}{dz}\\dfrac{dz}{dx} = \\dfrac{\\frac{dy}{dz}}{\\frac{dx}{dz}} && \\text{Chain rule} \\\\\n\\end{aligned} \\]\nFrom these we can easily find a Quotient Rule, a Power Rule and a Reciprocal Rule:\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}\\dfrac{f(x)}{g(x)} & = \\dfrac{\\frac{d}{dx}f(x)g(x) - f(x)\\frac{d}{dx}g(x)}{g(x)^2} && \\text{Quotient rule} \\\\\n    \\dfrac{d}{dx}f(x)^n & = nf(x)^{n-1}\\dfrac{d}{dx}f(x) && \\text{Power rule} \\\\\n    \\dfrac{dy}{dx}\\dfrac{1}{f(x)} & = -\\dfrac{\\frac{d}{dx}f(x)}{f(x)^2} && \\text{Reciprocal rule} \\\\\n\\end{aligned} \\]\n\n\n\n\n\n\nNote\n\n\n\nBecause of the definition of derivative as a rate of change, this is possible \\(\\dfrac{dy}{dx} = \\dfrac{1}{\\frac{dx}{dy}}\\)\n\n\n\n\n\n\n\n\nVery similar to univariate calculus, but now our function takes a vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\) as input and returns a scalar \\(y \\in \\mathbb{R}\\).\nTo paraphrase D2L because their explanation is perfect:\nLet \\(y = f(x_1, x_2, \\ldots, x_n)\\) be a function with \\(n\\) variables. The partial derivative of \\(y\\) with respect to its \\(i^\\textrm{th}\\) parameter \\(x_i\\) is\n\\[ \\dfrac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.\\]\nFor \\(f \\colon \\mathbb{R}^n \\to \\mathbb{R}\\), we collect/concatenate all the partial derivatives to obtain the gradient of the output \\(y = f(\\mathbf{x})\\) with respect to the input \\(\\mathbf{x}\\) \\[ \\nabla_{\\mathbf{x}}f(\\mathbf{x}) = \\nabla_{\\mathbf{x}}y =\\begin{bmatrix} \\frac{\\partial y}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial y}{\\partial x_n} \\end{bmatrix} \\] sometimes written \\(\\nabla f(\\mathbf{x})\\) or \\(\\nabla y\\) when not ambiguous.\n\n\n\nThe Jacobian \\(\\mathbf{J} \\in \\mathbb{R}^{m \\times n}\\) is a generalization of the gradient to \\(\\mathbf{y} = f(\\mathbf{x})\\) with \\(f \\colon \\mathbb{R}^n \\to \\mathbb{R}^m\\), where \\(j_{i,j} = \\dfrac{\\partial y_i}{\\partial\\mathbf{x}_j}\\), \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\in \\mathbb{R}^n\\) and \\(\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_m \\end{bmatrix} \\in \\mathbb{R}^m\\).\nExplicitly \\[ \\displaystyle{ \\mathbf{J} = \\begin{bmatrix} \\dfrac{\\partial \\mathbf{y}}{\\partial x_{1}} & \\cdots & \\dfrac{\\partial \\mathbf{y}}{\\partial x_{n}} \\end{bmatrix}\n= \\begin{bmatrix} \\nabla^{\\top}y_{1} \\\\ \\vdots \\\\ \\nabla^{\\top}y_{m} \\end{bmatrix}\n= \\begin{bmatrix}\n\\dfrac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\dfrac{\\partial y_{1}}{\\partial x_{n}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\dfrac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\dfrac{\\partial y_{m}}{\\partial x_{n}}\n\\end{bmatrix}} \\]\n\n\n\nThe following rules come straight from D2L:\n\nFor all \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) we have \\(\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top\\) and \\(\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}\\).\nFor square matrices \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) we have that \\(\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}\\) and in particular \\(\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}\\).\n\nThen the chain rule states that\n\\[\\frac{\\partial y}{\\partial x_{i}} = \\frac{\\partial y}{\\partial u_{1}} \\frac{\\partial u_{1}}{\\partial x_{i}} + \\frac{\\partial y}{\\partial u_{2}} \\frac{\\partial u_{2}}{\\partial x_{i}} + \\ldots + \\frac{\\partial y}{\\partial u_{m}} \\frac{\\partial u_{m}}{\\partial x_{i}} \\ \\textrm{ and so } \\ \\nabla_{\\mathbf{x}} y =  \\mathbf{A} \\nabla_{\\mathbf{u}} y,\\]\nwhere \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times m}\\) is a matrix that contains the derivative of vector \\(\\mathbf{u}\\) with respect to vector \\(\\mathbf{x}\\).\n\n\n\n\nIntegrals are\n\na way to compute the signed area under a curve\nantiderivatives\na way of adding up tiny bits\n\n\\[ \\int_a^b f(x)\\,dx \\]\n\n\nThe integral \\[\\int_a^b f(x)\\,dx\\] is the limit of sums of tiny rectangular areas.\nIf we cut the interval \\([a,b]\\) into \\(n\\) equal chunks of width \\[\\Delta x = \\frac{b-a}{n},\\] then the total area is approximated by \\[\\sum_{k=1}^{n} f(a + k\\Delta x)\\,\\Delta x.\\]\nAs we make the chunks thinner (\\(n \\to \\infty\\), so \\(\\Delta x \\to 0\\)), this sum becomes exact:\n\\[\\int_a^b f(x)\\,dx = \\lim_{n\\to\\infty} \\sum_{k=1}^{n} f(a + k\\Delta x)\\,\\Delta x.\\]\n\\(\\int_a^b f(x)\\,dx\\) is a definite integral of \\(f(x)\\) from \\([a,b]\\).\n\\(\\int f(x)\\,dx\\) is an indefinite integral.\n\n\n\n\nThe fundamental theorem of calculus links differentiation (derivatives) and integration (integrals).\n\\[ \\int_a^b f(x)\\,dx = F(b) - F(a)\\]\nWhere\n\\[ \\dfrac{d}{dx}F(x) = f(x) \\]\n\n\nSay we have a function \\(A(x)\\) being the area under the curve of \\(f(x)\\) between \\(0\\) and \\(x\\).\nto find the area under the curve between \\(x\\) and \\(x+h\\), we could compute \\[ A(x+h) - A(x) \\approx f(x)h \\] \\[ \\iff \\dfrac{A(x+h) - A(x)}{h} \\approx f(x) \\] \\[ \\iff \\lim_{h \\to 0} \\dfrac{A(x+h) - A(x)}{h} = f(x) \\] \\[ \\iff \\dfrac{d}{dx}A(x) = f(x) \\]\n\n\n\n\\(dx\\) = the differential of \\(x\\)\nA single symbol that means “infinitesimal change in \\(x\\).”\nIn derivatives, it appears in a ratio (\\(\\dfrac{dy}{dx}\\)). We are differentiating \\(f(x)\\) wrt \\(x\\).\nIn integrals, it appears as a piece being added up (\\(f(x)\\,dx\\)). We are integrating \\(f(x)\\) wrt \\(x\\).\n\n\n\n\nIf \\(\\lim_{x \\to c} f(x) = \\lim_{x \\to c} g(x) = 0\\) or \\(\\pm \\infty\\), and \\(\\lim_{x \\to c} \\dfrac{\\frac{d}{dx}f(x)}{\\frac{d}{dx}g(x)}\\) exists, then\n\\[\\lim_{x \\to c} \\dfrac{f(x)}{g(x)} = \\lim_{x \\to c} \\dfrac{\\frac{d}{dx}f(x)}{\\frac{d}{dx}g(x)}\\]\n\n\n\nWe can use \\[a = b \\implies \\dfrac{d}{dx}a = \\dfrac{d}{dx}b\\] to compute derivatives of relations (linked variables eg. the equation for a circle centered at the origin and of radius 5: \\(x^2 + y^2 = 25\\))\nOr even to compute the derivative of \\(ln(x)\\):\n\\[\\begin{aligned}\ny = ln(x) & \\iff e^y = x \\implies \\dfrac{d}{dx} e^y = \\dfrac{d}{dx} x \\\\\n& \\iff \\dfrac{d}{dx} e^y = 1 \\\\\n& \\iff e^y\\dfrac{d}{dx}y = 1 \\\\\n& \\iff \\dfrac{d}{dx}y = \\dfrac{1}{e^y} \\\\\n& \\iff \\dfrac{d}{dx}y = \\dfrac{1}{e^{ln(x)}} \\\\\n& \\iff \\dfrac{d}{dx}y = \\dfrac{1}{x}\n\\end{aligned}\\]"
  },
  {
    "objectID": "notes/calculus.html#definitions-formulas",
    "href": "notes/calculus.html#definitions-formulas",
    "title": "Calculus",
    "section": "",
    "text": "As the name states, we analyze differences in the output(s) of a function based on differences in the input(s) \\[ \\dfrac{dy}{dx} = \\lim_{h \\rightarrow 0} \\dfrac{f(x + h) - f(x)}{h} \\]\nA function is said to be differentiable at \\(x\\) if this limit exists, and differentiable on an interval if it exists at any \\(x\\) in this interval.\n\n\nFrom that we can get common derivatives\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}C & = 0 && \\text{for any constant C} \\\\\n    \\dfrac{d}{dx}x^n & = nx^{n - 1} && \\text{for n} \\neq 0 \\\\\n    \\dfrac{d}{dx}e^x & = e^x \\\\\n    \\dfrac{d}{dx}\\ln x & = x^{-1} \\\\\n    \\dfrac{d}{dx}a^x & = \\ln(a)a^x \\\\\n    \\dfrac{d}{dx}\\cos x & = -\\sin x \\\\\n    \\dfrac{d}{dx}\\sin x & = \\cos x \\\\\n\\end{aligned} \\]\nFrom these it becomes trivial to derive \\(\\tan\\), \\(\\sec\\), \\(\\csc\\) and \\(\\cot\\).\n\n\n\nFinally a composition of differentiable functions is also differentiable, so we have the following rules that allow us to derive almost any function:\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}Cf(x) & = C\\dfrac{d}{dx}f(x) && \\text{Constant multiple rule} \\\\\n    \\dfrac{d}{dx}[f(x) + g(x)] & = \\dfrac{d}{dx}f(x) + \\dfrac{d}{dx}g(x) && \\text{Sum rule} \\\\\n    \\dfrac{d}{dx}[f(x)g(x)] & = \\dfrac{d}{dx}f(x)g(x) + f(x)\\dfrac{d}{dx}g(x) && \\text{Product rule} \\\\\n    \\dfrac{dy}{dx} & = \\dfrac{dy}{dz}\\dfrac{dz}{dx} = \\dfrac{\\frac{dy}{dz}}{\\frac{dx}{dz}} && \\text{Chain rule} \\\\\n\\end{aligned} \\]\nFrom these we can easily find a Quotient Rule, a Power Rule and a Reciprocal Rule:\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}\\dfrac{f(x)}{g(x)} & = \\dfrac{\\frac{d}{dx}f(x)g(x) - f(x)\\frac{d}{dx}g(x)}{g(x)^2} && \\text{Quotient rule} \\\\\n    \\dfrac{d}{dx}f(x)^n & = nf(x)^{n-1}\\dfrac{d}{dx}f(x) && \\text{Power rule} \\\\\n    \\dfrac{dy}{dx}\\dfrac{1}{f(x)} & = -\\dfrac{\\frac{d}{dx}f(x)}{f(x)^2} && \\text{Reciprocal rule} \\\\\n\\end{aligned} \\]\n\n\n\n\n\n\nNote\n\n\n\nBecause of the definition of derivative as a rate of change, this is possible \\(\\dfrac{dy}{dx} = \\dfrac{1}{\\frac{dx}{dy}}\\)\n\n\n\n\n\n\n\n\nVery similar to univariate calculus, but now our function takes a vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\) as input and returns a scalar \\(y \\in \\mathbb{R}\\).\nTo paraphrase D2L because their explanation is perfect:\nLet \\(y = f(x_1, x_2, \\ldots, x_n)\\) be a function with \\(n\\) variables. The partial derivative of \\(y\\) with respect to its \\(i^\\textrm{th}\\) parameter \\(x_i\\) is\n\\[ \\dfrac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.\\]\nFor \\(f \\colon \\mathbb{R}^n \\to \\mathbb{R}\\), we collect/concatenate all the partial derivatives to obtain the gradient of the output \\(y = f(\\mathbf{x})\\) with respect to the input \\(\\mathbf{x}\\) \\[ \\nabla_{\\mathbf{x}}f(\\mathbf{x}) = \\nabla_{\\mathbf{x}}y =\\begin{bmatrix} \\frac{\\partial y}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial y}{\\partial x_n} \\end{bmatrix} \\] sometimes written \\(\\nabla f(\\mathbf{x})\\) or \\(\\nabla y\\) when not ambiguous.\n\n\n\nThe Jacobian \\(\\mathbf{J} \\in \\mathbb{R}^{m \\times n}\\) is a generalization of the gradient to \\(\\mathbf{y} = f(\\mathbf{x})\\) with \\(f \\colon \\mathbb{R}^n \\to \\mathbb{R}^m\\), where \\(j_{i,j} = \\dfrac{\\partial y_i}{\\partial\\mathbf{x}_j}\\), \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\in \\mathbb{R}^n\\) and \\(\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_m \\end{bmatrix} \\in \\mathbb{R}^m\\).\nExplicitly \\[ \\displaystyle{ \\mathbf{J} = \\begin{bmatrix} \\dfrac{\\partial \\mathbf{y}}{\\partial x_{1}} & \\cdots & \\dfrac{\\partial \\mathbf{y}}{\\partial x_{n}} \\end{bmatrix}\n= \\begin{bmatrix} \\nabla^{\\top}y_{1} \\\\ \\vdots \\\\ \\nabla^{\\top}y_{m} \\end{bmatrix}\n= \\begin{bmatrix}\n\\dfrac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\dfrac{\\partial y_{1}}{\\partial x_{n}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\dfrac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\dfrac{\\partial y_{m}}{\\partial x_{n}}\n\\end{bmatrix}} \\]\n\n\n\nThe following rules come straight from D2L:\n\nFor all \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) we have \\(\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top\\) and \\(\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}\\).\nFor square matrices \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) we have that \\(\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}\\) and in particular \\(\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}\\).\n\nThen the chain rule states that\n\\[\\frac{\\partial y}{\\partial x_{i}} = \\frac{\\partial y}{\\partial u_{1}} \\frac{\\partial u_{1}}{\\partial x_{i}} + \\frac{\\partial y}{\\partial u_{2}} \\frac{\\partial u_{2}}{\\partial x_{i}} + \\ldots + \\frac{\\partial y}{\\partial u_{m}} \\frac{\\partial u_{m}}{\\partial x_{i}} \\ \\textrm{ and so } \\ \\nabla_{\\mathbf{x}} y =  \\mathbf{A} \\nabla_{\\mathbf{u}} y,\\]\nwhere \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times m}\\) is a matrix that contains the derivative of vector \\(\\mathbf{u}\\) with respect to vector \\(\\mathbf{x}\\).\n\n\n\n\nIntegrals are\n\na way to compute the signed area under a curve\nantiderivatives\na way of adding up tiny bits\n\n\\[ \\int_a^b f(x)\\,dx \\]\n\n\nThe integral \\[\\int_a^b f(x)\\,dx\\] is the limit of sums of tiny rectangular areas.\nIf we cut the interval \\([a,b]\\) into \\(n\\) equal chunks of width \\[\\Delta x = \\frac{b-a}{n},\\] then the total area is approximated by \\[\\sum_{k=1}^{n} f(a + k\\Delta x)\\,\\Delta x.\\]\nAs we make the chunks thinner (\\(n \\to \\infty\\), so \\(\\Delta x \\to 0\\)), this sum becomes exact:\n\\[\\int_a^b f(x)\\,dx = \\lim_{n\\to\\infty} \\sum_{k=1}^{n} f(a + k\\Delta x)\\,\\Delta x.\\]\n\\(\\int_a^b f(x)\\,dx\\) is a definite integral of \\(f(x)\\) from \\([a,b]\\).\n\\(\\int f(x)\\,dx\\) is an indefinite integral.\n\n\n\n\nThe fundamental theorem of calculus links differentiation (derivatives) and integration (integrals).\n\\[ \\int_a^b f(x)\\,dx = F(b) - F(a)\\]\nWhere\n\\[ \\dfrac{d}{dx}F(x) = f(x) \\]\n\n\nSay we have a function \\(A(x)\\) being the area under the curve of \\(f(x)\\) between \\(0\\) and \\(x\\).\nto find the area under the curve between \\(x\\) and \\(x+h\\), we could compute \\[ A(x+h) - A(x) \\approx f(x)h \\] \\[ \\iff \\dfrac{A(x+h) - A(x)}{h} \\approx f(x) \\] \\[ \\iff \\lim_{h \\to 0} \\dfrac{A(x+h) - A(x)}{h} = f(x) \\] \\[ \\iff \\dfrac{d}{dx}A(x) = f(x) \\]\n\n\n\n\\(dx\\) = the differential of \\(x\\)\nA single symbol that means “infinitesimal change in \\(x\\).”\nIn derivatives, it appears in a ratio (\\(\\dfrac{dy}{dx}\\)). We are differentiating \\(f(x)\\) wrt \\(x\\).\nIn integrals, it appears as a piece being added up (\\(f(x)\\,dx\\)). We are integrating \\(f(x)\\) wrt \\(x\\).\n\n\n\n\nIf \\(\\lim_{x \\to c} f(x) = \\lim_{x \\to c} g(x) = 0\\) or \\(\\pm \\infty\\), and \\(\\lim_{x \\to c} \\dfrac{\\frac{d}{dx}f(x)}{\\frac{d}{dx}g(x)}\\) exists, then\n\\[\\lim_{x \\to c} \\dfrac{f(x)}{g(x)} = \\lim_{x \\to c} \\dfrac{\\frac{d}{dx}f(x)}{\\frac{d}{dx}g(x)}\\]\n\n\n\nWe can use \\[a = b \\implies \\dfrac{d}{dx}a = \\dfrac{d}{dx}b\\] to compute derivatives of relations (linked variables eg. the equation for a circle centered at the origin and of radius 5: \\(x^2 + y^2 = 25\\))\nOr even to compute the derivative of \\(ln(x)\\):\n\\[\\begin{aligned}\ny = ln(x) & \\iff e^y = x \\implies \\dfrac{d}{dx} e^y = \\dfrac{d}{dx} x \\\\\n& \\iff \\dfrac{d}{dx} e^y = 1 \\\\\n& \\iff e^y\\dfrac{d}{dx}y = 1 \\\\\n& \\iff \\dfrac{d}{dx}y = \\dfrac{1}{e^y} \\\\\n& \\iff \\dfrac{d}{dx}y = \\dfrac{1}{e^{ln(x)}} \\\\\n& \\iff \\dfrac{d}{dx}y = \\dfrac{1}{x}\n\\end{aligned}\\]"
  },
  {
    "objectID": "notes/calculus.html#proofs",
    "href": "notes/calculus.html#proofs",
    "title": "Calculus",
    "section": "Proofs",
    "text": "Proofs\nLater!"
  },
  {
    "objectID": "notes/calculus.html#notation",
    "href": "notes/calculus.html#notation",
    "title": "Calculus",
    "section": "Notation",
    "text": "Notation\n\n\\(f(\\cdot)\\): a function\n\\(\\dfrac{dy}{dx}\\): derivative of \\(y\\) with respect to \\(x\\)\n\\(\\dfrac{\\partial y}{\\partial x}\\): partial derivative of \\(y\\) with respect to \\(x\\)\n\\(\\nabla_{\\mathbf{x}} y\\): gradient of \\(y\\) with respect to \\(\\mathbf{x}\\)\n\\(\\mathbf{J}_f (\\mathbf{x}), \\dfrac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\): Jacobian of \\(\\mathbf{y} = f(\\mathbf{x})\\) with respect to \\(\\mathbf{x}\\)\n\\(\\int_a^b f(x) \\;dx\\): definite integral of \\(f\\) from \\(a\\) to \\(b\\) with respect to \\(x\\)\n\\(\\int f(x) \\;dx\\): indefinite integral of \\(f\\) with respect to \\(x\\)"
  },
  {
    "objectID": "notes/distributed-training.html",
    "href": "notes/distributed-training.html",
    "title": "Distributed Training",
    "section": "",
    "text": "send and recv to send or receive a tensor synchronously — from/to a single rank.\nAnd their async counterparts, isend and irecv.\n\nrank = dist.get_rank()\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\nif rank == 0:\n    request = dist.isend(tensor, 1)\n    ...\n    # can do something else, like more sends for example!\n    ...\n    request.wait() # now block until it's been fulfilled\nelif rank == 1:\n    dist.recv(tensor, 0) # recv is synchronous, so it will block until tensor is fully received\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n\n\n\n\nCollective operations allow communication (data-transfer) from All-&gt;Point, Point-&gt;All and All-&gt;All.\n\n\n\n\nBroadcast (torch.distributed.broadcast(tensor, src, ...)) allows a rank to broadcast a tensor to the whole group.\n\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\ndist.broadcast(tensor, src=0)\n\nprint(f\"After: {tensor})\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 2 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n\n\n\n\nScatter (torch.distributed.scatter(tensor, scatter_list, src, ...)) allows us to scatter — split and broadcast different chunks of — a tensor from a rank to the whole group.\n\ntensor = torch.zeros(3)\nscatter_list = [torch.arange(3 * i, 3 * i + 3) if rank == 0 else torch.zeros(3) for i in range(world_size)]\n\nprint(f\"Scatter list: {scatter_list}\")\nprint(f\"Before: {tensor}\")\n\ndist.scatter(tensor, scatter_list, src=0)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nScatter list: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nScatter list: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([3, 4, 5])\n\n\n\n\n\n\n\n\nReduce (torch.distributed.reduce(tensor, dst, op, ...)) performs a reduction operation (N-&gt;1, eg. sum, max, min, prod, …) and the dst rank receives the result.\n\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 4, 5])\n\n\n\n\n\nGather (torch.distributed.gather(tensor, gather_list, dst, ...)) gathers — pulls — a tensor, of the same size, from every rank and stores them in a list in a single rank.\n\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.gather(tensor, gather_list, dst=0)\n\nprint(f\"After: {gather_list}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([3, 4, 5])\n\n========== rank 1 ==========\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nAfter: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\n\n\n\n\n\n\n\n\nAll-Reduce (torch.distributed.all_reduce(tensor, op, ...)) performs a reduction operation, like reduce, but every rank receives the result — rather than a single one with reduce. Think of it as reduce + broadcast — though it is optimized by techniques like ring-reduce.\n\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 5, 7])\n\n\n\n\n\nAll-Gather (torch.distributed.all_gather(tensor, gather_list, ...)) gathers — pulls — a tensor, of the same size, from every rank and stores them in a list in every rank. Think of it as running gather on all ranks.\n\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.all_gather(tensor, gather_list)\n\nprint(f\"After: {gather_list}\")\n\n\n\n========== rank 2 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n\n\n\n\n\nReduce-Scatter (torch.distributed.reduce_scatter(output_tensor, input_list, op, ...)) performs a reduction operation — like other reduce functions — and scatters the resulting tensor. Think of it like reduce + scatter.\n\n\n\n\n\n\nNote\n\n\n\nIt needs len(input_list) == world_size and every tensor in input_list to have the same shape of output_tensor.\n\n\n\ntensor = torch.zeros(2)\nscatter_list = [torch.tensor([(rank + 1) * i for i in range(1, 3)]) ** (j + 1) for j in range(3)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {scatter_list}\")\n\ndist.reduce_scatter(tensor, scatter_list, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([1, 2]), tensor([1, 4]), tensor([1, 8])]\nAfter: tensor([ 6, 12])\n\n========== rank 1 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([2, 4]), tensor([ 4, 16]), tensor([ 8, 64])]\nAfter: tensor([14, 56])\n\n========== rank 2 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([3, 6]), tensor([ 9, 36]), tensor([ 27, 216])]\nAfter: tensor([ 36, 288])\n\n\n\n\n\n\n\n\nBarrier (torch.distributed.barrier(...)) synchronizes all ranks (processes), waiting for all of them to reach the barrier. (think of .join() for threads or processes)"
  },
  {
    "objectID": "notes/distributed-training.html#functions-methods",
    "href": "notes/distributed-training.html#functions-methods",
    "title": "Distributed Training",
    "section": "",
    "text": "send and recv to send or receive a tensor synchronously — from/to a single rank.\nAnd their async counterparts, isend and irecv.\n\nrank = dist.get_rank()\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\nif rank == 0:\n    request = dist.isend(tensor, 1)\n    ...\n    # can do something else, like more sends for example!\n    ...\n    request.wait() # now block until it's been fulfilled\nelif rank == 1:\n    dist.recv(tensor, 0) # recv is synchronous, so it will block until tensor is fully received\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n\n\n\n\nCollective operations allow communication (data-transfer) from All-&gt;Point, Point-&gt;All and All-&gt;All.\n\n\n\n\nBroadcast (torch.distributed.broadcast(tensor, src, ...)) allows a rank to broadcast a tensor to the whole group.\n\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\ndist.broadcast(tensor, src=0)\n\nprint(f\"After: {tensor})\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 2 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n\n\n\n\nScatter (torch.distributed.scatter(tensor, scatter_list, src, ...)) allows us to scatter — split and broadcast different chunks of — a tensor from a rank to the whole group.\n\ntensor = torch.zeros(3)\nscatter_list = [torch.arange(3 * i, 3 * i + 3) if rank == 0 else torch.zeros(3) for i in range(world_size)]\n\nprint(f\"Scatter list: {scatter_list}\")\nprint(f\"Before: {tensor}\")\n\ndist.scatter(tensor, scatter_list, src=0)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nScatter list: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nScatter list: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([3, 4, 5])\n\n\n\n\n\n\n\n\nReduce (torch.distributed.reduce(tensor, dst, op, ...)) performs a reduction operation (N-&gt;1, eg. sum, max, min, prod, …) and the dst rank receives the result.\n\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 4, 5])\n\n\n\n\n\nGather (torch.distributed.gather(tensor, gather_list, dst, ...)) gathers — pulls — a tensor, of the same size, from every rank and stores them in a list in a single rank.\n\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.gather(tensor, gather_list, dst=0)\n\nprint(f\"After: {gather_list}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([3, 4, 5])\n\n========== rank 1 ==========\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nAfter: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\n\n\n\n\n\n\n\n\nAll-Reduce (torch.distributed.all_reduce(tensor, op, ...)) performs a reduction operation, like reduce, but every rank receives the result — rather than a single one with reduce. Think of it as reduce + broadcast — though it is optimized by techniques like ring-reduce.\n\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 5, 7])\n\n\n\n\n\nAll-Gather (torch.distributed.all_gather(tensor, gather_list, ...)) gathers — pulls — a tensor, of the same size, from every rank and stores them in a list in every rank. Think of it as running gather on all ranks.\n\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.all_gather(tensor, gather_list)\n\nprint(f\"After: {gather_list}\")\n\n\n\n========== rank 2 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n\n\n\n\n\nReduce-Scatter (torch.distributed.reduce_scatter(output_tensor, input_list, op, ...)) performs a reduction operation — like other reduce functions — and scatters the resulting tensor. Think of it like reduce + scatter.\n\n\n\n\n\n\nNote\n\n\n\nIt needs len(input_list) == world_size and every tensor in input_list to have the same shape of output_tensor.\n\n\n\ntensor = torch.zeros(2)\nscatter_list = [torch.tensor([(rank + 1) * i for i in range(1, 3)]) ** (j + 1) for j in range(3)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {scatter_list}\")\n\ndist.reduce_scatter(tensor, scatter_list, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([1, 2]), tensor([1, 4]), tensor([1, 8])]\nAfter: tensor([ 6, 12])\n\n========== rank 1 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([2, 4]), tensor([ 4, 16]), tensor([ 8, 64])]\nAfter: tensor([14, 56])\n\n========== rank 2 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([3, 6]), tensor([ 9, 36]), tensor([ 27, 216])]\nAfter: tensor([ 36, 288])\n\n\n\n\n\n\n\n\nBarrier (torch.distributed.barrier(...)) synchronizes all ranks (processes), waiting for all of them to reach the barrier. (think of .join() for threads or processes)"
  },
  {
    "objectID": "notes/distributed-training.html#algorithms-techniques",
    "href": "notes/distributed-training.html#algorithms-techniques",
    "title": "Distributed Training",
    "section": "Algorithms / Techniques",
    "text": "Algorithms / Techniques\n\nTypes of parallelism\nThe goal of parallelism is to maximize throughput / cluster utilization.\n\nData Parallelism (DP): Each rank has a replica of the model — they’re replicants — and receives a different mini-batch. After [Gradient Accumulation] (optional), average the gradients (all_reduce).\nPipeline Parallelism (PP): The model is split along the layers. Each rank has 1+ layer of the model, and we orchestrate sequential forward/backward passes along the ranks. This is inter-layer parallelism.\nTensor Parallelism (TP): The model’s layers are split across ranks. We need more complex orchestration since a single tensor’s values are on different ranks. This is intra-layer parallelism.\nExpert Parallelism (EP): A specific type of TP where we only split the experts of an MoE across ranks.\n\n\n\n\n\n\n\nImportant\n\n\n\nZeRO/FSDP is not a parallelism strategy, but a modelling strategy.\nParallelism = optimizing throughput Modelling \\(\\approx\\) optimizing memory\nBoth can (and should) be combined.\n\n\n\n\nDDP — Distributed Data Parallelism\nDistributed Data Parallelism is a type of parallelism where each rank loads a copy — replica — of the model, after each optimizer step they always all have the same parameters, they are replicants. Each rank then trains on a different mini-batch (hence the importance of data sharding). We then average the gradients (all_reduce sum + division by world_size), perform a step of gradient descent, rinse and repeat. If we can use this, we should, it has the least amount of overhead, but it requires that the model + optimizer states all fit in the device’s VRAM.\n\n\n\n\n\n\nNote\n\n\n\nThe difference between DDP and DP is that DDP uses processes to avoid the GIL and DP uses threads. Do not use DP, only DDP.\n\n\nclass SimpleDataParaellism():\n    def __init__(self, model):\n        self.model = model\n\n        for param in model.parameters():\n            rank_0_params = param.data.clone()\n            dist.broadcast(rank_0_params, src=0)\n            assert torch.equal(param.data, rank_0_params), \"Parameters mismatch at initialization\"\n\n    def sync_grad(self):\n        for param in model.parameters():\n            dist.all_reduce(param.grad, op=dist.ReduceOp.SUM)\n            param.grad /= dist.get_world_size()\n\n\nData Sharding\nData Sharding is the process of sharding — splitting — the dataset / dataloader so that each rank only pulls their own unique mini-batches of the training data. This avoids duplicates and is more commucation / memory efficient that duplicating the same full dataset on every rank. To do this with torch, setup the DataLoader with sampler=[instance of DistributedSampler].\n\n\nZeRO\nZero Redudency Optimizer is a modeling strategy involving sharding states and parameters during training as a mean of optimizing peak memory. The core idea is that the states and/or parameters are sharded, retrieved only when necessary for some computation, updated information is shared, then anything we do not use anymore is discarded."
  },
  {
    "objectID": "notes/distributed-training.html#notes",
    "href": "notes/distributed-training.html#notes",
    "title": "Distributed Training",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "notes/distributed-training.html#terminology",
    "href": "notes/distributed-training.html#terminology",
    "title": "Distributed Training",
    "section": "Terminology",
    "text": "Terminology\n\ndevice: Hardware unit — GPU \"cuda:0\", CPU \"cpu\" etc. that’s where tensors and computations live\nnode: Phyisical machine/server (or VPS whatever) that has 1+ devices\nprocess: Python process/worker, executing a copy of the code/script — often on a single device (GPU)\nrank: ID of a process — often that maps to a single device. rank without qualifiers is global rank\nworld: Set of all processes part of our current distributed job\nglobal rank, world rank: rank across all processes/nodes. note: collective operations take the global rank (or just rank) as input for src/dst\nlocal rank: rank within a single node (node not group). note: device takes the local rank \"cuda:{local_rank}\"\ngroup: subset of processes (1+ nodes) that we’ve grouped for sub-communications. note: we still use global rank for intra-group communication."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "rss\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "journal/2025-09-02.html",
    "href": "journal/2025-09-02.html",
    "title": "Starting Scratch To Scale (S2S) and Dive into Deep Learning (D2L)",
    "section": "",
    "text": "Today I revised the basics of calculus on D2L, and ended the day with the first “lesson” on Distributed Training (S2S) by Zach Mueller.\n\nCalculus\nI worked a bit on calculus, and there’s always something to learn, even we you go as far back as the high-school level stuff. Small example, it’s just today that I realized that \\(\\dfrac{dx}{dy} = \\dfrac{1}{\\frac{dy}{dx}}\\) (cf. the definition of derivative as a limit).\nI also got my hands back into multivariate calculus and learning useful identities.\n\n\nDistributed Training (S2S)\nFinally, I finished the day learning the basics of distributed/parallel processing/training on GPUs (using torch.distributed, we’re not yet at the triton or CUDA level, but someday we’ll be there, just watch).\nWe went from the primitives — (i)send and (i)recv — to the collective operations — reduce, all_reduce, scatter, reduce_scatter, broadcast, barrier, all2all, gather, all_gather. I can now much more easily conceive how distributed training algorithms work.\nI learned a few distributed training concepts, such as the rank.\nI concluded the day by running my first notebooks accelerated by more than 1 GPU on Modal. I’d done some lightly GPU accelerated stuff on Kaggle, but now I could grasp how to do stuff with multiple GPUs."
  },
  {
    "objectID": "journal/2025-08-29.html",
    "href": "journal/2025-08-29.html",
    "title": "Started reading The Manga Guide to Linear Algebra",
    "section": "",
    "text": "Yes, this might sound weird if you don’t know the concept, but hear me out. After seing it praised on Twitter/X by an anon, I got myself a copy of The Manga Guide to Linear Algebra.\nIt’s pretty good if you want to quickly get up to speed, develop intuition, but does not go too deep into formal proofs.\nYou might not know me, but before going to a Software Engineering School, I studied math for 2 years in Uni, and dropped out because it was too “theorical”, not concrete enough for me, I needed to grok the formulas, to visualize, feel the proofs naturally and see the applications. Sadly, partly due to how Uni works — partly due to playing League of Legends too much — I did not get the level of understanding required for my attention to stick.\nNow that I’ve become a SWE, I’ve seen the practical uses of such Math, and I now find myself needing more of it! So, step by step I’ll try to go from intuition and basics (this book) -&gt; implementing existing algorithms -&gt; proofs and more advanced formulas and theorem (some other textbook like Linear Algebra Done Right) -&gt; understanding ML architecture choices -&gt; being able to invent/discover new arch and techniques?\n\n\n\nBeing somewhat familiar with Linear Algebra and computer science, I already knew a few things, so I read pretty fast through the first bits.\nThe first thing I learned were the techniques for computing the determinant for matrices of dimensions 4 and above. There’s a technique based on the generalisation of the formula \\(\\det(\\mathbf{A}) = a_{11}a_{22} - a_{12}a_{21}\\).\nAlong with the interpretation/intuition for the determinant (consolidated by 3b1b).\nBut there’s also an algorithm (LU decomposition) used in computer science based on a set of rules for determinant and gaussian reduction."
  },
  {
    "objectID": "journal/2025-08-29.html#the-manga-guide-to-linear-algebra",
    "href": "journal/2025-08-29.html#the-manga-guide-to-linear-algebra",
    "title": "Started reading The Manga Guide to Linear Algebra",
    "section": "",
    "text": "Yes, this might sound weird if you don’t know the concept, but hear me out. After seing it praised on Twitter/X by an anon, I got myself a copy of The Manga Guide to Linear Algebra.\nIt’s pretty good if you want to quickly get up to speed, develop intuition, but does not go too deep into formal proofs.\nYou might not know me, but before going to a Software Engineering School, I studied math for 2 years in Uni, and dropped out because it was too “theorical”, not concrete enough for me, I needed to grok the formulas, to visualize, feel the proofs naturally and see the applications. Sadly, partly due to how Uni works — partly due to playing League of Legends too much — I did not get the level of understanding required for my attention to stick.\nNow that I’ve become a SWE, I’ve seen the practical uses of such Math, and I now find myself needing more of it! So, step by step I’ll try to go from intuition and basics (this book) -&gt; implementing existing algorithms -&gt; proofs and more advanced formulas and theorem (some other textbook like Linear Algebra Done Right) -&gt; understanding ML architecture choices -&gt; being able to invent/discover new arch and techniques?\n\n\n\nBeing somewhat familiar with Linear Algebra and computer science, I already knew a few things, so I read pretty fast through the first bits.\nThe first thing I learned were the techniques for computing the determinant for matrices of dimensions 4 and above. There’s a technique based on the generalisation of the formula \\(\\det(\\mathbf{A}) = a_{11}a_{22} - a_{12}a_{21}\\).\nAlong with the interpretation/intuition for the determinant (consolidated by 3b1b).\nBut there’s also an algorithm (LU decomposition) used in computer science based on a set of rules for determinant and gaussian reduction."
  },
  {
    "objectID": "journal/2025-09-12.html",
    "href": "journal/2025-09-12.html",
    "title": "Finished eigendecomposition notes + Jacobian",
    "section": "",
    "text": "Eigendecomposition\nI had learned about diagonalization/decomposition using eigenvectors and eigenvalues in The Manga Guide to Linear Algebra. Now reading more about it in the appendix of D2L, I felt ready to write my notes about them, and after doing a little more research that’s what I did, and I feel I’ve got a strong grasp on the subject.\n\n\nJacobian\nWith the little free time I still had in this (once again) busy day, I wrote my notes on the Jacobian, a generalization of the gradient to functions with vector inputs and outputs."
  },
  {
    "objectID": "journal/2025-09-09.html",
    "href": "journal/2025-09-09.html",
    "title": "ZeRO / FSDP with Sylvain Gugger and Scott Mueller",
    "section": "",
    "text": "ZeRO / FSDP\nTonight we had a superb lesson — very dense — by Sylvain Gugger{_target=blank} on ZeRO, followed by a code dive-in with Scott. Overall takeaway (more detailed in my notes): * adam is stateful: has states, so ~4x model size in total to store - ZeRO: Zero Redudency Optimizer -&gt; sharding optimizer state: each gpu updates a subset of the models params then they share it all together all_gather - ZeRO2: Also sharding gradients - ZeRO3 == FSDP (PyTorch version): also sharding the model!\nZeRO is NOT A PARALLELISM strategy, it’s a modeling one. Think parallelism = more throughput, modeling strategy \\(\\approx\\) memory optimizations"
  },
  {
    "objectID": "journal/2025-08-28.html",
    "href": "journal/2025-08-28.html",
    "title": "Setting up this blog!",
    "section": "",
    "text": "Today’s the day!\nI’ve decided to document my learning process to “transition” from SWE / using AI \\(\\longrightarrow\\) MLE/Applied Research/ building and training AI.\nAfter experimenting with marimo, loving it and struggling to make a blog out of marimo notebooks and experiments, I’ve stumbled upon this article by Jeremy Howard encouraging me to write my blog via notebooks + Quarto.\nIt also happens that I had just purchased the Scratch To Scale course/conference by the brilliant Zach Mueller and his blog was also using Quarto — which I would soon learn was no coincidence, Zach and Jeremy were closed and Zach contributed to Jeremy’s framework, fast.ai.\nSo there it is, I intend to document my learning process, render notebooks of experiments, store knowledge, formulas I’ve finally understood even if I’m the last person on earth to do so, bookmarks, reading notes etc."
  },
  {
    "objectID": "journal/2025-08-28.html#this-blog",
    "href": "journal/2025-08-28.html#this-blog",
    "title": "Setting up this blog!",
    "section": "",
    "text": "Today’s the day!\nI’ve decided to document my learning process to “transition” from SWE / using AI \\(\\longrightarrow\\) MLE/Applied Research/ building and training AI.\nAfter experimenting with marimo, loving it and struggling to make a blog out of marimo notebooks and experiments, I’ve stumbled upon this article by Jeremy Howard encouraging me to write my blog via notebooks + Quarto.\nIt also happens that I had just purchased the Scratch To Scale course/conference by the brilliant Zach Mueller and his blog was also using Quarto — which I would soon learn was no coincidence, Zach and Jeremy were closed and Zach contributed to Jeremy’s framework, fast.ai.\nSo there it is, I intend to document my learning process, render notebooks of experiments, store knowledge, formulas I’ve finally understood even if I’m the last person on earth to do so, bookmarks, reading notes etc."
  },
  {
    "objectID": "journal/2025-09-13.html",
    "href": "journal/2025-09-13.html",
    "title": "Connectionism and review of Daniel Han’s lecture",
    "section": "",
    "text": "Connectionism - Defeating Nondeterminism in LLM Inference\n\n\nDaniel Han’s Triton Kernels lecture"
  },
  {
    "objectID": "journal/2025-09-10.html",
    "href": "journal/2025-09-10.html",
    "title": "FP8 Training with Phuc Nguyen (HF)",
    "section": "",
    "text": "Big day at work today, so I only got the time for the evening call and some time to re-read yesterday’s lecture (ZeRO/FSDP)\n\nFP8\nPhuc gave us a nice presentation on how and why to train FP8 precision models. Why do it? To speed up training. Issues: the model diverges suuuuuper fast under full FP8 regime, need to be careful. Solution: mixed precision training. We then saw a quick overview of how frontier labs do it (DeepSeek, Meta etc) and frameworks for it (torch/oa, torchtitan)"
  },
  {
    "objectID": "journal/2025-09-05.html",
    "href": "journal/2025-09-05.html",
    "title": "Ray / Anyscale workshop with S2S",
    "section": "",
    "text": "Going down the ray rabbit hole\nToday we had a workshop on Ray x Anyscale by Robert Nishihara — co-founder and CEO of Anyscale, co-creator of Ray — himself!\nAt first I thought this was just gonna be a cloud ad. Like “look this is our platform, here’s how to use it, please do” (lol). I didn’t know Ray, since I don’t know much — yet — about distributed workloads.\nThe workshop was nice, we covered Ray Data and Ray Train, their data and training libraries. These looked like powerful stuff, but I was still cautious about it, they looked like some frameworks that were too high level, too much abstraction for me, à la fast.ai (it’s good, but too high level for me sometimes, I like the from scratch feel of some other stuff, the tweakability).\nDuring the workshop someone asked about using Ray + vLLM (LLM inference engine). I thought “one’s for training, the other for inference, I don’t see the intersection here”. Oh boy was I wrong. After seing the Anyscale employes answer, I realized I didn’t fully grasp what Ray was and what it offered. So, naturally, I started digging. Know I know that Ray is fully OSS, it’s a library for distributed pythonic applications. Ray Core is just that, and that’s already a lot, it allows us to create remote Tasks and Actors, very good parallel / distributed primitives. Then they built higher-level utilities above that, with Ray {Data, Train, Tune and Serve}.\nI had badly misjuged it, I feel like we’re gonna become very close friends.\nBefore I use a lib I like to know what it does and how it does it, but once I’ll have built a minimal toy version, it’s gonna be you and me buddy.\n\n\nLapace Expansion\nTo finish the day, I wrote a quick Laplace Expansion function in DeepML."
  },
  {
    "objectID": "journal/2025-09-04.html",
    "href": "journal/2025-09-04.html",
    "title": "Going through DDP with S2S",
    "section": "",
    "text": "Finishing the preliminaries for D2L\nWell, finally I’m done with the “catching up” for D2L, with a good refresher on Probabilities. Since I studied stats and probabilities at university, I didn’t learn anything new here, but it’s always good to refresh some knowledge, and re-derive formulas, to have them in mind and grok or accept them as true. Like the classic Bayes Theorem, I always accepted the formula as a way to flip conditional probabilities, but rarely as a way of updating a prior.\n\n\nEvening study session on DDP\nWe discussed the different types of parallelism (for ML distributed training):\n\n(Distributed) Data Parallelism (DDP): Each rank loads a copy — replica — of the model, after each optimizer step they always all have the same parameters, they are replicants. Each rank then trains on a different mini-batch (hence the importance of data sharding). We then average the gradients, perform a step of gradient descent, rinse and repeat. If we can use this, we should, it has the least amount of overhead, but it requires that the model + optimizer states all fit in the device’s VRAM.\nPipeline Parallelism (PP): We split the model across different ranks without splitting the layers (so we split along the layers). That is inter-layer parallelism. An exaggeration would be having a model with 2 hidden layers and 1 output layer split across 3 GPUs.\nTensor Parallelism (TP): We split the layers of the model across different ranks, that’s intra-layer parallelism. This could be useful if some layers are so large they don’t even fit in a single device. To reduce overhead, it’s advisable that even if a layer is split across different ranks, all parts remain on the same node. (See terminology)\n\nThere’s also\n\nExpert Parallelism (EP): For Mixture-of-Experts (MoE) we can split the experts on different devices.\n\nWe didn’t discuss much this last one but I knew about it already and searched a bit to know more about it.\nLast but not least we discussed the issue of DataLoaders / Samplers, and the importance of data sharding. They are a mean of “splitting” efficiently our dataset so that each rank sees a different mini-batch of data. Also, each process only loads its shard so it’s more memory/communication efficient."
  },
  {
    "objectID": "journal/2025-09-01.html",
    "href": "journal/2025-09-01.html",
    "title": "Finished reading The Manga Guide to Linear Algebra",
    "section": "",
    "text": "I had a lot of work to do for my day job, we are trying to build and commercialize V2 of our intercom system, and I was faced with technical difficulties that left me very little time to work on math / ML. I still found the time to finish The Manga Guide to Linear Algebra, and learned a few important concepts and techniques!\n\nStill, I did a few things\nTIL about [subspaces], [linear spans], the [rank] of a matrix and a technique to find it via Gaussian Eliminination — as usual, linear algebra appears to be the art of applying Gaussian Elimination correctly —, [Eigenvalues] and [Eigenvectors]! That was still packed!\nI skimmed through the materials for Scratch To Scale, the course I’m following by Scott Mueller, beginning tomorrow."
  },
  {
    "objectID": "journal/index.html",
    "href": "journal/index.html",
    "title": "Journal",
    "section": "",
    "text": "rss\n\n\n\n\n\n\n\n\n\nConnectionism and review of Daniel Han’s lecture\n\n\n\nmath\n\nmle\n\n\n\nRead thinking machines’s first blog article + reviewing the Triton Kernels lesson\n\n\n\n\n\nSep 13, 2025\n\n1 min\n\n\n\n\n\n\nFinished eigendecomposition notes + Jacobian\n\n\n\nmath\n\n\n\nWrote some notes on eigendecomposition and the Jacobian matrix\n\n\n\n\n\nSep 12, 2025\n\n1 min\n\n\n\n\n\n\nWriting triton kernels with Daniel Han from Unsloth + linear algebra\n\n\n\nmle\n\nmath\n\n\n\nA guest speaker session with Daniel Han on writig triton kernel (S2S), a light mode to this website and new linear algebra notes\n\n\n\n\n\nSep 11, 2025\n\n1 min\n\n\n\n\n\n\nFP8 Training with Phuc Nguyen (HF)\n\n\n\nmle\n\n\n\nEvening guest speaker: FP8 Training with Phuc (S2S)\n\n\n\n\n\nSep 10, 2025\n\n1 min\n\n\n\n\n\n\nZeRO / FSDP with Sylvain Gugger and Scott Mueller\n\n\n\npython\n\nmle\n\n\n\nEvening study session for Scratch To Scale (S2S)\n\n\n\n\n\nSep 9, 2025\n\n1 min\n\n\n\n\n\n\nMarimo Workshop + Integrals refresher\n\n\n\npython\n\nmath\n\n\n\nA superb Marimo workshop with Vincent Warmerdam (S2S) and binge watching Essence Of Calculus (3b1b)\n\n\n\n\n\nSep 8, 2025\n\n1 min\n\n\n\n\n\n\nRay / Anyscale workshop with S2S\n\n\n\npython\n\nmle\n\n\n\nAn enlighting workshop to learn Ray with Robert Nishihara of Anyscale\n\n\n\n\n\nSep 5, 2025\n\n2 min\n\n\n\n\n\n\nGoing through DDP with S2S\n\n\n\npython\n\nmle\n\nmath\n\n\n\nFinishing the preliminaries for D2L and quick evening study session on DDP with torch.distributed primitives\n\n\n\n\n\nSep 4, 2025\n\n2 min\n\n\n\n\n\n\nFireside chat QA with Yuxiang Wei from Meta FAIR with S2S\n\n\n\nmle\n\npython\n\n\n\nQuestions surrounding distributed training at FAIR\n\n\n\n\n\nSep 3, 2025\n\n1 min\n\n\n\n\n\n\nStarting Scratch To Scale (S2S) and Dive into Deep Learning (D2L)\n\n\n\nmath\n\npython\n\nmle\n\n\n\nLearning about distributed training on GPUs (S2S) and preliminaries for D2L\n\n\n\n\n\nSep 2, 2025\n\n1 min\n\n\n\n\n\n\nFinished reading The Manga Guide to Linear Algebra\n\n\n\nmath\n\n\n\nLearning more about Linear Transformations, Eigen{vectors, values} and Diagonalization\n\n\n\n\n\nSep 1, 2025\n\n1 min\n\n\n\n\n\n\nStarted reading The Manga Guide to Linear Algebra\n\n\n\nmath\n\n\n\n(Re-)Developping intuition around Linear Algebra\n\n\n\n\n\nAug 29, 2025\n\n2 min\n\n\n\n\n\n\nSetting up this blog!\n\n\n\npython\n\n\n\nToday I decided I would convert my old Next.js blog to Notebooks + Quarto and document my learning process\n\n\n\n\n\nAug 28, 2025\n\n1 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "journal/2025-09-03.html",
    "href": "journal/2025-09-03.html",
    "title": "Fireside chat QA with Yuxiang Wei from Meta FAIR with S2S",
    "section": "",
    "text": "Well today I woke up at 5am to go on-site at my startup in Paris (I live 500km away, in Bordeaux) and got home at around 7pm, so not much free time to study, so I just read a bit in the train and ML related stuff was just the evening fireside chat.\n\nFireside chat with Yuxiang Wei from Meta FAIR (S2S)\nAs part of the course (conference) Scratch To Scale, we have a lot of very qualified engineers and researchers from top labs / companies dedicating time for guest lectures or QA sessions.\nToday we had the chance to listen to Yuxiang Wei — of Meta FAIR, of the SWE-RL paper — and ask him loads of questions. We discussed topics such as {pre, mid, post}-training, training infrastructure, the advent of RL and its impacts on said infrastructure and more."
  },
  {
    "objectID": "journal/2025-09-11.html",
    "href": "journal/2025-09-11.html",
    "title": "Writing triton kernels with Daniel Han from Unsloth + linear algebra",
    "section": "",
    "text": "Linear algebra notes\nToday I wrote some more linear algebra notes, on linear dependence, rank, basis and dimension. In the process I also wrote about vector spaces. I really begin to feel the value of these notes, both as an external memory to refer to later, but also as a learning process allowing me to really understand to summarize and re-explain.\n\n\nDaniel Han\nAs part of S2S we had a guest lecture by Daniel Han from Unsloth on Triton and custom kernels. It was super interesting because it was working from first principles, and that’s the way I work. Instead of diving into triton’s DSL etc, he took a pen and a stack of paper and wrote neural network graphs of computations, deriving them by hand and explaining how custom training kernels were written as a way to speed-up backprop by using derivation tricks, using our knowledge of calculus, caching computations and results to achieve a better result than a torch.compile or autograd.\n\n\nLight mode\nI also took way too much time to get an OK-tier light mode for this website."
  },
  {
    "objectID": "journal/2025-09-08.html",
    "href": "journal/2025-09-08.html",
    "title": "Marimo Workshop + Integrals refresher",
    "section": "",
    "text": "Binge watched Essence of Calculus\nAfter digging back into probabilities, I realized how important it was that I not only know integrals and formulas, but also that I completely grok and am able to derive these formulas from first principles. So I re-watched the entire Essence of Calculus series by the incredible 3b1b / Grant Sanderson.\nAnd grokking I did, between the videos and my old high-school and college lessons somewhere hidden deep inside my mind, I feel a renewed understanding of integration, so I wrote notes for the future me that will obviously forget again.\n\n\nSome more Linear Algebra\nSome research into the basics — first principles always wins — led me to learn about Elementary Matrices. I’d never given more thought to Gaussian Elimination that just using the row operations to achieve my goal. But thinking about elementary matrices and their properties, made things like LU decomposition click even more.\n\n\nMarimo Workshop\nTo finish off this awesome day of learning, we had a workshop with Vincent Warmerdam from the Marimo team. I love marimo and I loved learning more about it’s capabilities and ways to hack (with / at) it."
  },
  {
    "objectID": "notes/probability.html",
    "href": "notes/probability.html",
    "title": "Probability and Information Theory",
    "section": "",
    "text": "When studying probability, we are performing experiments, random trials or observations. The set of all possible outcomes of this experiment is \\(\\mathcal{\\Omega}\\) (or \\(\\mathcal{S}\\)). eg. When rolling a die, \\(\\mathcal{\\Omega} = \\{1,2,3,4,5,6\\}\\).\nWe can group these outcomes into events — \\(\\mathcal{E} \\subseteq \\mathcal{\\Omega}\\). eg. The event \\(\\mathcal{E} = \\{\\)die shows an even number\\(\\} = \\{2, 4, 6\\}\\). Whenever the outcome \\(z\\) of the random experiment satisfies \\(z \\in \\mathcal{E}\\), the event \\(\\mathcal{E}\\) has occurred. Multiple events can occur from the same outcome, say we have \\(\\mathcal{A} = \\{3, 6\\}\\) “the result is divisible by 3” and \\(\\mathcal{B} = \\{2, 4, 6\\}\\). \\(z = 6\\) satisfies both \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\).\n\n\n\nThe probability function maps events onto a real value \\(P\\colon \\mathcal{E} \\subseteq \\mathcal{\\Omega} \\to [0, 1]\\).\n\\(\\operatorname{P}(\\mathcal{E})\\) is the probability associated with event \\(\\mathcal{E}\\).\n\n\n\n\\(\\operatorname{P}(\\mathcal{E}) \\geq 0\\)\n\\(\\operatorname{P}(\\mathcal{\\Omega}) = 1, \\operatorname{P}(\\mathcal{\\emptyset}) = 0\\)\n\\(\\operatorname{P}(\\mathcal{A} \\cup \\mathcal{B}) = \\operatorname{P}(\\mathcal{A}) + \\operatorname{P}(\\mathcal{B}) - \\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B})\\)\n\\(\\operatorname{P}(\\bigcup_{i=1}^{\\infty} \\mathcal{A}_i) = \\sum_{i=1}^{\\infty} \\operatorname{P}(\\mathcal{A}_i), \\quad \\mathcal{A}_i \\cap \\mathcal{A}_j = \\emptyset\\: \\text{for all}\\: i \\neq j\\) (= if all events \\(\\mathcal{A}_i\\) are mutually exclusive)\n\\(\\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B}) = \\operatorname{P}(\\mathcal{A} \\mid \\mathcal{B})\\operatorname{P}(\\mathcal{B})\\)\n\\(\\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B}) = \\operatorname{P}(\\mathcal{A})\\operatorname{P}(\\mathcal{B}) \\iff \\mathcal{A} \\perp \\mathcal{B}\\) (eg. 2 fair dice rolls)\n\\(\\mathcal{A} \\perp \\mathcal{B} \\iff \\operatorname{P}(\\mathcal{A} \\mid \\mathcal{B}) = \\operatorname{P}(\\mathcal{A})\\)\n\n\n\n\n\nA random variable \\(X\\) is a measurable function (mapping) \\(X \\colon \\mathcal{\\Omega} \\to \\mathcal{E}\\) from a sample space \\(\\mathcal{\\Omega}\\) as a set of possible outcomes to a measurable space \\(\\mathcal{E}\\).\nThe probability that \\(X\\) takes on a value in a measurable set \\(\\mathcal{S} \\in \\mathcal{E}\\) is written as \\(\\operatorname{P}(X \\in \\mathcal{S}) = \\operatorname{P}(\\{\\omega \\in \\mathcal{\\Omega} \\mid X(\\omega) \\in \\mathcal{S}\\})\\)\nThe probability that \\(X\\) takes discrete value \\(v\\), denoted \\(X = v\\), is \\(\\operatorname{P}(X=v)\\).\n\\(X = v\\) or \\(X \\geq v\\) are events.\nRandom variables allow us to go from outcomes to values, like \\(X(\\omega) = \\omega\\) the random variable that associates to each die its value (identity function). This is also an example of a discrete random variable.\nWhen \\(X\\) is continuous it doesn’t make sense to have events like \\(X = v\\) (and \\(\\operatorname{P}(X = v) = 0\\)), rather we use \\(v \\leq X \\leq w\\) and probability densities. An example would be the height of a population.\nWe note \\(\\operatorname{P}(X)\\) the probability distribution of X. (Abuse of notation: strictly \\(P_X\\) is the distribution of \\(X\\), but we’ll often just write \\(P(X)\\))\n\n\n\\(\\operatorname{P}(A = a, B = b)\\) is the joint probability of \\(A = a\\) and \\(B = b\\) (it’s the intersection of the events \\(A = a\\) and \\(B = b\\)). Equivalently it’s \\(\\operatorname{P}(\\{A = a\\} \\cap \\{B = b\\})\\), with an overloaded notation, the joint probability distribution becomes \\(\\operatorname{P}(A, B)\\)\nObviously \\[ \\operatorname{P}(A = a, B = b) \\leq \\operatorname{P}(A=a) \\quad \\text{and} \\quad \\operatorname{P}(A = a, B = b) \\leq \\operatorname{P}(B=b) \\]\nAlso, we can marginalize \\[ \\operatorname{P}(A = a) = \\sum_v \\operatorname{P}(A = a, B = v) \\]\nBecause \\(A = a\\) and \\(B = b\\) are events, \\[\\begin{aligned}\n\\operatorname{P}(A = a, B = b) & = \\operatorname{P}(A = a \\mid B = b)\\operatorname{P}(B = b) \\\\\n\\iff \\operatorname{P}(A = a \\mid B = b) & = \\operatorname{P}(A = a, B = b)/\\operatorname{P}(B = b)\n\\end{aligned}\\]\n\n\n\n\nFrom the properties and definitions above, we can derive the following formula\n\\[ \\overbrace{\\operatorname{P}(A \\mid B)}^{\\text{posterior probability}} = \\dfrac{\\overbrace{\\operatorname{P}(B \\mid A)}^{\\text{likelihood}}\\overbrace{\\operatorname{P}(A)}^{\\text{prior}}}{\\underbrace{\\operatorname{P}(B)}_{\\text{observation}}} \\]\n\nprior/hypothesis: our estimate or current belief about the probability of \\(A\\)\nobservation/marginal likelihood/evidence: the evidence or observations we’ve made regarding \\(B\\)\nlikelihood: a measure of how compatible our hypothesis is with our observation\n\nA simplified version is \\(\\operatorname{P}(A \\mid B) \\propto \\operatorname{P}(B \\mid A)\\operatorname{P}(A)\\)\n\n\n\n\n\nThe expectation (or expected value) is the weighted average of the values of \\(X\\).\nDiscrete case:\n\\[ \\operatorname{E}[X] = \\operatorname{E}_{x \\sim P}[X] = \\sum_x x\\operatorname{P}(X=x) \\]\nContinuous case:\n\\[\\operatorname{E}[X] = \\int_{-\\infty}^{\\infty} x f(x) \\;dx \\]\nTo follow mathematical notation, sometimes we use \\(\\mu\\) to denote this average.\n\n\n\nThe variance is a measure of dispersion, it quantifies how much do values vary relative to the expectation (average) on average. The variance is the expectation of the squared difference between the values and the expected value.\n\\[ \\operatorname{Var}(X) = \\operatorname{E}[(X - \\operatorname{E}[X])^2] = \\operatorname{E}[X^2] - (\\operatorname{E}[X])^2 \\]\nBecause\n\\[ \\operatorname{E}[X^2 - 2X\\operatorname{E}[X] + \\operatorname{E}[X]^2] = \\operatorname{E}[X^2] - 2(\\operatorname{E}[X])^2 + (\\operatorname{E}[X])^2 \\]\n\n\n\nBecause the variance is a squared difference, we can take its square root to get the standard deviation which has the benefit of being in the same unit as our random variable.\n\\[ \\operatorname{Var}(X) = \\sigma^2_X \\iff \\sigma_X = \\sqrt{\\operatorname{Var}(X)} \\]\n\n\n\nTODO once I understand it fully enough to explain it."
  },
  {
    "objectID": "notes/probability.html#definitions-formulas",
    "href": "notes/probability.html#definitions-formulas",
    "title": "Probability and Information Theory",
    "section": "",
    "text": "When studying probability, we are performing experiments, random trials or observations. The set of all possible outcomes of this experiment is \\(\\mathcal{\\Omega}\\) (or \\(\\mathcal{S}\\)). eg. When rolling a die, \\(\\mathcal{\\Omega} = \\{1,2,3,4,5,6\\}\\).\nWe can group these outcomes into events — \\(\\mathcal{E} \\subseteq \\mathcal{\\Omega}\\). eg. The event \\(\\mathcal{E} = \\{\\)die shows an even number\\(\\} = \\{2, 4, 6\\}\\). Whenever the outcome \\(z\\) of the random experiment satisfies \\(z \\in \\mathcal{E}\\), the event \\(\\mathcal{E}\\) has occurred. Multiple events can occur from the same outcome, say we have \\(\\mathcal{A} = \\{3, 6\\}\\) “the result is divisible by 3” and \\(\\mathcal{B} = \\{2, 4, 6\\}\\). \\(z = 6\\) satisfies both \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\).\n\n\n\nThe probability function maps events onto a real value \\(P\\colon \\mathcal{E} \\subseteq \\mathcal{\\Omega} \\to [0, 1]\\).\n\\(\\operatorname{P}(\\mathcal{E})\\) is the probability associated with event \\(\\mathcal{E}\\).\n\n\n\n\\(\\operatorname{P}(\\mathcal{E}) \\geq 0\\)\n\\(\\operatorname{P}(\\mathcal{\\Omega}) = 1, \\operatorname{P}(\\mathcal{\\emptyset}) = 0\\)\n\\(\\operatorname{P}(\\mathcal{A} \\cup \\mathcal{B}) = \\operatorname{P}(\\mathcal{A}) + \\operatorname{P}(\\mathcal{B}) - \\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B})\\)\n\\(\\operatorname{P}(\\bigcup_{i=1}^{\\infty} \\mathcal{A}_i) = \\sum_{i=1}^{\\infty} \\operatorname{P}(\\mathcal{A}_i), \\quad \\mathcal{A}_i \\cap \\mathcal{A}_j = \\emptyset\\: \\text{for all}\\: i \\neq j\\) (= if all events \\(\\mathcal{A}_i\\) are mutually exclusive)\n\\(\\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B}) = \\operatorname{P}(\\mathcal{A} \\mid \\mathcal{B})\\operatorname{P}(\\mathcal{B})\\)\n\\(\\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B}) = \\operatorname{P}(\\mathcal{A})\\operatorname{P}(\\mathcal{B}) \\iff \\mathcal{A} \\perp \\mathcal{B}\\) (eg. 2 fair dice rolls)\n\\(\\mathcal{A} \\perp \\mathcal{B} \\iff \\operatorname{P}(\\mathcal{A} \\mid \\mathcal{B}) = \\operatorname{P}(\\mathcal{A})\\)\n\n\n\n\n\nA random variable \\(X\\) is a measurable function (mapping) \\(X \\colon \\mathcal{\\Omega} \\to \\mathcal{E}\\) from a sample space \\(\\mathcal{\\Omega}\\) as a set of possible outcomes to a measurable space \\(\\mathcal{E}\\).\nThe probability that \\(X\\) takes on a value in a measurable set \\(\\mathcal{S} \\in \\mathcal{E}\\) is written as \\(\\operatorname{P}(X \\in \\mathcal{S}) = \\operatorname{P}(\\{\\omega \\in \\mathcal{\\Omega} \\mid X(\\omega) \\in \\mathcal{S}\\})\\)\nThe probability that \\(X\\) takes discrete value \\(v\\), denoted \\(X = v\\), is \\(\\operatorname{P}(X=v)\\).\n\\(X = v\\) or \\(X \\geq v\\) are events.\nRandom variables allow us to go from outcomes to values, like \\(X(\\omega) = \\omega\\) the random variable that associates to each die its value (identity function). This is also an example of a discrete random variable.\nWhen \\(X\\) is continuous it doesn’t make sense to have events like \\(X = v\\) (and \\(\\operatorname{P}(X = v) = 0\\)), rather we use \\(v \\leq X \\leq w\\) and probability densities. An example would be the height of a population.\nWe note \\(\\operatorname{P}(X)\\) the probability distribution of X. (Abuse of notation: strictly \\(P_X\\) is the distribution of \\(X\\), but we’ll often just write \\(P(X)\\))\n\n\n\\(\\operatorname{P}(A = a, B = b)\\) is the joint probability of \\(A = a\\) and \\(B = b\\) (it’s the intersection of the events \\(A = a\\) and \\(B = b\\)). Equivalently it’s \\(\\operatorname{P}(\\{A = a\\} \\cap \\{B = b\\})\\), with an overloaded notation, the joint probability distribution becomes \\(\\operatorname{P}(A, B)\\)\nObviously \\[ \\operatorname{P}(A = a, B = b) \\leq \\operatorname{P}(A=a) \\quad \\text{and} \\quad \\operatorname{P}(A = a, B = b) \\leq \\operatorname{P}(B=b) \\]\nAlso, we can marginalize \\[ \\operatorname{P}(A = a) = \\sum_v \\operatorname{P}(A = a, B = v) \\]\nBecause \\(A = a\\) and \\(B = b\\) are events, \\[\\begin{aligned}\n\\operatorname{P}(A = a, B = b) & = \\operatorname{P}(A = a \\mid B = b)\\operatorname{P}(B = b) \\\\\n\\iff \\operatorname{P}(A = a \\mid B = b) & = \\operatorname{P}(A = a, B = b)/\\operatorname{P}(B = b)\n\\end{aligned}\\]\n\n\n\n\nFrom the properties and definitions above, we can derive the following formula\n\\[ \\overbrace{\\operatorname{P}(A \\mid B)}^{\\text{posterior probability}} = \\dfrac{\\overbrace{\\operatorname{P}(B \\mid A)}^{\\text{likelihood}}\\overbrace{\\operatorname{P}(A)}^{\\text{prior}}}{\\underbrace{\\operatorname{P}(B)}_{\\text{observation}}} \\]\n\nprior/hypothesis: our estimate or current belief about the probability of \\(A\\)\nobservation/marginal likelihood/evidence: the evidence or observations we’ve made regarding \\(B\\)\nlikelihood: a measure of how compatible our hypothesis is with our observation\n\nA simplified version is \\(\\operatorname{P}(A \\mid B) \\propto \\operatorname{P}(B \\mid A)\\operatorname{P}(A)\\)\n\n\n\n\n\nThe expectation (or expected value) is the weighted average of the values of \\(X\\).\nDiscrete case:\n\\[ \\operatorname{E}[X] = \\operatorname{E}_{x \\sim P}[X] = \\sum_x x\\operatorname{P}(X=x) \\]\nContinuous case:\n\\[\\operatorname{E}[X] = \\int_{-\\infty}^{\\infty} x f(x) \\;dx \\]\nTo follow mathematical notation, sometimes we use \\(\\mu\\) to denote this average.\n\n\n\nThe variance is a measure of dispersion, it quantifies how much do values vary relative to the expectation (average) on average. The variance is the expectation of the squared difference between the values and the expected value.\n\\[ \\operatorname{Var}(X) = \\operatorname{E}[(X - \\operatorname{E}[X])^2] = \\operatorname{E}[X^2] - (\\operatorname{E}[X])^2 \\]\nBecause\n\\[ \\operatorname{E}[X^2 - 2X\\operatorname{E}[X] + \\operatorname{E}[X]^2] = \\operatorname{E}[X^2] - 2(\\operatorname{E}[X])^2 + (\\operatorname{E}[X])^2 \\]\n\n\n\nBecause the variance is a squared difference, we can take its square root to get the standard deviation which has the benefit of being in the same unit as our random variable.\n\\[ \\operatorname{Var}(X) = \\sigma^2_X \\iff \\sigma_X = \\sqrt{\\operatorname{Var}(X)} \\]\n\n\n\nTODO once I understand it fully enough to explain it."
  },
  {
    "objectID": "notes/probability.html#notes",
    "href": "notes/probability.html#notes",
    "title": "Probability and Information Theory",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "notes/probability.html#proofs",
    "href": "notes/probability.html#proofs",
    "title": "Probability and Information Theory",
    "section": "Proofs",
    "text": "Proofs\nLater!"
  },
  {
    "objectID": "notes/probability.html#algorithms",
    "href": "notes/probability.html#algorithms",
    "title": "Probability and Information Theory",
    "section": "Algorithms",
    "text": "Algorithms"
  },
  {
    "objectID": "notes/probability.html#notation",
    "href": "notes/probability.html#notation",
    "title": "Probability and Information Theory",
    "section": "Notation",
    "text": "Notation\n\n\\(\\mathcal{X}\\): a set\n\\(\\{a, b, c\\}\\): a set, with its elements\n\\(\\emptyset\\): the empty set\n\\(\\mathcal{A} \\subset \\mathcal{B}\\), \\(\\mathcal{A} \\subsetneq \\mathcal{B}\\): \\(\\mathcal{A}\\) is a proper/strict subset of \\(\\mathcal{B}\\)\n\\(\\mathcal{A} \\subseteq \\mathcal{B}\\): \\(\\mathcal{A}\\) is a subest of \\(\\mathcal{B}\\)\n\\(\\mathcal{A} \\cap \\mathcal{B}\\): the intersection of sets \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) — “\\(\\mathcal{A}\\) and \\(\\mathcal{B}\\)”\n\\(\\mathcal{A} \\cup \\mathcal{B}\\): the union of sets \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) — “\\(\\mathcal{A}\\) or \\(\\mathcal{B}\\)”\n\\(\\mathcal{A} \\setminus \\mathcal{B}\\): set subtraction of \\(\\mathcal{B}\\) from \\(\\mathcal{A}\\), elements from \\(\\mathcal{A}\\) but not in \\(\\mathcal{B}\\)\n\\(\\mathcal{S}\\), \\(\\mathcal{\\Omega}\\): the sample space / universe (the set of all possible outcomes)\n\\(|\\mathcal{X}|\\): the cardinality of set \\(\\mathcal{X}\\) (its number of events)\n\\(X\\): a random variable\n\\(P\\): a probability distribution\n\\(X \\sim P\\): the random variable \\(X\\) follows the probability distribution \\(P\\)\n\\(a \\propto b\\): \\(a\\) is proportional to \\(b\\), eg. \\(a = kb\\)\n\\(\\operatorname{P}(\\cdot)\\): the probability function, maps events to their probability and random variables to their probability distributions\n\\(\\operatorname{P}(X)\\): depending on the context, a probability distribution or the probability of any \\(X=x\\), meaning the formula is true for any value\n\\(\\operatorname{P}(X=x)\\): the probability assigned to the event where random variable \\(X\\) takes value \\(x\\)\n\\(\\operatorname{P}(X \\mid Y)\\): the conditional probability distribution of \\(X\\) given \\(Y\\)\n\\(\\operatorname{p}(\\cdot)\\): a probability density function (PDF) associated with distribution \\(P\\)\n\\(\\operatorname{E}[X]\\): expectation of a random variable \\(X\\)\n\\(X \\perp Y\\): random variables \\(X\\) and \\(Y\\) are independent\n\\(X \\perp Y \\mid Z\\): random variables \\(X\\) and \\(Y\\) are conditionally independent given \\(Z\\)\n\\(\\sigma_X\\): standard deviation of random variable \\(X\\)\n\\(\\textrm{Var}(X)\\): variance of random variable \\(X\\), equal to \\(\\sigma^2_X\\)\n\\(\\textrm{Cov}(X, Y)\\): covariance of random variables \\(X\\) and \\(Y\\)\n\\(\\operatorname{\\rho}(X, Y)\\): the Pearson correlation coefficient between \\(X\\) and \\(Y\\), equals \\(\\frac{\\textrm{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\)\n\\(\\operatorname{H}(X)\\): entropy of random variable \\(X\\)\n\\(D_{\\textrm{KL}}(P\\|Q)\\): the KL-divergence (or relative entropy) from distribution \\(Q\\) to distribution \\(P\\)"
  },
  {
    "objectID": "notes/pytorch.html#notes",
    "href": "notes/pytorch.html#notes",
    "title": "PyTorch",
    "section": "Notes",
    "text": "Notes\nLater!"
  },
  {
    "objectID": "notes/pytorch.html#terminology",
    "href": "notes/pytorch.html#terminology",
    "title": "PyTorch",
    "section": "Terminology",
    "text": "Terminology"
  },
  {
    "objectID": "notes/linear-algebra.html",
    "href": "notes/linear-algebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "A vector space (over a field, like \\(\\mathbb{R}\\), which provides the scalars) is a set of vectors equipped with vector addition and scalar multiplication that satisfies the following conditions:\n\nClosure under addition: \\(\\mathbf{u} + \\mathbf{v}\\) is in the space for any \\(\\mathbf{u}, \\mathbf{v}\\) in the space.\nClosure under scalar multiplication: \\(\\alpha \\mathbf{u}\\) is in the space for any scalar \\(\\alpha\\).\nAssociativity of addition: \\(\\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}\\).\nCommutativity of addition: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\).\nAdditive identity: there exists \\(\\mathbf{0}\\) such that \\(\\mathbf{u} + \\mathbf{0} = \\mathbf{u}\\).\nAdditive inverse: for each \\(\\mathbf{u}\\), there exists \\(-\\mathbf{u}\\) such that \\(\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}\\).\nCompatibility of scalar multiplication: \\(\\alpha(\\beta \\mathbf{u}) = (\\alpha\\beta) \\mathbf{u}\\).\nIdentity of scalar multiplication: \\(1 \\cdot \\mathbf{u} = \\mathbf{u}\\).\nDistributivity over vector addition: \\(\\alpha(\\mathbf{u} + \\mathbf{v}) = \\alpha \\mathbf{u} + \\alpha \\mathbf{v}\\).\nDistributivity over scalar addition: \\((\\alpha + \\beta)\\mathbf{u} = \\alpha \\mathbf{u} + \\beta \\mathbf{u}\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThis might be useful later, but for all intents and purposes in these notes, the field will be \\(\\mathbb{R}\\), vectors will be in \\(\\mathbb{R}^n\\), and addition and scalar multiplication will be element-wise.\n\n\n\n\n\n\\[ (\\mathbf{A}^\\top)_{i,j} = \\mathbf{A}_{j,i}  \\]\n\n\n\n\\[ \\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i} u_i v_i \\]\n\nu, v = torch.arange(3), torch.arange(3)  # torch.tensor([0, 1, 2])\ntorch.dot(u, v)  # u @ v\n\ntensor(5)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe dot product has a geometric meaning.\n\\[\\mathbf{v}\\cdot\\mathbf{w} = \\|\\mathbf{v}\\|\\|\\mathbf{w}\\|\\cos{\\theta}\\] \\[\\iff \\theta = \\arccos\\left(\\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}\\right).\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn linear algebra, vectors are column vectors by default, so \\[ \\mathbf{u} \\in \\mathbb{R}^{n \\times 1}, \\quad \\mathbf{u} = \\begin{bmatrix} u_1 \\\\ \\vdots \\\\ u_n \\end{bmatrix} \\]\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{u})_{i} = \\mathbf{a}_{i,:} \\cdot \\mathbf{u} = \\sum_{j} a_{i,j} u_j  \\]\n\nA, u = torch.arange(6).reshape(3, 2), torch.arange(2)\nA @ u\n\ntensor([1, 3, 5])\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen we say “we apply a linear transformation \\(\\mathbf{A}\\) to \\(\\mathbf{v}\\)”, we mean \\(\\mathbf{A}\\mathbf{v}\\).\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{B})_{i,j} = \\mathbf{a}_{i,:} \\cdot \\mathbf{b}_{:, j} = \\sum_{k} a_{i,k} b_{k,j} \\]\n\nU, V = torch.arange(6).reshape(3, 2), torch.arange(6).reshape(2, 3)\nU @ V\n\ntensor([[ 3,  4,  5],\n        [ 9, 14, 19],\n        [15, 24, 33]])\n\n\n\n\n\nNon-commutativity: most of the time \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\)\nDistributivity: \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{B}\\mathbf{C}\\) and \\((\\mathbf{A} + \\mathbf{B})\\mathbf{C} = \\mathbf{A}\\mathbf{C} + \\mathbf{B}\\mathbf{C}\\)\nAssociativity: \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\)\nTranspose: \\((\\mathbf{A}\\mathbf{B})^\\top = \\mathbf{B}^\\top\\mathbf{A}^\\top\\)\n\n\n\n\n\n\\[ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I},\\quad \\mathbf{A} \\in \\mathbb{R}^{n \\times n} \\]\n\n\n\n\n\n\nNote\n\n\n\nMight not exist if \\(\\mathbf{A}\\) is not invertible, that is \\(\\det(\\mathbf{A}) = 0\\)\n\n\n\nA = torch.randn(3, 3, dtype=torch.double)\nA_i = A.inverse()\nI = torch.eye(3, dtype=torch.double)\ntorch.allclose(A @ A_i, I), torch.allclose(A_i @ A, I)\n\n(True, True)\n\n\n\n\n\nI think of the determinant of \\(\\mathbf{A}\\) as the scaling factor of the linear transformation represented by \\(\\mathbf{A}\\).\nThis explains why a matrix whose determinant is 0 is not invertible: it “collapses”, and two images might have the same original input \\(\\mathbf{x}\\)\n\n\n\\[ \\det(\\mathbf{A}^\\top) = \\det(\\mathbf{A}) \\] \\[ \\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A})\\det(\\mathbf{B}) \\] \\[ \\det(\\mathbf{A}^{-1}) = \\dfrac{1}{\\det(\\mathbf{A})} \\iff \\det(\\mathbf{A}) \\neq 0 \\]\n\n\n\n\\[ A = \\operatorname{diag}(a_1, \\dots, a_n), \\quad \\det(A) = \\prod_{i=1}^n a_i \\]\n\n# using I as a mask\nI = torch.eye(3, dtype=torch.float32)\nA = torch.arange(1, 10, dtype=torch.float32).reshape(3, 3) * I  # A = diag(1, 5, 9)\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\\[ \\det(T) = \\prod_{i} t_{i,i}, \\quad \\text{where $T$ is triangular} \\]\n\nA = torch.triu(torch.arange(1, 10, dtype=torch.float32).reshape(3, 3))\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}\\: (= \\mathbf{A}\\mathbf{A}^\\top) \\]\nInterstingly,\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^{-1} = \\mathbf{A}^\\top \\]\n\n\n\nAn elementary matrix is a matrix obtained from the application of a single elementary row operation to the identity matrix \\(\\mathbf{I}\\).\nBecause of associativity, the product of an elementary matrix and another matrix is the same as applying the row operation to the other matrix.\n\n\nMultiplying row \\(i\\) by \\(m\\)\n\\[\\mathbf{D}_i(m) = \\operatorname{diag}(1, \\dots, 1, d_i = m, 1, \\dots, 1)\\]\n\n\n\n\\(\\mathbf{D}_i(m) = \\mathbf{D}_i(\\dfrac{1}{m})^{-1} = \\mathbf{D}_i(m)^\\top\\)\n\\(\\operatorname{det}(\\mathbf{D}_i(m)) = m\\) — obviously because it’s a diagonal matrix full of \\(1\\)’s and a single \\(m\\)\n\n\n\n\n\nAdding \\(m\\) times row \\(j\\) to row \\(i\\)\n\\[\\mathbf{L}_{ij}(m) =\n\\begin{bmatrix}\n1 &&&&&& \\\\\n& \\ddots &&&&& \\\\\n&& 1 &&&& \\\\\n&&& \\ddots &&& \\\\\n&& l_{i,j} = m && 1 && \\\\\n&&&&& \\ddots & \\\\\n&&&&&& 1\n\\end{bmatrix}\\]\n\n\n\n\\(\\mathbf{L}_{ij}(m)^{-1} = \\mathbf{L}_{ij}(-m)\\) — the inverse of adding a row multiple to another, is subtracting the same row multiple\n\\(\\mathbf{L}_{ij}(m)\\) and \\(\\mathbf{L}_{ij}(m)^{-1}\\) are triangular matrices\nSo \\(\\operatorname{det}(\\mathbf{L}_{ij}(m)) = 1 \\iff \\operatorname{det}(\\mathbf{L}_{ij}(m)\\mathbf{A}) = \\operatorname{det}(\\mathbf{A})\\)\n\n\n\n\n\nSwapping rows \\(i\\) and \\(j\\)\n\\[\\mathbf{T}_{ij} =\n\\begin{bmatrix}\n1 &&&&&& \\\\\n& \\ddots &&&&& \\\\\n&& 0 && t_{j,i} = 1 && \\\\\n&&& \\ddots &&& \\\\\n&& t_{i,j} = 1 && 0 && \\\\\n&&&&& \\ddots & \\\\\n&&&&&& 1\n\\end{bmatrix}\\]\n\n\n\n\\(\\mathbf{T}_{ij}^\\top = \\mathbf{T}_{ij}^{-1} = \\mathbf{T}_{ij}\\)\n\\(\\operatorname{det}(\\mathbf{T}_{ij}) = -1\\:^{(*)} \\iff \\operatorname{det}(\\mathbf{T}_{ij}\\mathbf{A}) = -\\operatorname{det}(\\mathbf{A})\\)\n\n\\(^{(*)}\\) Consider that swapping two rows can be expressed as applying consecutive additions/subtractions as follows \\[\\begin{aligned}\n\\mathbf{T}_{ij} & = \\mathbf{D}_{i}(-1)\\mathbf{L}_{ij}(-1)\\mathbf{L}_{ji}(1)\\mathbf{L}_{ij}(-1) \\\\\n\\iff \\operatorname{det}(\\mathbf{T}_{ij}) & = \\operatorname{det}(\\mathbf{D}_{i}(-1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ij}(-1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ji}(1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ij}(-1)) \\\\\n\\iff \\operatorname{det}(\\mathbf{T}_{ij}) & = -1 \\\\\n\\end{aligned}\\]\n\n\n\n\n\nWe say of \\(n\\) vectors — \\(\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\) that they are linearly dependent if there exists scalars \\(a_1, \\dots, a_n\\) not all equal to 0 satisfying \\[\\sum_{i=1}^n a_i\\mathbf{v}_i = 0\\]\nIf the only solution is \\(a_i = 0\\) for \\(i\\) in \\(0, \\dots, n\\), they are linearly independent.\n\n\n\n\n\n\nNote\n\n\n\nLinear dependence means that some vectors could be expressed as a weighted sum (linear combination) of others, hence carrying redudant information/operation.\n\n\n\n\n\nThe rank of a matrix is the size of the largest subset of linearly independent columns among its columns. Equivalently, it is the dimension of the column space.\n\n\n\n\n\n\nNote\n\n\n\nThis reflects that if some columns are linearly dependent, they are redundant: they do not contribute a new direction. Thus, the matrix can be expressed with fewer independent columns, and the image (output space) of the matrix has correspondingly fewer dimensions.\n\n\n\n\n\nA basis (not the, since a vector space can have many different bases) of a vector space is a linearly independent subset that spans the space.\nAll bases of the same vector space have the same size = the dimension\n\n\n\n\n\n\nNote\n\n\n\nThat means: a basis is a set of vectors that you can combine (with weighted sums) to form any vector in the space — and none of them are redundant.\n\n\n\n\n\nThe dimension of a vector space is the number of vectors in a basis of that space.\n\n\n\n\n\n\nNote\n\n\n\nSince every basis of a vector space has the same size, the dimension is well-defined. It tells you “how many independent directions” the space has.\n\n\n\n\n\n\n\nAn eigenvector is a vector whose direction (eg. “line”) is unchanged by a linear transformation (represented by a matrix). That means that when applying the linear transformation \\(\\mathbf{A}\\) to an eigeinvector \\(\\mathbf{v}\\) it simply gets scaled by a constant scalar factor, the eigenvalue \\(\\lambda\\). Represented by this eigenequation. \\[\\begin{aligned}\n\\mathbf{A}\\mathbf{v} & = \\lambda\\mathbf{v} \\\\\n\\iff (\\mathbf{A} - \\lambda\\mathbf{I})\\mathbf{v} & = \\mathbf{0} \\label{eq:1}\n\\end{aligned} \\tag{1}\\]\nThis equation 1 has a non-zero solution \\(\\mathbf{v}\\) only if \\(\\operatorname{det}(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\\) — called the characteristic equation of \\(\\mathbf{A}\\). The values of \\(\\lambda\\) that satisfy this equation are the egeinvalues of \\(\\mathbf{A}\\).\nThe eigenvectors \\(\\mathbf{v}_{\\lambda=n}\\) corresponding to each eigenvalue can be found by plugging the values of \\(\\lambda\\) in equation 1.\n\n\n\nBy definition of eigenvectors, for a square matrix \\(\\mathbf{A}\\), if it has a full set of linearly independent eigenvectors (ie. it’s diagonalizable), we have\n\\[ \\mathbf{A}\\mathbf{V} = \\mathbf{V}\\mathbf{\\Lambda} \\]\nwhere \\(\\mathbf{V}\\) a matrix whose columns are the the eigenvectors of \\(\\mathbf{A}\\), and \\(\\mathbf{\\Lambda}\\) the diagonal of associated eigenvalues.\nBecause \\(\\mathbf{V}\\) is invertible, we can multiply \\(\\mathbf{V}^{-1}\\) by both sides:\n\\[ \\mathbf{A} = \\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1} \\]\n\n\n\\[ \\mathbf{A}^n = \\overbrace{\\mathbf{A}\\dots\\mathbf{A}}^{\\text{n times}} = \\overbrace{(\\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1})\\dots(\\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1})}^{\\text{n times}} = \\mathbf{V}\\overbrace{\\mathbf{\\Lambda}\\dots\\mathbf{\\Lambda}}^{\\text{n times}}\\mathbf{V}^{-1} = \\mathbf{V}\\mathbf{\\Lambda}^n\\mathbf{V}^{-1}\\]\nFor negative powers: \\[\\mathbf{A}^{-1} = \\mathbf{V}\\mathbf{\\Lambda}^{-1}\\mathbf{V}^{-1}\\]\nas long as all diagonal entry \\(\\lambda_{i,i}\\) (ie. eigenvalues) are non-zero. Since \\[\\begin{aligned}\n\\det(\\mathbf{A}) & = \\det(\\mathbf{V})\\cdot\\det(\\mathbf{\\Lambda})\\cdot\\det(\\mathbf{V}^{-1}) \\\\\n& = \\det(\\mathbf{V})\\cdot\\det(\\mathbf{\\Lambda})\\cdot\\dfrac{1}{\\det(\\mathbf{V})} \\\\\n& = \\det(\\mathbf{\\Lambda}) = \\prod_{i=1}^n \\lambda_i\n\\end{aligned}\\]\n(see determinant identities and determinant of diagonal matrix)\nThis reflects that a matrix is invertible if its determinant is 0."
  },
  {
    "objectID": "notes/linear-algebra.html#definitions-formulas",
    "href": "notes/linear-algebra.html#definitions-formulas",
    "title": "Linear Algebra",
    "section": "",
    "text": "A vector space (over a field, like \\(\\mathbb{R}\\), which provides the scalars) is a set of vectors equipped with vector addition and scalar multiplication that satisfies the following conditions:\n\nClosure under addition: \\(\\mathbf{u} + \\mathbf{v}\\) is in the space for any \\(\\mathbf{u}, \\mathbf{v}\\) in the space.\nClosure under scalar multiplication: \\(\\alpha \\mathbf{u}\\) is in the space for any scalar \\(\\alpha\\).\nAssociativity of addition: \\(\\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}\\).\nCommutativity of addition: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\).\nAdditive identity: there exists \\(\\mathbf{0}\\) such that \\(\\mathbf{u} + \\mathbf{0} = \\mathbf{u}\\).\nAdditive inverse: for each \\(\\mathbf{u}\\), there exists \\(-\\mathbf{u}\\) such that \\(\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}\\).\nCompatibility of scalar multiplication: \\(\\alpha(\\beta \\mathbf{u}) = (\\alpha\\beta) \\mathbf{u}\\).\nIdentity of scalar multiplication: \\(1 \\cdot \\mathbf{u} = \\mathbf{u}\\).\nDistributivity over vector addition: \\(\\alpha(\\mathbf{u} + \\mathbf{v}) = \\alpha \\mathbf{u} + \\alpha \\mathbf{v}\\).\nDistributivity over scalar addition: \\((\\alpha + \\beta)\\mathbf{u} = \\alpha \\mathbf{u} + \\beta \\mathbf{u}\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThis might be useful later, but for all intents and purposes in these notes, the field will be \\(\\mathbb{R}\\), vectors will be in \\(\\mathbb{R}^n\\), and addition and scalar multiplication will be element-wise.\n\n\n\n\n\n\\[ (\\mathbf{A}^\\top)_{i,j} = \\mathbf{A}_{j,i}  \\]\n\n\n\n\\[ \\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i} u_i v_i \\]\n\nu, v = torch.arange(3), torch.arange(3)  # torch.tensor([0, 1, 2])\ntorch.dot(u, v)  # u @ v\n\ntensor(5)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe dot product has a geometric meaning.\n\\[\\mathbf{v}\\cdot\\mathbf{w} = \\|\\mathbf{v}\\|\\|\\mathbf{w}\\|\\cos{\\theta}\\] \\[\\iff \\theta = \\arccos\\left(\\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}\\right).\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn linear algebra, vectors are column vectors by default, so \\[ \\mathbf{u} \\in \\mathbb{R}^{n \\times 1}, \\quad \\mathbf{u} = \\begin{bmatrix} u_1 \\\\ \\vdots \\\\ u_n \\end{bmatrix} \\]\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{u})_{i} = \\mathbf{a}_{i,:} \\cdot \\mathbf{u} = \\sum_{j} a_{i,j} u_j  \\]\n\nA, u = torch.arange(6).reshape(3, 2), torch.arange(2)\nA @ u\n\ntensor([1, 3, 5])\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen we say “we apply a linear transformation \\(\\mathbf{A}\\) to \\(\\mathbf{v}\\)”, we mean \\(\\mathbf{A}\\mathbf{v}\\).\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{B})_{i,j} = \\mathbf{a}_{i,:} \\cdot \\mathbf{b}_{:, j} = \\sum_{k} a_{i,k} b_{k,j} \\]\n\nU, V = torch.arange(6).reshape(3, 2), torch.arange(6).reshape(2, 3)\nU @ V\n\ntensor([[ 3,  4,  5],\n        [ 9, 14, 19],\n        [15, 24, 33]])\n\n\n\n\n\nNon-commutativity: most of the time \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\)\nDistributivity: \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{B}\\mathbf{C}\\) and \\((\\mathbf{A} + \\mathbf{B})\\mathbf{C} = \\mathbf{A}\\mathbf{C} + \\mathbf{B}\\mathbf{C}\\)\nAssociativity: \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\)\nTranspose: \\((\\mathbf{A}\\mathbf{B})^\\top = \\mathbf{B}^\\top\\mathbf{A}^\\top\\)\n\n\n\n\n\n\\[ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I},\\quad \\mathbf{A} \\in \\mathbb{R}^{n \\times n} \\]\n\n\n\n\n\n\nNote\n\n\n\nMight not exist if \\(\\mathbf{A}\\) is not invertible, that is \\(\\det(\\mathbf{A}) = 0\\)\n\n\n\nA = torch.randn(3, 3, dtype=torch.double)\nA_i = A.inverse()\nI = torch.eye(3, dtype=torch.double)\ntorch.allclose(A @ A_i, I), torch.allclose(A_i @ A, I)\n\n(True, True)\n\n\n\n\n\nI think of the determinant of \\(\\mathbf{A}\\) as the scaling factor of the linear transformation represented by \\(\\mathbf{A}\\).\nThis explains why a matrix whose determinant is 0 is not invertible: it “collapses”, and two images might have the same original input \\(\\mathbf{x}\\)\n\n\n\\[ \\det(\\mathbf{A}^\\top) = \\det(\\mathbf{A}) \\] \\[ \\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A})\\det(\\mathbf{B}) \\] \\[ \\det(\\mathbf{A}^{-1}) = \\dfrac{1}{\\det(\\mathbf{A})} \\iff \\det(\\mathbf{A}) \\neq 0 \\]\n\n\n\n\\[ A = \\operatorname{diag}(a_1, \\dots, a_n), \\quad \\det(A) = \\prod_{i=1}^n a_i \\]\n\n# using I as a mask\nI = torch.eye(3, dtype=torch.float32)\nA = torch.arange(1, 10, dtype=torch.float32).reshape(3, 3) * I  # A = diag(1, 5, 9)\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\\[ \\det(T) = \\prod_{i} t_{i,i}, \\quad \\text{where $T$ is triangular} \\]\n\nA = torch.triu(torch.arange(1, 10, dtype=torch.float32).reshape(3, 3))\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}\\: (= \\mathbf{A}\\mathbf{A}^\\top) \\]\nInterstingly,\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^{-1} = \\mathbf{A}^\\top \\]\n\n\n\nAn elementary matrix is a matrix obtained from the application of a single elementary row operation to the identity matrix \\(\\mathbf{I}\\).\nBecause of associativity, the product of an elementary matrix and another matrix is the same as applying the row operation to the other matrix.\n\n\nMultiplying row \\(i\\) by \\(m\\)\n\\[\\mathbf{D}_i(m) = \\operatorname{diag}(1, \\dots, 1, d_i = m, 1, \\dots, 1)\\]\n\n\n\n\\(\\mathbf{D}_i(m) = \\mathbf{D}_i(\\dfrac{1}{m})^{-1} = \\mathbf{D}_i(m)^\\top\\)\n\\(\\operatorname{det}(\\mathbf{D}_i(m)) = m\\) — obviously because it’s a diagonal matrix full of \\(1\\)’s and a single \\(m\\)\n\n\n\n\n\nAdding \\(m\\) times row \\(j\\) to row \\(i\\)\n\\[\\mathbf{L}_{ij}(m) =\n\\begin{bmatrix}\n1 &&&&&& \\\\\n& \\ddots &&&&& \\\\\n&& 1 &&&& \\\\\n&&& \\ddots &&& \\\\\n&& l_{i,j} = m && 1 && \\\\\n&&&&& \\ddots & \\\\\n&&&&&& 1\n\\end{bmatrix}\\]\n\n\n\n\\(\\mathbf{L}_{ij}(m)^{-1} = \\mathbf{L}_{ij}(-m)\\) — the inverse of adding a row multiple to another, is subtracting the same row multiple\n\\(\\mathbf{L}_{ij}(m)\\) and \\(\\mathbf{L}_{ij}(m)^{-1}\\) are triangular matrices\nSo \\(\\operatorname{det}(\\mathbf{L}_{ij}(m)) = 1 \\iff \\operatorname{det}(\\mathbf{L}_{ij}(m)\\mathbf{A}) = \\operatorname{det}(\\mathbf{A})\\)\n\n\n\n\n\nSwapping rows \\(i\\) and \\(j\\)\n\\[\\mathbf{T}_{ij} =\n\\begin{bmatrix}\n1 &&&&&& \\\\\n& \\ddots &&&&& \\\\\n&& 0 && t_{j,i} = 1 && \\\\\n&&& \\ddots &&& \\\\\n&& t_{i,j} = 1 && 0 && \\\\\n&&&&& \\ddots & \\\\\n&&&&&& 1\n\\end{bmatrix}\\]\n\n\n\n\\(\\mathbf{T}_{ij}^\\top = \\mathbf{T}_{ij}^{-1} = \\mathbf{T}_{ij}\\)\n\\(\\operatorname{det}(\\mathbf{T}_{ij}) = -1\\:^{(*)} \\iff \\operatorname{det}(\\mathbf{T}_{ij}\\mathbf{A}) = -\\operatorname{det}(\\mathbf{A})\\)\n\n\\(^{(*)}\\) Consider that swapping two rows can be expressed as applying consecutive additions/subtractions as follows \\[\\begin{aligned}\n\\mathbf{T}_{ij} & = \\mathbf{D}_{i}(-1)\\mathbf{L}_{ij}(-1)\\mathbf{L}_{ji}(1)\\mathbf{L}_{ij}(-1) \\\\\n\\iff \\operatorname{det}(\\mathbf{T}_{ij}) & = \\operatorname{det}(\\mathbf{D}_{i}(-1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ij}(-1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ji}(1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ij}(-1)) \\\\\n\\iff \\operatorname{det}(\\mathbf{T}_{ij}) & = -1 \\\\\n\\end{aligned}\\]\n\n\n\n\n\nWe say of \\(n\\) vectors — \\(\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\) that they are linearly dependent if there exists scalars \\(a_1, \\dots, a_n\\) not all equal to 0 satisfying \\[\\sum_{i=1}^n a_i\\mathbf{v}_i = 0\\]\nIf the only solution is \\(a_i = 0\\) for \\(i\\) in \\(0, \\dots, n\\), they are linearly independent.\n\n\n\n\n\n\nNote\n\n\n\nLinear dependence means that some vectors could be expressed as a weighted sum (linear combination) of others, hence carrying redudant information/operation.\n\n\n\n\n\nThe rank of a matrix is the size of the largest subset of linearly independent columns among its columns. Equivalently, it is the dimension of the column space.\n\n\n\n\n\n\nNote\n\n\n\nThis reflects that if some columns are linearly dependent, they are redundant: they do not contribute a new direction. Thus, the matrix can be expressed with fewer independent columns, and the image (output space) of the matrix has correspondingly fewer dimensions.\n\n\n\n\n\nA basis (not the, since a vector space can have many different bases) of a vector space is a linearly independent subset that spans the space.\nAll bases of the same vector space have the same size = the dimension\n\n\n\n\n\n\nNote\n\n\n\nThat means: a basis is a set of vectors that you can combine (with weighted sums) to form any vector in the space — and none of them are redundant.\n\n\n\n\n\nThe dimension of a vector space is the number of vectors in a basis of that space.\n\n\n\n\n\n\nNote\n\n\n\nSince every basis of a vector space has the same size, the dimension is well-defined. It tells you “how many independent directions” the space has.\n\n\n\n\n\n\n\nAn eigenvector is a vector whose direction (eg. “line”) is unchanged by a linear transformation (represented by a matrix). That means that when applying the linear transformation \\(\\mathbf{A}\\) to an eigeinvector \\(\\mathbf{v}\\) it simply gets scaled by a constant scalar factor, the eigenvalue \\(\\lambda\\). Represented by this eigenequation. \\[\\begin{aligned}\n\\mathbf{A}\\mathbf{v} & = \\lambda\\mathbf{v} \\\\\n\\iff (\\mathbf{A} - \\lambda\\mathbf{I})\\mathbf{v} & = \\mathbf{0} \\label{eq:1}\n\\end{aligned} \\tag{1}\\]\nThis equation 1 has a non-zero solution \\(\\mathbf{v}\\) only if \\(\\operatorname{det}(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\\) — called the characteristic equation of \\(\\mathbf{A}\\). The values of \\(\\lambda\\) that satisfy this equation are the egeinvalues of \\(\\mathbf{A}\\).\nThe eigenvectors \\(\\mathbf{v}_{\\lambda=n}\\) corresponding to each eigenvalue can be found by plugging the values of \\(\\lambda\\) in equation 1.\n\n\n\nBy definition of eigenvectors, for a square matrix \\(\\mathbf{A}\\), if it has a full set of linearly independent eigenvectors (ie. it’s diagonalizable), we have\n\\[ \\mathbf{A}\\mathbf{V} = \\mathbf{V}\\mathbf{\\Lambda} \\]\nwhere \\(\\mathbf{V}\\) a matrix whose columns are the the eigenvectors of \\(\\mathbf{A}\\), and \\(\\mathbf{\\Lambda}\\) the diagonal of associated eigenvalues.\nBecause \\(\\mathbf{V}\\) is invertible, we can multiply \\(\\mathbf{V}^{-1}\\) by both sides:\n\\[ \\mathbf{A} = \\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1} \\]\n\n\n\\[ \\mathbf{A}^n = \\overbrace{\\mathbf{A}\\dots\\mathbf{A}}^{\\text{n times}} = \\overbrace{(\\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1})\\dots(\\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1})}^{\\text{n times}} = \\mathbf{V}\\overbrace{\\mathbf{\\Lambda}\\dots\\mathbf{\\Lambda}}^{\\text{n times}}\\mathbf{V}^{-1} = \\mathbf{V}\\mathbf{\\Lambda}^n\\mathbf{V}^{-1}\\]\nFor negative powers: \\[\\mathbf{A}^{-1} = \\mathbf{V}\\mathbf{\\Lambda}^{-1}\\mathbf{V}^{-1}\\]\nas long as all diagonal entry \\(\\lambda_{i,i}\\) (ie. eigenvalues) are non-zero. Since \\[\\begin{aligned}\n\\det(\\mathbf{A}) & = \\det(\\mathbf{V})\\cdot\\det(\\mathbf{\\Lambda})\\cdot\\det(\\mathbf{V}^{-1}) \\\\\n& = \\det(\\mathbf{V})\\cdot\\det(\\mathbf{\\Lambda})\\cdot\\dfrac{1}{\\det(\\mathbf{V})} \\\\\n& = \\det(\\mathbf{\\Lambda}) = \\prod_{i=1}^n \\lambda_i\n\\end{aligned}\\]\n(see determinant identities and determinant of diagonal matrix)\nThis reflects that a matrix is invertible if its determinant is 0."
  },
  {
    "objectID": "notes/linear-algebra.html#proofs",
    "href": "notes/linear-algebra.html#proofs",
    "title": "Linear Algebra",
    "section": "Proofs",
    "text": "Proofs\nLater!"
  },
  {
    "objectID": "notes/linear-algebra.html#algorithms",
    "href": "notes/linear-algebra.html#algorithms",
    "title": "Linear Algebra",
    "section": "Algorithms",
    "text": "Algorithms\n\nGaussian Elimination\nGaussian elimination — or row reduction – is an algorithm for solving systems of linear equations, based on the following elementary row operations:\n\nScaling a row: \\(\\mathbf{R}_i \\gets k\\mathbf{R}_i\\), where \\(k \\neq 0\\)\nAdding a multiple of one row to another: \\(\\mathbf{R}_i \\gets \\mathbf{R}_i + k\\mathbf{R}_j\\), where \\(i \\neq j\\)\nSwapping two rows: \\(\\mathbf{R}_i \\leftrightarrow \\mathbf{R}_j\\)\n\n\n\nLaplace Expansion\ntodo\n\n\nLU decomposition (or LU factorization)\nComputing the determinant of a Matrix is not trivial at first glance.\nWe know how to easily compute:\n\nthe determinant of a product of matrices\nthe determinant of a elementary matrices\n\nKnowing that, we want to find a representation of our original matrix \\(\\mathbf{A}\\) that involves an Upper Triangular Matrix \\(\\mathbf{U}\\), and one or more other matrices whose determinant is known or trivial to compute, as \\(\\mathbf{P}\\mathbf{A} = \\mathbf{L}\\mathbf{U}\\)\nTo go from \\(\\mathbf{A}\\) to \\(\\mathbf{U}\\) we’ll use Gaussian Elimination, \\(\\mathbf{P}\\) tracks our permutations (row swaps) and \\(\\mathbf{L}\\) tracks our row operations (row additions).\nNow, because \\(\\mathbf{P}\\) is orthogonal, we have \\[ \\mathbf{P}\\mathbf{A} = \\mathbf{L}\\mathbf{U} \\implies \\mathbf{A} = \\mathbf{P}^{-1}\\mathbf{L}\\mathbf{U} = \\mathbf{P}^\\top\\mathbf{L}\\mathbf{U} \\]\nFinally, this means that \\[ \\det(\\mathbf{A}) = \\det(\\mathbf{P}^\\top) \\cdot \\det(\\mathbf{L}) \\cdot \\det(\\mathbf{U}) = \\det(\\mathbf{P}) \\cdot \\det(\\mathbf{L}) \\cdot \\det(\\mathbf{U}) \\]\n\n\\(\\det(\\mathbf{P}) = (-1)^{\\#swaps}\\)\n\\(\\det(\\mathbf{L}) = 1\\) — product of “multiplied row additions” elementary matrices\n\nNow, if we just keep track of row swaps, we can easily compute \\(\\det(\\mathbf{A})\\)!"
  },
  {
    "objectID": "notes/linear-algebra.html#notation",
    "href": "notes/linear-algebra.html#notation",
    "title": "Linear Algebra",
    "section": "Notation",
    "text": "Notation\n\n\\(x\\): a scalar\n\\(\\mathbf{x}\\): a vector\n\\(\\mathbf{X}\\): a matrix\n\\(x_i\\): the \\(i^\\textrm{th}\\) element of vector \\(\\mathbf{x}\\)\n\\(x_{i,j}\\): the element of matrix \\(\\mathbf{X}\\) at row \\(i\\) and column \\(j\\)\n\\(\\mathbf{x}_{i, :}\\): the \\(i^\\textrm{th}\\) row-vector of \\(\\mathbf{X}\\)\n\\(\\mathbf{x}_{:,j}\\): the \\(j^\\textrm{th}\\) column-vector of \\(\\mathbf{X}\\)\n\\(\\operatorname{diag}(a_1, \\dots, a_n)\\): a diagonal matrix\n\\(\\mathbf{I}\\): the indentity matrix\n\\(\\mathbf{0}\\): the zero vector / zero matrix — depending on context\n\\((\\cdot)^\\top\\): the transpose of a vector or matrix\n\\(\\mathbf{A}^{-1}\\): the inverse of a matrix\n\\([\\cdot, \\cdot]\\): concatenation"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "rss\n\n\n\n\n\n\n\n\n\nDistributed Training\n\n\n\nmle\n\npython\n\n\n\nDistributed training study notes and algorithms.\n\n\n\n\n\nSep 10, 2025\n\n\n\n\n\n\n\nProbability and Information Theory\n\n\n\nmath\n\n\n\nSmall Probability & Information Theory cheatsheet.\n\n\n\n\n\nSep 4, 2025\n\n\n\n\n\n\n\nPyTorch\n\n\n\npython\n\nmle\n\n\n\ntorch cheatsheet, common methods or tricks.\n\n\n\n\n\nSep 4, 2025\n\n\n\n\n\n\n\nCalculus\n\n\n\nmath\n\n\n\nAll things calculus, differential or integral.\n\n\n\n\n\nSep 4, 2025\n\n\n\n\n\n\n\nLinear Algebra\n\n\n\nmath\n\n\n\nAll things Linear Algebra, matrices, vectors etc.\n\n\n\n\n\nSep 4, 2025\n\n\n\n\n\n\n\nOn Typescript\n\n\n\ntypescript\n\n\n\nA few notes on Typescript for my future self.\n\n\n\n\n\nFeb 15, 2025\n\n\n\n\n\nNo matching items"
  }
]