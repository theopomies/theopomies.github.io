[
  {
    "objectID": "notes/pytorch.html#notes",
    "href": "notes/pytorch.html#notes",
    "title": "PyTorch",
    "section": "Notes",
    "text": "Notes\nLater!"
  },
  {
    "objectID": "notes/pytorch.html#terminology",
    "href": "notes/pytorch.html#terminology",
    "title": "PyTorch",
    "section": "Terminology",
    "text": "Terminology"
  },
  {
    "objectID": "notes/calculus.html",
    "href": "notes/calculus.html",
    "title": "Calculus",
    "section": "",
    "text": "As the name states, we analyze differences in the output(s) of a function based on differences in the input(s) \\[ \\dfrac{dy}{dx} = \\lim_{h \\rightarrow 0} \\dfrac{f(x + h) - f(x)}{h} \\]\nA function is said to be differentiable at \\(x\\) if this limit exists, and differentiable on an interval if it exists at any \\(x\\) in this interval.\n\n\nFrom that we can get common derivatives\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}C & = 0 && \\text{for any constant C} \\\\\n    \\dfrac{d}{dx}x^n & = nx^{n - 1} && \\text{for n} \\neq 0 \\\\\n    \\dfrac{d}{dx}e^x & = e^x \\\\\n    \\dfrac{d}{dx}\\ln x & = x^{-1} \\\\\n    \\dfrac{d}{dx}a^x & = \\ln(a)a^x \\\\\n    \\dfrac{d}{dx}\\cos x & = -\\sin x \\\\\n    \\dfrac{d}{dx}\\sin x & = \\cos x \\\\\n\\end{aligned} \\]\nFrom these it becomes trivial to derive \\(\\tan\\), \\(\\sec\\), \\(\\csc\\) and \\(\\cot\\).\n\n\n\nFinally a composition of differentiable functions is also differentiable, so we have the following rules that allow us to derive almost any function:\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}Cf(x) & = C\\dfrac{d}{dx}f(x) && \\text{Constant multiple rule} \\\\\n    \\dfrac{d}{dx}[f(x) + g(x)] & = \\dfrac{d}{dx}f(x) + \\dfrac{d}{dx}g(x) && \\text{Sum rule} \\\\\n    \\dfrac{d}{dx}[f(x)g(x)] & = \\dfrac{d}{dx}f(x)g(x) + f(x)\\dfrac{d}{dx}g(x) && \\text{Product rule} \\\\\n    \\dfrac{dy}{dx} & = \\dfrac{dy}{dz}\\dfrac{dz}{dx} = \\dfrac{\\frac{dy}{dz}}{\\frac{dx}{dz}} && \\text{Chain rule} \\\\\n\\end{aligned} \\]\nFrom these we can easily find a Quotient Rule, a Power Rule and a Reciprocal Rule:\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}\\dfrac{f(x)}{g(x)} & = \\dfrac{\\frac{d}{dx}f(x)g(x) - f(x)\\frac{d}{dx}g(x)}{g(x)^2} && \\text{Quotient rule} \\\\\n    \\dfrac{d}{dx}f(x)^n & = nf(x)^{n-1}\\dfrac{d}{dx}f(x) && \\text{Power rule} \\\\\n    \\dfrac{dy}{dx}\\dfrac{1}{f(x)} & = -\\dfrac{\\frac{d}{dx}f(x)}{f(x)^2} && \\text{Reciprocal rule} \\\\\n\\end{aligned} \\]\n\n\n\n\n\n\nNote\n\n\n\nBecause of the definition of derivative as a rate of change, this is possible \\(\\dfrac{dy}{dx} = \\dfrac{1}{\\frac{dx}{dy}}\\)\n\n\n\n\n\n\n\n\nVery similar to univariate calculus, but now our function takes a vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\) as input and returns a scalar \\(y \\in \\mathbb{R}\\).\nTo paraphrase D2L because their explanation is perfect:\nLet \\(y = f(x_1, x_2, \\ldots, x_n)\\) be a function with \\(n\\) variables. The partial derivative of \\(y\\) with respect to its \\(i^\\textrm{th}\\) parameter \\(x_i\\) is\n\\[ \\dfrac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.\\]\nFor \\(f \\colon \\mathbb{R}^n \\to \\mathbb{R}\\), we collect/concatenate all the partial derivatives to obtain the gradient of the output \\(y = f(\\mathbf{x})\\) with respect to the input \\(\\mathbf{x}\\) \\[ \\nabla_{\\mathbf{x}}f(\\mathbf{x}) = \\nabla_{\\mathbf{x}}y =\\begin{bmatrix} \\frac{\\partial y}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial y}{\\partial x_n} \\end{bmatrix} \\] sometimes written \\(\\nabla f(\\mathbf{x})\\) or \\(\\nabla y\\) when not ambiguous.\n\n\n\nThe Jacobian \\(\\mathbf{J} \\in \\mathbb{R}^{m \\times n}\\) is a generalization of the gradient to \\(\\mathbf{y} = f(\\mathbf{x})\\) with \\(f \\colon \\mathbb{R}^n \\to \\mathbb{R}^m\\), where \\(j_{i,j} = \\dfrac{\\partial y_i}{\\partial\\mathbf{x}_j}\\), \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\in \\mathbb{R}^n\\) and \\(\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_m \\end{bmatrix} \\in \\mathbb{R}^m\\).\nExplicitly \\[ \\displaystyle{ \\mathbf{J} = \\begin{bmatrix} \\dfrac{\\partial \\mathbf{y}}{\\partial x_{1}} & \\cdots & \\dfrac{\\partial \\mathbf{y}}{\\partial x_{n}} \\end{bmatrix}\n= \\begin{bmatrix} \\nabla^{\\top}y_{1} \\\\ \\vdots \\\\ \\nabla^{\\top}y_{m} \\end{bmatrix}\n= \\begin{bmatrix}\n\\dfrac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\dfrac{\\partial y_{1}}{\\partial x_{n}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\dfrac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\dfrac{\\partial y_{m}}{\\partial x_{n}}\n\\end{bmatrix}} \\]\n\n\n\nThe following rules come straight from D2L:\n\nFor all \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) we have \\(\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top\\) and \\(\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}\\).\nFor square matrices \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) we have that \\(\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}\\) and in particular \\(\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}\\).\n\nThen the chain rule states that\n\\[\\frac{\\partial y}{\\partial x_{i}} = \\frac{\\partial y}{\\partial u_{1}} \\frac{\\partial u_{1}}{\\partial x_{i}} + \\frac{\\partial y}{\\partial u_{2}} \\frac{\\partial u_{2}}{\\partial x_{i}} + \\ldots + \\frac{\\partial y}{\\partial u_{m}} \\frac{\\partial u_{m}}{\\partial x_{i}} \\ \\textrm{ and so } \\ \\nabla_{\\mathbf{x}} y =  \\mathbf{A} \\nabla_{\\mathbf{u}} y,\\]\nwhere \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times m}\\) is a matrix that contains the derivative of vector \\(\\mathbf{u}\\) with respect to vector \\(\\mathbf{x}\\).\n\n\n\n\nIntegrals are\n\na way to compute the signed area under a curve\nantiderivatives\na way of adding up tiny bits\n\n\\[ \\int_a^b f(x)\\,dx \\]\n\n\nThe integral \\[\\int_a^b f(x)\\,dx\\] is the limit of sums of tiny rectangular areas.\nIf we cut the interval \\([a,b]\\) into \\(n\\) equal chunks of width \\[\\Delta x = \\frac{b-a}{n},\\] then the total area is approximated by \\[\\sum_{k=1}^{n} f(a + k\\Delta x)\\,\\Delta x.\\]\nAs we make the chunks thinner (\\(n \\to \\infty\\), so \\(\\Delta x \\to 0\\)), this sum becomes exact:\n\\[\\int_a^b f(x)\\,dx = \\lim_{n\\to\\infty} \\sum_{k=1}^{n} f(a + k\\Delta x)\\,\\Delta x.\\]\n\\(\\int_a^b f(x)\\,dx\\) is a definite integral of \\(f(x)\\) from \\([a,b]\\).\n\\(\\int f(x)\\,dx\\) is an indefinite integral.\n\n\n\n\nThe fundamental theorem of calculus links differentiation (derivatives) and integration (integrals).\n\\[ \\int_a^b f(x)\\,dx = F(b) - F(a)\\]\nWhere\n\\[ \\dfrac{d}{dx}F(x) = f(x) \\]\n\n\nSay we have a function \\(A(x)\\) being the area under the curve of \\(f(x)\\) between \\(0\\) and \\(x\\).\nto find the area under the curve between \\(x\\) and \\(x+h\\), we could compute \\[ A(x+h) - A(x) \\approx f(x)h \\] \\[ \\iff \\dfrac{A(x+h) - A(x)}{h} \\approx f(x) \\] \\[ \\iff \\lim_{h \\to 0} \\dfrac{A(x+h) - A(x)}{h} = f(x) \\] \\[ \\iff \\dfrac{d}{dx}A(x) = f(x) \\]\n\n\n\n\\(dx\\) = the differential of \\(x\\)\nA single symbol that means “infinitesimal change in \\(x\\).”\nIn derivatives, it appears in a ratio (\\(\\dfrac{dy}{dx}\\)). We are differentiating \\(f(x)\\) wrt \\(x\\).\nIn integrals, it appears as a piece being added up (\\(f(x)\\,dx\\)). We are integrating \\(f(x)\\) wrt \\(x\\).\n\n\n\n\nIf \\(\\lim_{x \\to c} f(x) = \\lim_{x \\to c} g(x) = 0\\) or \\(\\pm \\infty\\), and \\(\\lim_{x \\to c} \\dfrac{\\frac{d}{dx}f(x)}{\\frac{d}{dx}g(x)}\\) exists, then\n\\[\\lim_{x \\to c} \\dfrac{f(x)}{g(x)} = \\lim_{x \\to c} \\dfrac{\\frac{d}{dx}f(x)}{\\frac{d}{dx}g(x)}\\]\n\n\n\nWe can use \\[a = b \\implies \\dfrac{d}{dx}a = \\dfrac{d}{dx}b\\] to compute derivatives of relations (linked variables eg. the equation for a circle centered at the origin and of radius 5: \\(x^2 + y^2 = 25\\))\nOr even to compute the derivative of \\(ln(x)\\):\n\\[\\begin{aligned}\ny = ln(x) & \\iff e^y = x \\implies \\dfrac{d}{dx} e^y = \\dfrac{d}{dx} x \\\\\n& \\iff \\dfrac{d}{dx} e^y = 1 \\\\\n& \\iff e^y\\dfrac{d}{dx}y = 1 \\\\\n& \\iff \\dfrac{d}{dx}y = \\dfrac{1}{e^y} \\\\\n& \\iff \\dfrac{d}{dx}y = \\dfrac{1}{e^{ln(x)}} \\\\\n& \\iff \\dfrac{d}{dx}y = \\dfrac{1}{x}\n\\end{aligned}\\]"
  },
  {
    "objectID": "notes/calculus.html#definitions-formulas",
    "href": "notes/calculus.html#definitions-formulas",
    "title": "Calculus",
    "section": "",
    "text": "As the name states, we analyze differences in the output(s) of a function based on differences in the input(s) \\[ \\dfrac{dy}{dx} = \\lim_{h \\rightarrow 0} \\dfrac{f(x + h) - f(x)}{h} \\]\nA function is said to be differentiable at \\(x\\) if this limit exists, and differentiable on an interval if it exists at any \\(x\\) in this interval.\n\n\nFrom that we can get common derivatives\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}C & = 0 && \\text{for any constant C} \\\\\n    \\dfrac{d}{dx}x^n & = nx^{n - 1} && \\text{for n} \\neq 0 \\\\\n    \\dfrac{d}{dx}e^x & = e^x \\\\\n    \\dfrac{d}{dx}\\ln x & = x^{-1} \\\\\n    \\dfrac{d}{dx}a^x & = \\ln(a)a^x \\\\\n    \\dfrac{d}{dx}\\cos x & = -\\sin x \\\\\n    \\dfrac{d}{dx}\\sin x & = \\cos x \\\\\n\\end{aligned} \\]\nFrom these it becomes trivial to derive \\(\\tan\\), \\(\\sec\\), \\(\\csc\\) and \\(\\cot\\).\n\n\n\nFinally a composition of differentiable functions is also differentiable, so we have the following rules that allow us to derive almost any function:\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}Cf(x) & = C\\dfrac{d}{dx}f(x) && \\text{Constant multiple rule} \\\\\n    \\dfrac{d}{dx}[f(x) + g(x)] & = \\dfrac{d}{dx}f(x) + \\dfrac{d}{dx}g(x) && \\text{Sum rule} \\\\\n    \\dfrac{d}{dx}[f(x)g(x)] & = \\dfrac{d}{dx}f(x)g(x) + f(x)\\dfrac{d}{dx}g(x) && \\text{Product rule} \\\\\n    \\dfrac{dy}{dx} & = \\dfrac{dy}{dz}\\dfrac{dz}{dx} = \\dfrac{\\frac{dy}{dz}}{\\frac{dx}{dz}} && \\text{Chain rule} \\\\\n\\end{aligned} \\]\nFrom these we can easily find a Quotient Rule, a Power Rule and a Reciprocal Rule:\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}\\dfrac{f(x)}{g(x)} & = \\dfrac{\\frac{d}{dx}f(x)g(x) - f(x)\\frac{d}{dx}g(x)}{g(x)^2} && \\text{Quotient rule} \\\\\n    \\dfrac{d}{dx}f(x)^n & = nf(x)^{n-1}\\dfrac{d}{dx}f(x) && \\text{Power rule} \\\\\n    \\dfrac{dy}{dx}\\dfrac{1}{f(x)} & = -\\dfrac{\\frac{d}{dx}f(x)}{f(x)^2} && \\text{Reciprocal rule} \\\\\n\\end{aligned} \\]\n\n\n\n\n\n\nNote\n\n\n\nBecause of the definition of derivative as a rate of change, this is possible \\(\\dfrac{dy}{dx} = \\dfrac{1}{\\frac{dx}{dy}}\\)\n\n\n\n\n\n\n\n\nVery similar to univariate calculus, but now our function takes a vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\) as input and returns a scalar \\(y \\in \\mathbb{R}\\).\nTo paraphrase D2L because their explanation is perfect:\nLet \\(y = f(x_1, x_2, \\ldots, x_n)\\) be a function with \\(n\\) variables. The partial derivative of \\(y\\) with respect to its \\(i^\\textrm{th}\\) parameter \\(x_i\\) is\n\\[ \\dfrac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.\\]\nFor \\(f \\colon \\mathbb{R}^n \\to \\mathbb{R}\\), we collect/concatenate all the partial derivatives to obtain the gradient of the output \\(y = f(\\mathbf{x})\\) with respect to the input \\(\\mathbf{x}\\) \\[ \\nabla_{\\mathbf{x}}f(\\mathbf{x}) = \\nabla_{\\mathbf{x}}y =\\begin{bmatrix} \\frac{\\partial y}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial y}{\\partial x_n} \\end{bmatrix} \\] sometimes written \\(\\nabla f(\\mathbf{x})\\) or \\(\\nabla y\\) when not ambiguous.\n\n\n\nThe Jacobian \\(\\mathbf{J} \\in \\mathbb{R}^{m \\times n}\\) is a generalization of the gradient to \\(\\mathbf{y} = f(\\mathbf{x})\\) with \\(f \\colon \\mathbb{R}^n \\to \\mathbb{R}^m\\), where \\(j_{i,j} = \\dfrac{\\partial y_i}{\\partial\\mathbf{x}_j}\\), \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\in \\mathbb{R}^n\\) and \\(\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_m \\end{bmatrix} \\in \\mathbb{R}^m\\).\nExplicitly \\[ \\displaystyle{ \\mathbf{J} = \\begin{bmatrix} \\dfrac{\\partial \\mathbf{y}}{\\partial x_{1}} & \\cdots & \\dfrac{\\partial \\mathbf{y}}{\\partial x_{n}} \\end{bmatrix}\n= \\begin{bmatrix} \\nabla^{\\top}y_{1} \\\\ \\vdots \\\\ \\nabla^{\\top}y_{m} \\end{bmatrix}\n= \\begin{bmatrix}\n\\dfrac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\dfrac{\\partial y_{1}}{\\partial x_{n}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\dfrac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\dfrac{\\partial y_{m}}{\\partial x_{n}}\n\\end{bmatrix}} \\]\n\n\n\nThe following rules come straight from D2L:\n\nFor all \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) we have \\(\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top\\) and \\(\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}\\).\nFor square matrices \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) we have that \\(\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}\\) and in particular \\(\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}\\).\n\nThen the chain rule states that\n\\[\\frac{\\partial y}{\\partial x_{i}} = \\frac{\\partial y}{\\partial u_{1}} \\frac{\\partial u_{1}}{\\partial x_{i}} + \\frac{\\partial y}{\\partial u_{2}} \\frac{\\partial u_{2}}{\\partial x_{i}} + \\ldots + \\frac{\\partial y}{\\partial u_{m}} \\frac{\\partial u_{m}}{\\partial x_{i}} \\ \\textrm{ and so } \\ \\nabla_{\\mathbf{x}} y =  \\mathbf{A} \\nabla_{\\mathbf{u}} y,\\]\nwhere \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times m}\\) is a matrix that contains the derivative of vector \\(\\mathbf{u}\\) with respect to vector \\(\\mathbf{x}\\).\n\n\n\n\nIntegrals are\n\na way to compute the signed area under a curve\nantiderivatives\na way of adding up tiny bits\n\n\\[ \\int_a^b f(x)\\,dx \\]\n\n\nThe integral \\[\\int_a^b f(x)\\,dx\\] is the limit of sums of tiny rectangular areas.\nIf we cut the interval \\([a,b]\\) into \\(n\\) equal chunks of width \\[\\Delta x = \\frac{b-a}{n},\\] then the total area is approximated by \\[\\sum_{k=1}^{n} f(a + k\\Delta x)\\,\\Delta x.\\]\nAs we make the chunks thinner (\\(n \\to \\infty\\), so \\(\\Delta x \\to 0\\)), this sum becomes exact:\n\\[\\int_a^b f(x)\\,dx = \\lim_{n\\to\\infty} \\sum_{k=1}^{n} f(a + k\\Delta x)\\,\\Delta x.\\]\n\\(\\int_a^b f(x)\\,dx\\) is a definite integral of \\(f(x)\\) from \\([a,b]\\).\n\\(\\int f(x)\\,dx\\) is an indefinite integral.\n\n\n\n\nThe fundamental theorem of calculus links differentiation (derivatives) and integration (integrals).\n\\[ \\int_a^b f(x)\\,dx = F(b) - F(a)\\]\nWhere\n\\[ \\dfrac{d}{dx}F(x) = f(x) \\]\n\n\nSay we have a function \\(A(x)\\) being the area under the curve of \\(f(x)\\) between \\(0\\) and \\(x\\).\nto find the area under the curve between \\(x\\) and \\(x+h\\), we could compute \\[ A(x+h) - A(x) \\approx f(x)h \\] \\[ \\iff \\dfrac{A(x+h) - A(x)}{h} \\approx f(x) \\] \\[ \\iff \\lim_{h \\to 0} \\dfrac{A(x+h) - A(x)}{h} = f(x) \\] \\[ \\iff \\dfrac{d}{dx}A(x) = f(x) \\]\n\n\n\n\\(dx\\) = the differential of \\(x\\)\nA single symbol that means “infinitesimal change in \\(x\\).”\nIn derivatives, it appears in a ratio (\\(\\dfrac{dy}{dx}\\)). We are differentiating \\(f(x)\\) wrt \\(x\\).\nIn integrals, it appears as a piece being added up (\\(f(x)\\,dx\\)). We are integrating \\(f(x)\\) wrt \\(x\\).\n\n\n\n\nIf \\(\\lim_{x \\to c} f(x) = \\lim_{x \\to c} g(x) = 0\\) or \\(\\pm \\infty\\), and \\(\\lim_{x \\to c} \\dfrac{\\frac{d}{dx}f(x)}{\\frac{d}{dx}g(x)}\\) exists, then\n\\[\\lim_{x \\to c} \\dfrac{f(x)}{g(x)} = \\lim_{x \\to c} \\dfrac{\\frac{d}{dx}f(x)}{\\frac{d}{dx}g(x)}\\]\n\n\n\nWe can use \\[a = b \\implies \\dfrac{d}{dx}a = \\dfrac{d}{dx}b\\] to compute derivatives of relations (linked variables eg. the equation for a circle centered at the origin and of radius 5: \\(x^2 + y^2 = 25\\))\nOr even to compute the derivative of \\(ln(x)\\):\n\\[\\begin{aligned}\ny = ln(x) & \\iff e^y = x \\implies \\dfrac{d}{dx} e^y = \\dfrac{d}{dx} x \\\\\n& \\iff \\dfrac{d}{dx} e^y = 1 \\\\\n& \\iff e^y\\dfrac{d}{dx}y = 1 \\\\\n& \\iff \\dfrac{d}{dx}y = \\dfrac{1}{e^y} \\\\\n& \\iff \\dfrac{d}{dx}y = \\dfrac{1}{e^{ln(x)}} \\\\\n& \\iff \\dfrac{d}{dx}y = \\dfrac{1}{x}\n\\end{aligned}\\]"
  },
  {
    "objectID": "notes/calculus.html#proofs",
    "href": "notes/calculus.html#proofs",
    "title": "Calculus",
    "section": "Proofs",
    "text": "Proofs\nLater!"
  },
  {
    "objectID": "notes/calculus.html#notation",
    "href": "notes/calculus.html#notation",
    "title": "Calculus",
    "section": "Notation",
    "text": "Notation\n\n\\(f(\\cdot)\\): a function\n\\(\\dfrac{dy}{dx}\\): derivative of \\(y\\) with respect to \\(x\\)\n\\(\\dfrac{\\partial y}{\\partial x}\\): partial derivative of \\(y\\) with respect to \\(x\\)\n\\(\\nabla_{\\mathbf{x}} y\\): gradient of \\(y\\) with respect to \\(\\mathbf{x}\\)\n\\(\\mathbf{J}_f (\\mathbf{x}), \\dfrac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\): Jacobian of \\(\\mathbf{y} = f(\\mathbf{x})\\) with respect to \\(\\mathbf{x}\\)\n\\(\\int_a^b f(x) \\;dx\\): definite integral of \\(f\\) from \\(a\\) to \\(b\\) with respect to \\(x\\)\n\\(\\int f(x) \\;dx\\): indefinite integral of \\(f\\) with respect to \\(x\\)"
  },
  {
    "objectID": "notes/linear-algebra.html",
    "href": "notes/linear-algebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "A vector space (over a field, like \\(\\mathbb{R}\\), which provides the scalars) is a set of vectors equipped with vector addition and scalar multiplication that satisfies the following conditions:\n\nClosure under addition: \\(\\mathbf{u} + \\mathbf{v}\\) is in the space for any \\(\\mathbf{u}, \\mathbf{v}\\) in the space.\nClosure under scalar multiplication: \\(\\alpha \\mathbf{u}\\) is in the space for any scalar \\(\\alpha\\).\nAssociativity of addition: \\(\\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}\\).\nCommutativity of addition: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\).\nAdditive identity: there exists \\(\\mathbf{0}\\) such that \\(\\mathbf{u} + \\mathbf{0} = \\mathbf{u}\\).\nAdditive inverse: for each \\(\\mathbf{u}\\), there exists \\(-\\mathbf{u}\\) such that \\(\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}\\).\nCompatibility of scalar multiplication: \\(\\alpha(\\beta \\mathbf{u}) = (\\alpha\\beta) \\mathbf{u}\\).\nIdentity of scalar multiplication: \\(1 \\cdot \\mathbf{u} = \\mathbf{u}\\).\nDistributivity over vector addition: \\(\\alpha(\\mathbf{u} + \\mathbf{v}) = \\alpha \\mathbf{u} + \\alpha \\mathbf{v}\\).\nDistributivity over scalar addition: \\((\\alpha + \\beta)\\mathbf{u} = \\alpha \\mathbf{u} + \\beta \\mathbf{u}\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThis might be useful later, but for all intents and purposes in these notes, the field will be \\(\\mathbb{R}\\), vectors will be in \\(\\mathbb{R}^n\\), and addition and scalar multiplication will be element-wise.\n\n\n\n\n\n\\[ (\\mathbf{A}^\\top)_{i,j} = \\mathbf{A}_{j,i}  \\]\n\n\n\n\\[ \\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i} u_i v_i \\]\n\nu, v = torch.arange(3), torch.arange(3)  # torch.tensor([0, 1, 2])\ntorch.dot(u, v)  # u @ v\n\ntensor(5)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe dot product has a geometric meaning.\n\\[\\mathbf{v}\\cdot\\mathbf{w} = \\|\\mathbf{v}\\|\\|\\mathbf{w}\\|\\cos{\\theta}\\] \\[\\iff \\theta = \\arccos\\left(\\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}\\right).\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn linear algebra, vectors are column vectors by default, so \\[ \\mathbf{u} \\in \\mathbb{R}^{n \\times 1}, \\quad \\mathbf{u} = \\begin{bmatrix} u_1 \\\\ \\vdots \\\\ u_n \\end{bmatrix} \\]\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{u})_{i} = \\mathbf{a}_{i,:} \\cdot \\mathbf{u} = \\sum_{j} a_{i,j} u_j  \\]\n\nA, u = torch.arange(6).reshape(3, 2), torch.arange(2)\nA @ u\n\ntensor([1, 3, 5])\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen we say “we apply a linear transformation \\(\\mathbf{A}\\) to \\(\\mathbf{v}\\)”, we mean \\(\\mathbf{A}\\mathbf{v}\\).\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{B})_{i,j} = \\mathbf{a}_{i,:} \\cdot \\mathbf{b}_{:, j} = \\sum_{k} a_{i,k} b_{k,j} \\]\n\nU, V = torch.arange(6).reshape(3, 2), torch.arange(6).reshape(2, 3)\nU @ V\n\ntensor([[ 3,  4,  5],\n        [ 9, 14, 19],\n        [15, 24, 33]])\n\n\n\n\n\nNon-commutativity: most of the time \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\)\nDistributivity: \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{B}\\mathbf{C}\\) and \\((\\mathbf{A} + \\mathbf{B})\\mathbf{C} = \\mathbf{A}\\mathbf{C} + \\mathbf{B}\\mathbf{C}\\)\nAssociativity: \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\)\nTranspose: \\((\\mathbf{A}\\mathbf{B})^\\top = \\mathbf{B}^\\top\\mathbf{A}^\\top\\)\n\n\n\n\n\n\\[ \\mathbf{u} \\otimes \\mathbf{v}\n= \\begin{bmatrix}\nu_1v_1 & u_1v_2 & \\dots & u_1v_n \\\\\nu_2v_1 & u_2v_2 & \\dots & u_2v_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nu_mv_1 & u_mv_2 & \\dots & u_mv_n \\\\\n\\end{bmatrix}\n= \\mathbf{u}\\mathbf{v}^\\top\\]\n\nu, v = torch.arange(3), torch.arange(3)\ntorch.outer(u, v), torch.all(torch.outer(u, v) == u.reshape(3, 1) @ v.reshape(1, 3))\n\n(tensor([[0, 0, 0],\n         [0, 1, 2],\n         [0, 2, 4]]),\n tensor(True))\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe outer product of two column vectors is the matrix whose entries are all products of an element in the first vector with an element in the second vector.\n\n\n\n\n\n\\[ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I},\\quad \\mathbf{A} \\in \\mathbb{R}^{n \\times n} \\]\n\n\n\n\n\n\nNote\n\n\n\nMight not exist if \\(\\mathbf{A}\\) is not invertible, that is \\(\\det(\\mathbf{A}) = 0\\)\n\n\n\nA = torch.randn(3, 3, dtype=torch.double)\nA_i = A.inverse()\nI = torch.eye(3, dtype=torch.double)\ntorch.allclose(A @ A_i, I), torch.allclose(A_i @ A, I)\n\n(True, True)\n\n\n\n\n\nI think of the determinant of \\(\\mathbf{A}\\) as the scaling factor of the linear transformation represented by \\(\\mathbf{A}\\).\nThis explains why a matrix whose determinant is 0 is not invertible: it “collapses”, and two images might have the same original input \\(\\mathbf{x}\\)\n\n\n\\[ \\det(\\mathbf{A}^\\top) = \\det(\\mathbf{A}) \\] \\[ \\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A})\\det(\\mathbf{B}) \\] \\[ \\det(\\mathbf{A}^{-1}) = \\dfrac{1}{\\det(\\mathbf{A})} \\iff \\det(\\mathbf{A}) \\neq 0 \\]\n\n\n\n\\[ A = \\operatorname{diag}(a_1, \\dots, a_n), \\quad \\det(A) = \\prod_{i=1}^n a_i \\]\n\n# using I as a mask\nI = torch.eye(3, dtype=torch.float32)\nA = torch.arange(1, 10, dtype=torch.float32).reshape(3, 3) * I  # A = diag(1, 5, 9)\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\\[ \\det(T) = \\prod_{i} t_{i,i}, \\quad \\text{where $T$ is triangular} \\]\n\nA = torch.triu(torch.arange(1, 10, dtype=torch.float32).reshape(3, 3))\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}\\: (= \\mathbf{A}\\mathbf{A}^\\top) \\]\nInterstingly,\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^{-1} = \\mathbf{A}^\\top \\]\n\n\n\nAn elementary matrix is a matrix obtained from the application of a single elementary row operation to the identity matrix \\(\\mathbf{I}\\).\nBecause of associativity, the product of an elementary matrix and another matrix is the same as applying the row operation to the other matrix.\n\n\nMultiplying row \\(i\\) by \\(m\\)\n\\[\\mathbf{D}_i(m) = \\operatorname{diag}(1, \\dots, 1, d_i = m, 1, \\dots, 1)\\]\n\n\n\n\\(\\mathbf{D}_i(m) = \\mathbf{D}_i(\\dfrac{1}{m})^{-1} = \\mathbf{D}_i(m)^\\top\\)\n\\(\\operatorname{det}(\\mathbf{D}_i(m)) = m\\) — obviously because it’s a diagonal matrix full of \\(1\\)’s and a single \\(m\\)\n\n\n\n\n\nAdding \\(m\\) times row \\(j\\) to row \\(i\\)\n\\[\\mathbf{L}_{ij}(m) =\n\\begin{bmatrix}\n1 &&&&&& \\\\\n& \\ddots &&&&& \\\\\n&& 1 &&&& \\\\\n&&& \\ddots &&& \\\\\n&& l_{i,j} = m && 1 && \\\\\n&&&&& \\ddots & \\\\\n&&&&&& 1\n\\end{bmatrix}\\]\n\n\n\n\\(\\mathbf{L}_{ij}(m)^{-1} = \\mathbf{L}_{ij}(-m)\\) — the inverse of adding a row multiple to another, is subtracting the same row multiple\n\\(\\mathbf{L}_{ij}(m)\\) and \\(\\mathbf{L}_{ij}(m)^{-1}\\) are triangular matrices\nSo \\(\\operatorname{det}(\\mathbf{L}_{ij}(m)) = 1 \\iff \\operatorname{det}(\\mathbf{L}_{ij}(m)\\mathbf{A}) = \\operatorname{det}(\\mathbf{A})\\)\n\n\n\n\n\nSwapping rows \\(i\\) and \\(j\\)\n\\[\\mathbf{T}_{ij} =\n\\begin{bmatrix}\n1 &&&&&& \\\\\n& \\ddots &&&&& \\\\\n&& 0 && t_{j,i} = 1 && \\\\\n&&& \\ddots &&& \\\\\n&& t_{i,j} = 1 && 0 && \\\\\n&&&&& \\ddots & \\\\\n&&&&&& 1\n\\end{bmatrix}\\]\n\n\n\n\\(\\mathbf{T}_{ij}^\\top = \\mathbf{T}_{ij}^{-1} = \\mathbf{T}_{ij}\\)\n\\(\\operatorname{det}(\\mathbf{T}_{ij}) = -1\\:^{(*)} \\iff \\operatorname{det}(\\mathbf{T}_{ij}\\mathbf{A}) = -\\operatorname{det}(\\mathbf{A})\\)\n\n\\(^{(*)}\\) Consider that swapping two rows can be expressed as applying consecutive additions/subtractions as follows \\[\\begin{aligned}\n\\mathbf{T}_{ij} & = \\mathbf{D}_{i}(-1)\\mathbf{L}_{ij}(-1)\\mathbf{L}_{ji}(1)\\mathbf{L}_{ij}(-1) \\\\\n\\iff \\operatorname{det}(\\mathbf{T}_{ij}) & = \\operatorname{det}(\\mathbf{D}_{i}(-1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ij}(-1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ji}(1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ij}(-1)) \\\\\n\\iff \\operatorname{det}(\\mathbf{T}_{ij}) & = -1 \\\\\n\\end{aligned}\\]\n\n\n\n\n\nWe say of \\(n\\) vectors — \\(\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\) that they are linearly dependent if there exists scalars \\(a_1, \\dots, a_n\\) not all equal to 0 satisfying \\[\\sum_{i=1}^n a_i\\mathbf{v}_i = 0\\]\nIf the only solution is \\(a_i = 0\\) for \\(i\\) in \\(0, \\dots, n\\), they are linearly independent.\n\n\n\n\n\n\nNote\n\n\n\nLinear dependence means that some vectors could be expressed as a weighted sum (linear combination) of others, hence carrying redudant information/operation.\n\n\n\n\n\nThe rank of a matrix is the size of the largest subset of linearly independent columns among its columns. Equivalently, it is the dimension of the column space.\n\n\n\n\n\n\nNote\n\n\n\nThis reflects that if some columns are linearly dependent, they are redundant: they do not contribute a new direction. Thus, the matrix can be expressed with fewer independent columns, and the image (output space) of the matrix has correspondingly fewer dimensions.\n\n\n\n\n\nA basis (not the, since a vector space can have many different bases) of a vector space is a linearly independent subset that spans the space.\nAll bases of the same vector space have the same size = the dimension\n\n\n\n\n\n\nNote\n\n\n\nThat means: a basis is a set of vectors that you can combine (with weighted sums) to form any vector in the space — and none of them are redundant.\n\n\n\n\n\nThe dimension of a vector space is the number of vectors in a basis of that space.\n\n\n\n\n\n\nNote\n\n\n\nSince every basis of a vector space has the same size, the dimension is well-defined. It tells you “how many independent directions” the space has.\n\n\n\n\n\n\n\nAn eigenvector is a vector whose direction (eg. “line”) is unchanged by a linear transformation (represented by a matrix). That means that when applying the linear transformation \\(\\mathbf{A}\\) to an eigeinvector \\(\\mathbf{v}\\) it simply gets scaled by a constant scalar factor, the eigenvalue \\(\\lambda\\). Represented by this eigenequation. \\[\\begin{aligned}\n\\mathbf{A}\\mathbf{v} & = \\lambda\\mathbf{v} \\\\\n\\iff (\\mathbf{A} - \\lambda\\mathbf{I})\\mathbf{v} & = \\mathbf{0} \\label{eq:1}\n\\end{aligned} \\tag{1}\\]\nThis equation 1 has a non-zero solution \\(\\mathbf{v}\\) only if \\(\\operatorname{det}(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\\) — called the characteristic equation of \\(\\mathbf{A}\\). The values of \\(\\lambda\\) that satisfy this equation are the egeinvalues of \\(\\mathbf{A}\\).\nThe eigenvectors \\(\\mathbf{v}_{\\lambda=n}\\) corresponding to each eigenvalue can be found by plugging the values of \\(\\lambda\\) in equation 1.\n\n\n\nBy definition of eigenvectors, for a square matrix \\(\\mathbf{A}\\), if it has a full set of linearly independent eigenvectors (ie. it’s diagonalizable), we have\n\\[ \\mathbf{A}\\mathbf{V} = \\mathbf{V}\\mathbf{\\Lambda} \\]\nwhere \\(\\mathbf{V}\\) a matrix whose columns are the the eigenvectors of \\(\\mathbf{A}\\), and \\(\\mathbf{\\Lambda}\\) the diagonal of associated eigenvalues.\nBecause \\(\\mathbf{V}\\) is invertible, we can multiply \\(\\mathbf{V}^{-1}\\) by both sides:\n\\[ \\mathbf{A} = \\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1} \\]\n\n\n\\[ \\mathbf{A}^n = \\overbrace{\\mathbf{A}\\dots\\mathbf{A}}^{\\text{n times}} = \\overbrace{(\\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1})\\dots(\\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1})}^{\\text{n times}} = \\mathbf{V}\\overbrace{\\mathbf{\\Lambda}\\dots\\mathbf{\\Lambda}}^{\\text{n times}}\\mathbf{V}^{-1} = \\mathbf{V}\\mathbf{\\Lambda}^n\\mathbf{V}^{-1}\\]\nFor negative powers: \\[\\mathbf{A}^{-1} = \\mathbf{V}\\mathbf{\\Lambda}^{-1}\\mathbf{V}^{-1}\\]\nas long as all diagonal entry \\(\\lambda_{i,i}\\) (ie. eigenvalues) are non-zero. Since \\[\\begin{aligned}\n\\det(\\mathbf{A}) & = \\det(\\mathbf{V})\\cdot\\det(\\mathbf{\\Lambda})\\cdot\\det(\\mathbf{V}^{-1}) \\\\\n& = \\det(\\mathbf{V})\\cdot\\det(\\mathbf{\\Lambda})\\cdot\\dfrac{1}{\\det(\\mathbf{V})} \\\\\n& = \\det(\\mathbf{\\Lambda}) = \\prod_{i=1}^n \\lambda_i\n\\end{aligned}\\]\n(see determinant identities and determinant of diagonal matrix)\nThis reflects that a matrix is invertible if its determinant is 0."
  },
  {
    "objectID": "notes/linear-algebra.html#definitions-formulas",
    "href": "notes/linear-algebra.html#definitions-formulas",
    "title": "Linear Algebra",
    "section": "",
    "text": "A vector space (over a field, like \\(\\mathbb{R}\\), which provides the scalars) is a set of vectors equipped with vector addition and scalar multiplication that satisfies the following conditions:\n\nClosure under addition: \\(\\mathbf{u} + \\mathbf{v}\\) is in the space for any \\(\\mathbf{u}, \\mathbf{v}\\) in the space.\nClosure under scalar multiplication: \\(\\alpha \\mathbf{u}\\) is in the space for any scalar \\(\\alpha\\).\nAssociativity of addition: \\(\\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}\\).\nCommutativity of addition: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\).\nAdditive identity: there exists \\(\\mathbf{0}\\) such that \\(\\mathbf{u} + \\mathbf{0} = \\mathbf{u}\\).\nAdditive inverse: for each \\(\\mathbf{u}\\), there exists \\(-\\mathbf{u}\\) such that \\(\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}\\).\nCompatibility of scalar multiplication: \\(\\alpha(\\beta \\mathbf{u}) = (\\alpha\\beta) \\mathbf{u}\\).\nIdentity of scalar multiplication: \\(1 \\cdot \\mathbf{u} = \\mathbf{u}\\).\nDistributivity over vector addition: \\(\\alpha(\\mathbf{u} + \\mathbf{v}) = \\alpha \\mathbf{u} + \\alpha \\mathbf{v}\\).\nDistributivity over scalar addition: \\((\\alpha + \\beta)\\mathbf{u} = \\alpha \\mathbf{u} + \\beta \\mathbf{u}\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThis might be useful later, but for all intents and purposes in these notes, the field will be \\(\\mathbb{R}\\), vectors will be in \\(\\mathbb{R}^n\\), and addition and scalar multiplication will be element-wise.\n\n\n\n\n\n\\[ (\\mathbf{A}^\\top)_{i,j} = \\mathbf{A}_{j,i}  \\]\n\n\n\n\\[ \\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i} u_i v_i \\]\n\nu, v = torch.arange(3), torch.arange(3)  # torch.tensor([0, 1, 2])\ntorch.dot(u, v)  # u @ v\n\ntensor(5)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe dot product has a geometric meaning.\n\\[\\mathbf{v}\\cdot\\mathbf{w} = \\|\\mathbf{v}\\|\\|\\mathbf{w}\\|\\cos{\\theta}\\] \\[\\iff \\theta = \\arccos\\left(\\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}\\right).\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn linear algebra, vectors are column vectors by default, so \\[ \\mathbf{u} \\in \\mathbb{R}^{n \\times 1}, \\quad \\mathbf{u} = \\begin{bmatrix} u_1 \\\\ \\vdots \\\\ u_n \\end{bmatrix} \\]\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{u})_{i} = \\mathbf{a}_{i,:} \\cdot \\mathbf{u} = \\sum_{j} a_{i,j} u_j  \\]\n\nA, u = torch.arange(6).reshape(3, 2), torch.arange(2)\nA @ u\n\ntensor([1, 3, 5])\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen we say “we apply a linear transformation \\(\\mathbf{A}\\) to \\(\\mathbf{v}\\)”, we mean \\(\\mathbf{A}\\mathbf{v}\\).\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{B})_{i,j} = \\mathbf{a}_{i,:} \\cdot \\mathbf{b}_{:, j} = \\sum_{k} a_{i,k} b_{k,j} \\]\n\nU, V = torch.arange(6).reshape(3, 2), torch.arange(6).reshape(2, 3)\nU @ V\n\ntensor([[ 3,  4,  5],\n        [ 9, 14, 19],\n        [15, 24, 33]])\n\n\n\n\n\nNon-commutativity: most of the time \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\)\nDistributivity: \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{B}\\mathbf{C}\\) and \\((\\mathbf{A} + \\mathbf{B})\\mathbf{C} = \\mathbf{A}\\mathbf{C} + \\mathbf{B}\\mathbf{C}\\)\nAssociativity: \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\)\nTranspose: \\((\\mathbf{A}\\mathbf{B})^\\top = \\mathbf{B}^\\top\\mathbf{A}^\\top\\)\n\n\n\n\n\n\\[ \\mathbf{u} \\otimes \\mathbf{v}\n= \\begin{bmatrix}\nu_1v_1 & u_1v_2 & \\dots & u_1v_n \\\\\nu_2v_1 & u_2v_2 & \\dots & u_2v_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nu_mv_1 & u_mv_2 & \\dots & u_mv_n \\\\\n\\end{bmatrix}\n= \\mathbf{u}\\mathbf{v}^\\top\\]\n\nu, v = torch.arange(3), torch.arange(3)\ntorch.outer(u, v), torch.all(torch.outer(u, v) == u.reshape(3, 1) @ v.reshape(1, 3))\n\n(tensor([[0, 0, 0],\n         [0, 1, 2],\n         [0, 2, 4]]),\n tensor(True))\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe outer product of two column vectors is the matrix whose entries are all products of an element in the first vector with an element in the second vector.\n\n\n\n\n\n\\[ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I},\\quad \\mathbf{A} \\in \\mathbb{R}^{n \\times n} \\]\n\n\n\n\n\n\nNote\n\n\n\nMight not exist if \\(\\mathbf{A}\\) is not invertible, that is \\(\\det(\\mathbf{A}) = 0\\)\n\n\n\nA = torch.randn(3, 3, dtype=torch.double)\nA_i = A.inverse()\nI = torch.eye(3, dtype=torch.double)\ntorch.allclose(A @ A_i, I), torch.allclose(A_i @ A, I)\n\n(True, True)\n\n\n\n\n\nI think of the determinant of \\(\\mathbf{A}\\) as the scaling factor of the linear transformation represented by \\(\\mathbf{A}\\).\nThis explains why a matrix whose determinant is 0 is not invertible: it “collapses”, and two images might have the same original input \\(\\mathbf{x}\\)\n\n\n\\[ \\det(\\mathbf{A}^\\top) = \\det(\\mathbf{A}) \\] \\[ \\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A})\\det(\\mathbf{B}) \\] \\[ \\det(\\mathbf{A}^{-1}) = \\dfrac{1}{\\det(\\mathbf{A})} \\iff \\det(\\mathbf{A}) \\neq 0 \\]\n\n\n\n\\[ A = \\operatorname{diag}(a_1, \\dots, a_n), \\quad \\det(A) = \\prod_{i=1}^n a_i \\]\n\n# using I as a mask\nI = torch.eye(3, dtype=torch.float32)\nA = torch.arange(1, 10, dtype=torch.float32).reshape(3, 3) * I  # A = diag(1, 5, 9)\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\\[ \\det(T) = \\prod_{i} t_{i,i}, \\quad \\text{where $T$ is triangular} \\]\n\nA = torch.triu(torch.arange(1, 10, dtype=torch.float32).reshape(3, 3))\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}\\: (= \\mathbf{A}\\mathbf{A}^\\top) \\]\nInterstingly,\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^{-1} = \\mathbf{A}^\\top \\]\n\n\n\nAn elementary matrix is a matrix obtained from the application of a single elementary row operation to the identity matrix \\(\\mathbf{I}\\).\nBecause of associativity, the product of an elementary matrix and another matrix is the same as applying the row operation to the other matrix.\n\n\nMultiplying row \\(i\\) by \\(m\\)\n\\[\\mathbf{D}_i(m) = \\operatorname{diag}(1, \\dots, 1, d_i = m, 1, \\dots, 1)\\]\n\n\n\n\\(\\mathbf{D}_i(m) = \\mathbf{D}_i(\\dfrac{1}{m})^{-1} = \\mathbf{D}_i(m)^\\top\\)\n\\(\\operatorname{det}(\\mathbf{D}_i(m)) = m\\) — obviously because it’s a diagonal matrix full of \\(1\\)’s and a single \\(m\\)\n\n\n\n\n\nAdding \\(m\\) times row \\(j\\) to row \\(i\\)\n\\[\\mathbf{L}_{ij}(m) =\n\\begin{bmatrix}\n1 &&&&&& \\\\\n& \\ddots &&&&& \\\\\n&& 1 &&&& \\\\\n&&& \\ddots &&& \\\\\n&& l_{i,j} = m && 1 && \\\\\n&&&&& \\ddots & \\\\\n&&&&&& 1\n\\end{bmatrix}\\]\n\n\n\n\\(\\mathbf{L}_{ij}(m)^{-1} = \\mathbf{L}_{ij}(-m)\\) — the inverse of adding a row multiple to another, is subtracting the same row multiple\n\\(\\mathbf{L}_{ij}(m)\\) and \\(\\mathbf{L}_{ij}(m)^{-1}\\) are triangular matrices\nSo \\(\\operatorname{det}(\\mathbf{L}_{ij}(m)) = 1 \\iff \\operatorname{det}(\\mathbf{L}_{ij}(m)\\mathbf{A}) = \\operatorname{det}(\\mathbf{A})\\)\n\n\n\n\n\nSwapping rows \\(i\\) and \\(j\\)\n\\[\\mathbf{T}_{ij} =\n\\begin{bmatrix}\n1 &&&&&& \\\\\n& \\ddots &&&&& \\\\\n&& 0 && t_{j,i} = 1 && \\\\\n&&& \\ddots &&& \\\\\n&& t_{i,j} = 1 && 0 && \\\\\n&&&&& \\ddots & \\\\\n&&&&&& 1\n\\end{bmatrix}\\]\n\n\n\n\\(\\mathbf{T}_{ij}^\\top = \\mathbf{T}_{ij}^{-1} = \\mathbf{T}_{ij}\\)\n\\(\\operatorname{det}(\\mathbf{T}_{ij}) = -1\\:^{(*)} \\iff \\operatorname{det}(\\mathbf{T}_{ij}\\mathbf{A}) = -\\operatorname{det}(\\mathbf{A})\\)\n\n\\(^{(*)}\\) Consider that swapping two rows can be expressed as applying consecutive additions/subtractions as follows \\[\\begin{aligned}\n\\mathbf{T}_{ij} & = \\mathbf{D}_{i}(-1)\\mathbf{L}_{ij}(-1)\\mathbf{L}_{ji}(1)\\mathbf{L}_{ij}(-1) \\\\\n\\iff \\operatorname{det}(\\mathbf{T}_{ij}) & = \\operatorname{det}(\\mathbf{D}_{i}(-1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ij}(-1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ji}(1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ij}(-1)) \\\\\n\\iff \\operatorname{det}(\\mathbf{T}_{ij}) & = -1 \\\\\n\\end{aligned}\\]\n\n\n\n\n\nWe say of \\(n\\) vectors — \\(\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\) that they are linearly dependent if there exists scalars \\(a_1, \\dots, a_n\\) not all equal to 0 satisfying \\[\\sum_{i=1}^n a_i\\mathbf{v}_i = 0\\]\nIf the only solution is \\(a_i = 0\\) for \\(i\\) in \\(0, \\dots, n\\), they are linearly independent.\n\n\n\n\n\n\nNote\n\n\n\nLinear dependence means that some vectors could be expressed as a weighted sum (linear combination) of others, hence carrying redudant information/operation.\n\n\n\n\n\nThe rank of a matrix is the size of the largest subset of linearly independent columns among its columns. Equivalently, it is the dimension of the column space.\n\n\n\n\n\n\nNote\n\n\n\nThis reflects that if some columns are linearly dependent, they are redundant: they do not contribute a new direction. Thus, the matrix can be expressed with fewer independent columns, and the image (output space) of the matrix has correspondingly fewer dimensions.\n\n\n\n\n\nA basis (not the, since a vector space can have many different bases) of a vector space is a linearly independent subset that spans the space.\nAll bases of the same vector space have the same size = the dimension\n\n\n\n\n\n\nNote\n\n\n\nThat means: a basis is a set of vectors that you can combine (with weighted sums) to form any vector in the space — and none of them are redundant.\n\n\n\n\n\nThe dimension of a vector space is the number of vectors in a basis of that space.\n\n\n\n\n\n\nNote\n\n\n\nSince every basis of a vector space has the same size, the dimension is well-defined. It tells you “how many independent directions” the space has.\n\n\n\n\n\n\n\nAn eigenvector is a vector whose direction (eg. “line”) is unchanged by a linear transformation (represented by a matrix). That means that when applying the linear transformation \\(\\mathbf{A}\\) to an eigeinvector \\(\\mathbf{v}\\) it simply gets scaled by a constant scalar factor, the eigenvalue \\(\\lambda\\). Represented by this eigenequation. \\[\\begin{aligned}\n\\mathbf{A}\\mathbf{v} & = \\lambda\\mathbf{v} \\\\\n\\iff (\\mathbf{A} - \\lambda\\mathbf{I})\\mathbf{v} & = \\mathbf{0} \\label{eq:1}\n\\end{aligned} \\tag{1}\\]\nThis equation 1 has a non-zero solution \\(\\mathbf{v}\\) only if \\(\\operatorname{det}(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\\) — called the characteristic equation of \\(\\mathbf{A}\\). The values of \\(\\lambda\\) that satisfy this equation are the egeinvalues of \\(\\mathbf{A}\\).\nThe eigenvectors \\(\\mathbf{v}_{\\lambda=n}\\) corresponding to each eigenvalue can be found by plugging the values of \\(\\lambda\\) in equation 1.\n\n\n\nBy definition of eigenvectors, for a square matrix \\(\\mathbf{A}\\), if it has a full set of linearly independent eigenvectors (ie. it’s diagonalizable), we have\n\\[ \\mathbf{A}\\mathbf{V} = \\mathbf{V}\\mathbf{\\Lambda} \\]\nwhere \\(\\mathbf{V}\\) a matrix whose columns are the the eigenvectors of \\(\\mathbf{A}\\), and \\(\\mathbf{\\Lambda}\\) the diagonal of associated eigenvalues.\nBecause \\(\\mathbf{V}\\) is invertible, we can multiply \\(\\mathbf{V}^{-1}\\) by both sides:\n\\[ \\mathbf{A} = \\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1} \\]\n\n\n\\[ \\mathbf{A}^n = \\overbrace{\\mathbf{A}\\dots\\mathbf{A}}^{\\text{n times}} = \\overbrace{(\\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1})\\dots(\\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1})}^{\\text{n times}} = \\mathbf{V}\\overbrace{\\mathbf{\\Lambda}\\dots\\mathbf{\\Lambda}}^{\\text{n times}}\\mathbf{V}^{-1} = \\mathbf{V}\\mathbf{\\Lambda}^n\\mathbf{V}^{-1}\\]\nFor negative powers: \\[\\mathbf{A}^{-1} = \\mathbf{V}\\mathbf{\\Lambda}^{-1}\\mathbf{V}^{-1}\\]\nas long as all diagonal entry \\(\\lambda_{i,i}\\) (ie. eigenvalues) are non-zero. Since \\[\\begin{aligned}\n\\det(\\mathbf{A}) & = \\det(\\mathbf{V})\\cdot\\det(\\mathbf{\\Lambda})\\cdot\\det(\\mathbf{V}^{-1}) \\\\\n& = \\det(\\mathbf{V})\\cdot\\det(\\mathbf{\\Lambda})\\cdot\\dfrac{1}{\\det(\\mathbf{V})} \\\\\n& = \\det(\\mathbf{\\Lambda}) = \\prod_{i=1}^n \\lambda_i\n\\end{aligned}\\]\n(see determinant identities and determinant of diagonal matrix)\nThis reflects that a matrix is invertible if its determinant is 0."
  },
  {
    "objectID": "notes/linear-algebra.html#proofs",
    "href": "notes/linear-algebra.html#proofs",
    "title": "Linear Algebra",
    "section": "Proofs",
    "text": "Proofs\nLater!"
  },
  {
    "objectID": "notes/linear-algebra.html#algorithms",
    "href": "notes/linear-algebra.html#algorithms",
    "title": "Linear Algebra",
    "section": "Algorithms",
    "text": "Algorithms\n\nGaussian Elimination\nGaussian elimination — or row reduction – is an algorithm for solving systems of linear equations, based on the following elementary row operations:\n\nScaling a row: \\(\\mathbf{R}_i \\gets k\\mathbf{R}_i\\), where \\(k \\neq 0\\)\nAdding a multiple of one row to another: \\(\\mathbf{R}_i \\gets \\mathbf{R}_i + k\\mathbf{R}_j\\), where \\(i \\neq j\\)\nSwapping two rows: \\(\\mathbf{R}_i \\leftrightarrow \\mathbf{R}_j\\)\n\n\n\nLaplace Expansion\ntodo\n\n\nLU decomposition (or LU factorization)\nComputing the determinant of a Matrix is not trivial at first glance.\nWe know how to easily compute:\n\nthe determinant of a product of matrices\nthe determinant of a elementary matrices\n\nKnowing that, we want to find a representation of our original matrix \\(\\mathbf{A}\\) that involves an Upper Triangular Matrix \\(\\mathbf{U}\\), and one or more other matrices whose determinant is known or trivial to compute, as \\(\\mathbf{P}\\mathbf{A} = \\mathbf{L}\\mathbf{U}\\)\nTo go from \\(\\mathbf{A}\\) to \\(\\mathbf{U}\\) we’ll use Gaussian Elimination, \\(\\mathbf{P}\\) tracks our permutations (row swaps) and \\(\\mathbf{L}\\) tracks our row operations (row additions).\nNow, because \\(\\mathbf{P}\\) is orthogonal, we have \\[ \\mathbf{P}\\mathbf{A} = \\mathbf{L}\\mathbf{U} \\implies \\mathbf{A} = \\mathbf{P}^{-1}\\mathbf{L}\\mathbf{U} = \\mathbf{P}^\\top\\mathbf{L}\\mathbf{U} \\]\nFinally, this means that \\[ \\det(\\mathbf{A}) = \\det(\\mathbf{P}^\\top) \\cdot \\det(\\mathbf{L}) \\cdot \\det(\\mathbf{U}) = \\det(\\mathbf{P}) \\cdot \\det(\\mathbf{L}) \\cdot \\det(\\mathbf{U}) \\]\n\n\\(\\det(\\mathbf{P}) = (-1)^{\\#swaps}\\)\n\\(\\det(\\mathbf{L}) = 1\\) — product of “multiplied row additions” elementary matrices\n\nNow, if we just keep track of row swaps, we can easily compute \\(\\det(\\mathbf{A})\\)!"
  },
  {
    "objectID": "notes/linear-algebra.html#notation",
    "href": "notes/linear-algebra.html#notation",
    "title": "Linear Algebra",
    "section": "Notation",
    "text": "Notation\n\n\\(x\\): a scalar\n\\(\\mathbf{x}\\): a vector\n\\(\\mathbf{X}\\): a matrix\n\\(x_i\\): the \\(i^\\textrm{th}\\) element of vector \\(\\mathbf{x}\\)\n\\(x_{i,j}\\): the element of matrix \\(\\mathbf{X}\\) at row \\(i\\) and column \\(j\\)\n\\(\\mathbf{x}_{i, :}\\): the \\(i^\\textrm{th}\\) row-vector of \\(\\mathbf{X}\\)\n\\(\\mathbf{x}_{:,j}\\): the \\(j^\\textrm{th}\\) column-vector of \\(\\mathbf{X}\\)\n\\(\\operatorname{diag}(a_1, \\dots, a_n)\\): a diagonal matrix\n\\(\\mathbf{I}\\): the indentity matrix\n\\(\\mathbf{0}\\): the zero vector / zero matrix — depending on context\n\\((\\cdot)^\\top\\): the transpose of a vector or matrix\n\\(\\mathbf{A}^{-1}\\): the inverse of a matrix\n\\([\\cdot, \\cdot]\\): concatenation\n\\(\\odot\\): Hadamard (elementwise) product\n\\(\\otimes\\): Outer product"
  },
  {
    "objectID": "notes/distributed-training.html",
    "href": "notes/distributed-training.html",
    "title": "Distributed Training",
    "section": "",
    "text": "send and recv to send or receive a tensor synchronously — from/to a single rank.\nAnd their async counterparts, isend and irecv.\n\nrank = dist.get_rank()\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\nif rank == 0:\n    request = dist.isend(tensor, 1)\n    ...\n    # can do something else, like more sends for example!\n    ...\n    request.wait() # now block until it's been fulfilled\nelif rank == 1:\n    dist.recv(tensor, 0) # recv is synchronous, so it will block until tensor is fully received\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n\n\n\n\nCollective operations allow communication (data-transfer) from All-&gt;Point, Point-&gt;All and All-&gt;All.\n\n\n\n\nBroadcast (torch.distributed.broadcast(tensor, src, ...)) allows a rank to broadcast a tensor to the whole group.\n\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\ndist.broadcast(tensor, src=0)\n\nprint(f\"After: {tensor})\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 2 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n\n\n\n\nScatter (torch.distributed.scatter(tensor, scatter_list, src, ...)) allows us to scatter — split and broadcast different chunks of — a tensor from a rank to the whole group.\n\ntensor = torch.zeros(3)\nscatter_list = [torch.arange(3 * i, 3 * i + 3) if rank == 0 else torch.zeros(3) for i in range(world_size)]\n\nprint(f\"Scatter list: {scatter_list}\")\nprint(f\"Before: {tensor}\")\n\ndist.scatter(tensor, scatter_list, src=0)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nScatter list: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nScatter list: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([3, 4, 5])\n\n\n\n\n\n\n\n\nReduce (torch.distributed.reduce(tensor, dst, op, ...)) performs a reduction operation (N-&gt;1, eg. sum, max, min, prod, …) and the dst rank receives the result.\n\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 4, 5])\n\n\n\n\n\nGather (torch.distributed.gather(tensor, gather_list, dst, ...)) gathers — pulls — a tensor, of the same size, from every rank and stores them in a list in a single rank.\n\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.gather(tensor, gather_list, dst=0)\n\nprint(f\"After: {gather_list}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([3, 4, 5])\n\n========== rank 1 ==========\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nAfter: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\n\n\n\n\n\n\n\n\nAll-Reduce (torch.distributed.all_reduce(tensor, op, ...)) performs a reduction operation, like reduce, but every rank receives the result — rather than a single one with reduce. Think of it as reduce + broadcast — though it is optimized by techniques like ring-reduce.\n\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 5, 7])\n\n\n\n\n\nAll-Gather (torch.distributed.all_gather(tensor, gather_list, ...)) gathers — pulls — a tensor, of the same size, from every rank and stores them in a list in every rank. Think of it as running gather on all ranks.\n\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.all_gather(tensor, gather_list)\n\nprint(f\"After: {gather_list}\")\n\n\n\n========== rank 2 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n\n\n\n\n\nReduce-Scatter (torch.distributed.reduce_scatter(output_tensor, input_list, op, ...)) performs a reduction operation — like other reduce functions — and scatters the resulting tensor. Think of it like reduce + scatter.\n\n\n\n\n\n\nNote\n\n\n\nIt needs len(input_list) == world_size and every tensor in input_list to have the same shape of output_tensor.\n\n\n\ntensor = torch.zeros(2)\nscatter_list = [torch.tensor([(rank + 1) * i for i in range(1, 3)]) ** (j + 1) for j in range(3)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {scatter_list}\")\n\ndist.reduce_scatter(tensor, scatter_list, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([1, 2]), tensor([1, 4]), tensor([1, 8])]\nAfter: tensor([ 6, 12])\n\n========== rank 1 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([2, 4]), tensor([ 4, 16]), tensor([ 8, 64])]\nAfter: tensor([14, 56])\n\n========== rank 2 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([3, 6]), tensor([ 9, 36]), tensor([ 27, 216])]\nAfter: tensor([ 36, 288])\n\n\n\n\n\n\n\n\nBarrier (torch.distributed.barrier(...)) synchronizes all ranks (processes), waiting for all of them to reach the barrier. (think of .join() for threads or processes)"
  },
  {
    "objectID": "notes/distributed-training.html#functions-methods",
    "href": "notes/distributed-training.html#functions-methods",
    "title": "Distributed Training",
    "section": "",
    "text": "send and recv to send or receive a tensor synchronously — from/to a single rank.\nAnd their async counterparts, isend and irecv.\n\nrank = dist.get_rank()\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\nif rank == 0:\n    request = dist.isend(tensor, 1)\n    ...\n    # can do something else, like more sends for example!\n    ...\n    request.wait() # now block until it's been fulfilled\nelif rank == 1:\n    dist.recv(tensor, 0) # recv is synchronous, so it will block until tensor is fully received\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n\n\n\n\nCollective operations allow communication (data-transfer) from All-&gt;Point, Point-&gt;All and All-&gt;All.\n\n\n\n\nBroadcast (torch.distributed.broadcast(tensor, src, ...)) allows a rank to broadcast a tensor to the whole group.\n\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\ndist.broadcast(tensor, src=0)\n\nprint(f\"After: {tensor})\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 2 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n\n\n\n\nScatter (torch.distributed.scatter(tensor, scatter_list, src, ...)) allows us to scatter — split and broadcast different chunks of — a tensor from a rank to the whole group.\n\ntensor = torch.zeros(3)\nscatter_list = [torch.arange(3 * i, 3 * i + 3) if rank == 0 else torch.zeros(3) for i in range(world_size)]\n\nprint(f\"Scatter list: {scatter_list}\")\nprint(f\"Before: {tensor}\")\n\ndist.scatter(tensor, scatter_list, src=0)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nScatter list: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nScatter list: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([3, 4, 5])\n\n\n\n\n\n\n\n\nReduce (torch.distributed.reduce(tensor, dst, op, ...)) performs a reduction operation (N-&gt;1, eg. sum, max, min, prod, …) and the dst rank receives the result.\n\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 4, 5])\n\n\n\n\n\nGather (torch.distributed.gather(tensor, gather_list, dst, ...)) gathers — pulls — a tensor, of the same size, from every rank and stores them in a list in a single rank.\n\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.gather(tensor, gather_list, dst=0)\n\nprint(f\"After: {gather_list}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([3, 4, 5])\n\n========== rank 1 ==========\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nAfter: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\n\n\n\n\n\n\n\n\nAll-Reduce (torch.distributed.all_reduce(tensor, op, ...)) performs a reduction operation, like reduce, but every rank receives the result — rather than a single one with reduce. Think of it as reduce + broadcast — though it is optimized by techniques like ring-reduce.\n\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 5, 7])\n\n\n\n\n\nAll-Gather (torch.distributed.all_gather(tensor, gather_list, ...)) gathers — pulls — a tensor, of the same size, from every rank and stores them in a list in every rank. Think of it as running gather on all ranks.\n\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.all_gather(tensor, gather_list)\n\nprint(f\"After: {gather_list}\")\n\n\n\n========== rank 2 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n\n\n\n\n\nReduce-Scatter (torch.distributed.reduce_scatter(output_tensor, input_list, op, ...)) performs a reduction operation — like other reduce functions — and scatters the resulting tensor. Think of it like reduce + scatter.\n\n\n\n\n\n\nNote\n\n\n\nIt needs len(input_list) == world_size and every tensor in input_list to have the same shape of output_tensor.\n\n\n\ntensor = torch.zeros(2)\nscatter_list = [torch.tensor([(rank + 1) * i for i in range(1, 3)]) ** (j + 1) for j in range(3)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {scatter_list}\")\n\ndist.reduce_scatter(tensor, scatter_list, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([1, 2]), tensor([1, 4]), tensor([1, 8])]\nAfter: tensor([ 6, 12])\n\n========== rank 1 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([2, 4]), tensor([ 4, 16]), tensor([ 8, 64])]\nAfter: tensor([14, 56])\n\n========== rank 2 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([3, 6]), tensor([ 9, 36]), tensor([ 27, 216])]\nAfter: tensor([ 36, 288])\n\n\n\n\n\n\n\n\nBarrier (torch.distributed.barrier(...)) synchronizes all ranks (processes), waiting for all of them to reach the barrier. (think of .join() for threads or processes)"
  },
  {
    "objectID": "notes/distributed-training.html#algorithms-techniques",
    "href": "notes/distributed-training.html#algorithms-techniques",
    "title": "Distributed Training",
    "section": "Algorithms / Techniques",
    "text": "Algorithms / Techniques\n\nData Sharding\nData Sharding is the process of sharding — splitting — the dataset / dataloader so that each rank only pulls their own unique mini-batches of the training data. This avoids duplicates and is more commucation / memory efficient that duplicating the same full dataset on every rank. To do this with torch, setup the DataLoader with sampler=[instance of DistributedSampler].\n\n\nTypes of parallelism\nThe goal of parallelism is to maximize throughput and cluster utilization.\n\n\n\n\nData Parallelism (DP): Each rank has a replica of the model — they’re replicants — and receives a different mini-batch. After optional [Gradient Accumulation] , gradients are averaged across ranks (all_reduce).\nPipeline Parallelism (PP): The model is split along the layers. Each rank has 1+ consecutive layers of the model, and we orchestrate sequential forward/backward passes along the ranks. This is inter-layer parallelism.\nTensor Parallelism (TP): The model’s layers themselves are split across ranks. We need more complex orchestration since a single tensor’s values are scattered across different ranks. This is intra-layer parallelism.\nExpert Parallelism (EP): A specific type of TP where we only split the experts of an MoE across ranks.\n\n\n\n\n\n\n\nImportant\n\n\n\nZeRO/FSDP is not a parallelism strategy in the strict sense, but a memory-optimization strategy. It’s a highly memory-efficient form DP.\n\nParallelism = distributing computation to increase throughput.\nMemory optimization (eg. ZeRO/FSDP) = sharding model states (parameters, gradients, optimizer states) across ranks so the model fits in memory, while each rank still computes the full forward and backward pass.\n\nThus:\n\nWith ZeRO/FSDP, every rank executes the full network computation but stores only a shard of the model states.\nWith TP/EP/PP, computation itself is partitioned across ranks, and the combined work reconstructs the whole forward/backward pass.\n\nThese approaches are complementary and usually combined in large-scale training.\n\n\n\n\nDDP — Distributed Data Parallelism\nDistributed Data Parallelism is a type of parallelism where each rank loads a copy — replica — of the model, after each optimizer step they always all have the same parameters, they are replicants. Each rank then trains on a different mini-batch (hence the importance of data sharding). We then average the gradients (all_reduce sum + division by world_size or avg operation if available), perform a step of gradient descent, rinse and repeat. If we can use this, we should, it has the least amount of overhead, but it requires that the model + optimizer states all fit in the device’s VRAM.\n\n\n\n\n\n\nNote\n\n\n\nThe difference between DDP and DP is that DDP uses processes to avoid the GIL and DP uses threads. Do not use DP, only DDP.\n\n\nclass SimpleDataParaellism():\n    def __init__(self, model):\n        self.model = model\n\n        for param in model.parameters():\n            rank_0_params = param.data.clone()\n            dist.broadcast(rank_0_params, src=0)\n            assert torch.equal(param.data, rank_0_params), \"Parameters mismatch at initialization\"\n\n    def sync_grad(self):\n        for param in model.parameters():\n            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG) # only available on NCCL backend\n            # eq.\n            # dist.all_reduce(param.grad, op=dist.ReduceOp.SUM)\n            # param.grad /= dist.get_world_size()\n\n\n\n\n\n\nNote\n\n\n\nThe above is a Toy implementation, in reality you do not waste time and resources by doing a single all_reduce at the end. This leaves GPUs idle. You interleave computations and communications\n\n\n\n\nZeRO / FSDP\nZero Redudency Optimizer (ZeRO) by DeepSpeed is a modeling strategy involving sharding states and parameters during training as a mean of optimizing peak memory. The core idea is that the optimizer states, gradients and/or model parameters are sharded, retrieved only when necessary for some computation, then anything we do not use anymore is discarded.\nFully Sharded Data Parallelism (FSDP) is PyTorch’s implementation of ZeRo.\nPaper Article FSDP Paper FSDP Doc\n\nZeRO-1\nZeRO stage 1 (aka. \\(P_{os}\\)) is the sharding/partitioning of optimizer states only. 4x memory reduction, communication volume of the same order as DP (gradient all-reduce dominates).\nForward pass\n\nSame as DP: each rank stores the full model parameters and runs the full forward pass.\n\nBackward pass\n\nSame as DP: each rank computes all gradients locally.\nSame as DP: gradients are averaged across ranks via all_reduce.\n\n\n\n\n\n\n\nNote\n\n\n\nCan be a reduce_scatter too\n\n\nOptimizer step\n\nEach rank holds the full parameters and full averaged gradients.\nEach rank updates only the parameter shard corresponding to its shard of the optimizer state.\nUpdated parameter shards are then exchanged (all_gather) so all ranks end up with the full updated model.\n\n\n\nZeRO-2\nZeRO stage 2 (aka. \\(P_{os} + P_g\\)) is the sharding/partitioning of optimizer states and gradient. 8x memory reduction, communication volume of the same order as DP and ZeRO-1.\nForward pass\n\nSame as DP: each rank stores the full model parameters and runs the full forward pass.\n\nBackward pass\n\nEach rank computes gradients locally, so gradients are temporarily materialized on every rank. This means ZeRO-2 has the same peak memory as ZeRO-1, but 8x lower persistent memory.\nGradients are averaged and sharded across ranks (reduce_scatter) — think averaging + sending to each rank the shard of the gradients that corresponds exactly to its optimizer state\n\nOptimizer step\n\nEach rank holds the full parameters.\nEach rank holds only the averaged gradients corresponding to its shard of the optimizer state.\nEach rank updates only the parameter shard corresponding to its shard of the optimizer state.\nUpdated parameter shards are then exchanged (all_gather) so all ranks end up with the full updated model.\n\n\n\nZeRO-3\nZeRO stage 3 (aka. \\(P_{os} + P_g + P_p\\)) is the sharding/partitioning of optimizer states and gradient and model parameters. Memory reduction scales linearly with our parallelism degree, larger communication overhead (≈50% more than DP/ZeRO-1/2) — (need to all_gather and reduce_scatter parameters before and after every computation requiring them).\n(Assuming FP16 params and FP32 optimizer states)\nForward pass\n\nEach rank stores its shard of the model parameters.\nWhenever a parameter is needed for computation, it is materialized (all_gather from its shard)\nThe computation is done\nThe local param is released/flushed (del/=None) on every rank but the one owning it\n\nBackward pass\n\nEach rank runs the backward pass for its full model replica, but parameters must be all-gathered on demand.\nGradients are produced during backprop, then immediately reduce-scattered so only the owning rank keeps the shard.\n\nOptimizer step\n\nEach rank holds only the parameters’ shard corresponding to its shard of the gradients and optimizer state.\nEach rank holds only the averaged gradients’ shard corresponding to its shard of the parameters and optimizer state.\nEach rank updates only the parameter shard corresponding to its shard of the optimizer state.\nUpdated parameter shards are then exchanged (all_gather) so all ranks end up with the full updated model."
  },
  {
    "objectID": "notes/distributed-training.html#terminology",
    "href": "notes/distributed-training.html#terminology",
    "title": "Distributed Training",
    "section": "Terminology",
    "text": "Terminology\n\ndevice: Hardware unit — GPU \"cuda:0\", CPU \"cpu\" etc. that’s where tensors and computations live\nnode: Phyisical machine/server (or VPS whatever) that has 1+ devices\nprocess: Python process/worker, executing a copy of the code/script — often on a single device (GPU)\nrank: ID of a process — often that maps to a single device. rank without qualifiers is global rank\nworld: Set of all processes part of our current distributed job\nglobal rank, world rank: rank across all processes/nodes. note: collective operations take the global rank (or just rank) as input for src/dst\nlocal rank: rank within a single node (node not group). note: device takes the local rank \"cuda:{local_rank}\"\ngroup: subset of processes (1+ nodes) that we’ve grouped for sub-communications. note: we still use global rank for intra-group communication."
  },
  {
    "objectID": "notes/distributed-training.html#resources-references-bookmarks",
    "href": "notes/distributed-training.html#resources-references-bookmarks",
    "title": "Distributed Training",
    "section": "Resources / References / Bookmarks",
    "text": "Resources / References / Bookmarks\n\nHF UltraScale Playbook\ntorch.distributed doc, RTFM"
  },
  {
    "objectID": "journal/2025-09-13.html",
    "href": "journal/2025-09-13.html",
    "title": "Connectionism and Cursor articles",
    "section": "",
    "text": "Today is the Lord’s day so I try not to do heavy work, and only personal stuff, nothing regular job related. That was the perfect occasion to read two articles from my reading list.\n\nConnectionism - Defeating Nondeterminism in LLM Inference\nThinking Machines, Mira Muramati’s very well funded superintelligence lab started their blog with their first article by Horace He tackling (non-)determinism — ie. reproducibility — in current LLMs. The root cause is the non-associative nature of floating-point arithmetic — eg. \\((x + y) + z \\neq x + (y + z)\\).\nThis issue arises because of two main components of our inference systems:\n\nConcurrency: the order in which threads finish has repercussions on the order of operations. They note that this is minimal and steps to avoid it are known and good eng. implement them\nNon batch-invariant kernels: some kernels implementations vary depending on batch size, and at inference we the users have no control over the actual batch size being fed to the LLM, we are pooled with other users. This can be solved by implementing batch-invariant kernels, notably for RMSNorm, Matmul and Attention, but requires engineering efforts.\n\n\n\nCursor Tab online RL article\nCursor recently released an interesting article where they announce the release of their updated tab completion model, Cursor Tab, with improved acceptance rate and suggestion rate — sometimes not suggesting anything is the right suggestion.\nThey explain the way they perform online on-policy RL, with rolling releases of the model every 1.5-2h — this is required because in order to have on-policy RL, the rewards collected must come from the current policy, the most recently updated one. They are aiming for even faster release/training cycle. Is this a way to have continual learning?"
  },
  {
    "objectID": "journal/2025-09-15.html",
    "href": "journal/2025-09-15.html",
    "title": "Reviewing Daniel Han’s guest lecture + Backprop Ninja",
    "section": "",
    "text": "Daniel’s Triton Kernels lecture\nI wanted to briefly review Daniel’s lecture from last week on Triton kernels from first principles, going through simplifying derivatives for LLM blocks and using calculus tricks to write custom backprop kernels in triton rather than letting the autograd to its magic.\n\n\nBackprop ninja\nThis reminded me of this lecture by Andrej Karpaty in his NN Zero To Hero series on youtube. So of course, just for practice and to get a stronger grasp on backprop, I did the exercises, implemented the entire backprop by hand (and Apple Pen + iPad). As a first principled kind of guy, there’s nothing better than going this low, understanding where the \\(\\mathbf{W}^\\top\\) comes from in the derivatives by writing all the dot products and derivating the loss wrt the inputs/weights and seeing the pattern emerge.\nI went through the playlist 2 years ago already, in my first wave of interest for ML, and it’s been good getting back in the game. I’m feeling nostalgic, I miss the good parts about living in New York and being locked in.\n\n\nMisc\nI saw the leaks for the incoming Meta Connect keynote, about the new Meta x {RayBan, Oakley}, and I can’t wait for us to enter in the era of good AR glasses and embedded AI assistants! This reminds me of a theory I saw on X that the iPhone 17 Air serves many very interesting purposes to Apple, the first being for “fashion” and taking this customer base away from the Pros, but also this is a crash test on miniaturization, and it will finance the progress and experiments towards including this kind of tech in Apple’s own AR glasses, or even using the thinness for foldable phones.\nGood time to be optimist about tech."
  },
  {
    "objectID": "journal/2025-09-18.html",
    "href": "journal/2025-09-18.html",
    "title": "ZeRO-2 + Arctic Sequence Length Training presentation by Tunji Ruwase from Snowflake",
    "section": "",
    "text": "ZeRo-1 -&gt; ZeRO-2\nWell, today I realized my toy implementation of ZeRO-1 was not very scalable, I flattened the optimizer states and split them so that each rank owned complete state tensors. This is no-bueno because very unbalanced, for 2 GPUs, rank 0 owned 99% of the total size, for 3 optimizer state tensors, and rank 1 owned 1% of the total size, for 3 state tensors too! Furthermore, it is a poor abstraction for my implementation of ZeRO-2. Now I’m flattening the weights, and sharding the tensors themselves.\n\n\nASLT by Tunji Ruwase\nOnce again, as part of Scratch To Scale, we’ve had another wonderful lecture by yet another industry expert, Tunji Ruwase from Snowflake, previously from Microsoft DeepSpeed! This technique allows the tiling of sequences to expand training to much longer sequences that would otherwise OOM. I’m not gonna lie, the level of these lectures is slightly out of my current knowledge frontier, but it’s great because it forces me to pull knowledge on-demande and learn very fast."
  },
  {
    "objectID": "journal/2025-09-25.html",
    "href": "journal/2025-09-25.html",
    "title": "Catching up on classes, DiLoCo, Decentralized Training and Expert Parallelism",
    "section": "",
    "text": "The last 2 days have been really busy, so I couldn’t even attend to the 3 classes that took place then. I caught up all of them this saturday, but it’s fine, I was already familiar with the topics\n\nDiLoCo\nThe first guest lecture was DiLoCo by Zach Charles from Google. DiLoCo is a distributed, internet-scale training strategy, very similar to federated training. Basically, on local nodes you implement regular DDP / other paralellism. Then, after H timesteps, you exchange parameters, and perform an outer-gradient step. The idea is that naively averaging parameters degrades performance, but trying to find the overall gradient over the H timesteps, and combining them, allows to find the distributed overall gradient, which is more pertinent. Overlapping some of the training examples allows to introduce a larger overlap of distributions (and gradients).\nTo further optimize this decentralized techine, researchers introduced Streaming DiLoCo, a variant, once again, overlapping computations and communications\n\n\nDecentralized Training\nThen, Sami Jaghouar from Prime Intellect discussed Decentralized Training overall, with DiLoCo, OpenDiLoCo (their version) and Intellect-1, their 10B model trained across 3 continents (!!!). He also discussed applying such decentralized patterns to RL, which is even easier, with one cluster/node performing inference, another the reward model, and yet another the training models. He insisted on the importance of fault tolerance, as the risk of having GPU failures grows with the number of GPU, and becomes daily occurences with frontier scale clusters.\n\n\nExpert Parallelism\nFinaly, Matej Sirovatka from GPU MODE explained in more details the math behind MoE and Expert Parellism, a form of tensor parallelism where the experts of a MoE are scattered across ranks."
  },
  {
    "objectID": "journal/2025-09-24.html",
    "href": "journal/2025-09-24.html",
    "title": "Async Tensor Parallelism with Less Wright + Efficient Strategies for Distributed Inference with Marc Sun",
    "section": "",
    "text": "Async TP\nYesterday we learned a bit more about TP, and today thanks to Less Wright we learned about Async TP. The idea is to decompose our operations (comm and computations) into more finegrained operations (say instead of a big matmul that would require receiving the full tensor, we do smaller matmuls and receive a sharded input slice by slice). We also have 2 streams, a computation stream performing matmuls (and other kernels) and a communication stream, that way we can do both in parallel and not waste cycles.\nHowever there is a “quantization wave” (see this article) at the end of matmuls, and because we split the work to interleave compute and comms, we have alot of matmuls. A solution is to swap the roles of the streams at the end of the first computations, having the compute stream become the comms stream and vice-versa.\n\n\nEfficient Strategies for Distributed Inference\nThen Marc Sun gave us a very complete talk on optimizing inference. This inside vLLM article and this Accelerating PyTorch inference article should cover most of the topics. We discussed topics such as prefill vs decode phase, KV Caching, PagedAttention, torch.compile, Quantization, speculative decoding, continuous batching, prefix caching, TP/PP/DP."
  },
  {
    "objectID": "journal/2025-09-02.html",
    "href": "journal/2025-09-02.html",
    "title": "Starting Scratch To Scale (S2S) and Dive into Deep Learning (D2L)",
    "section": "",
    "text": "Today I revised the basics of calculus on D2L, and ended the day with the first “lesson” on Distributed Training (S2S) by Zach Mueller.\n\nCalculus\nI worked a bit on calculus, and there’s always something to learn, even we you go as far back as the high-school level stuff. Small example, it’s just today that I realized that \\(\\dfrac{dx}{dy} = \\dfrac{1}{\\frac{dy}{dx}}\\) (cf. the definition of derivative as a limit).\nI also got my hands back into multivariate calculus and learning useful identities.\n\n\nDistributed Training (S2S)\nFinally, I finished the day learning the basics of distributed/parallel processing/training on GPUs (using torch.distributed, we’re not yet at the triton or CUDA level, but someday we’ll be there, just watch).\nWe went from the primitives — (i)send and (i)recv — to the collective operations — reduce, all_reduce, scatter, reduce_scatter, broadcast, barrier, all2all, gather, all_gather. I can now much more easily conceive how distributed training algorithms work.\nI learned a few distributed training concepts, such as the rank.\nI concluded the day by running my first notebooks accelerated by more than 1 GPU on Modal. I’d done some lightly GPU accelerated stuff on Kaggle, but now I could grasp how to do stuff with multiple GPUs."
  },
  {
    "objectID": "journal/2025-09-04.html",
    "href": "journal/2025-09-04.html",
    "title": "Going through DDP with S2S",
    "section": "",
    "text": "Finishing the preliminaries for D2L\nWell, finally I’m done with the “catching up” for D2L, with a good refresher on Probabilities. Since I studied stats and probabilities at university, I didn’t learn anything new here, but it’s always good to refresh some knowledge, and re-derive formulas, to have them in mind and grok or accept them as true. Like the classic Bayes Theorem, I always accepted the formula as a way to flip conditional probabilities, but rarely as a way of updating a prior.\n\n\nEvening study session on DDP\nWe discussed the different types of parallelism (for ML distributed training):\n\n(Distributed) Data Parallelism (DDP): Each rank loads a copy — replica — of the model, after each optimizer step they always all have the same parameters, they are replicants. Each rank then trains on a different mini-batch (hence the importance of data sharding). We then average the gradients, perform a step of gradient descent, rinse and repeat. If we can use this, we should, it has the least amount of overhead, but it requires that the model + optimizer states all fit in the device’s VRAM.\nPipeline Parallelism (PP): We split the model across different ranks without splitting the layers (so we split along the layers). That is inter-layer parallelism. An exaggeration would be having a model with 2 hidden layers and 1 output layer split across 3 GPUs.\nTensor Parallelism (TP): We split the layers of the model across different ranks, that’s intra-layer parallelism. This could be useful if some layers are so large they don’t even fit in a single device. To reduce overhead, it’s advisable that even if a layer is split across different ranks, all parts remain on the same node. (See terminology)\n\nThere’s also\n\nExpert Parallelism (EP): For Mixture-of-Experts (MoE) we can split the experts on different devices.\n\nWe didn’t discuss much this last one but I knew about it already and searched a bit to know more about it.\nLast but not least we discussed the issue of DataLoaders / Samplers, and the importance of data sharding. They are a mean of “splitting” efficiently our dataset so that each rank sees a different mini-batch of data. Also, each process only loads its shard so it’s more memory/communication efficient."
  },
  {
    "objectID": "journal/2025-09-05.html",
    "href": "journal/2025-09-05.html",
    "title": "Ray / Anyscale workshop with S2S",
    "section": "",
    "text": "Going down the ray rabbit hole\nToday we had a workshop on Ray x Anyscale by Robert Nishihara — co-founder and CEO of Anyscale, co-creator of Ray — himself!\nAt first I thought this was just gonna be a cloud ad. Like “look this is our platform, here’s how to use it, please do” (lol). I didn’t know Ray, since I don’t know much — yet — about distributed workloads.\nThe workshop was nice, we covered Ray Data and Ray Train, their data and training libraries. These looked like powerful stuff, but I was still cautious about it, they looked like some frameworks that were too high level, too much abstraction for me, à la fast.ai (it’s good, but too high level for me sometimes, I like the from scratch feel of some other stuff, the tweakability).\nDuring the workshop someone asked about using Ray + vLLM (LLM inference engine). I thought “one’s for training, the other for inference, I don’t see the intersection here”. Oh boy was I wrong. After seing the Anyscale employes answer, I realized I didn’t fully grasp what Ray was and what it offered. So, naturally, I started digging. Know I know that Ray is fully OSS, it’s a library for distributed pythonic applications. Ray Core is just that, and that’s already a lot, it allows us to create remote Tasks and Actors, very good parallel / distributed primitives. Then they built higher-level utilities above that, with Ray {Data, Train, Tune and Serve}.\nI had badly misjuged it, I feel like we’re gonna become very close friends.\nBefore I use a lib I like to know what it does and how it does it, but once I’ll have built a minimal toy version, it’s gonna be you and me buddy.\n\n\nLapace Expansion\nTo finish the day, I wrote a quick Laplace Expansion function in DeepML."
  },
  {
    "objectID": "journal/2025-09-12.html",
    "href": "journal/2025-09-12.html",
    "title": "Finished eigendecomposition notes + Jacobian",
    "section": "",
    "text": "Eigendecomposition\nI had learned about diagonalization/decomposition using eigenvectors and eigenvalues in The Manga Guide to Linear Algebra. Now reading more about it in the appendix of D2L, I felt ready to write my notes about them, and after doing a little more research that’s what I did, and I feel I’ve got a strong grasp on the subject.\n\n\nJacobian\nWith the little free time I still had in this (once again) busy day, I wrote my notes on the Jacobian, a generalization of the gradient to functions with vector inputs and outputs."
  },
  {
    "objectID": "journal/2025-09-16.html",
    "href": "journal/2025-09-16.html",
    "title": "Some more probability & statistics and a ZeRO3 + HSDP lesson (S2S)",
    "section": "",
    "text": "Covariance + Random Vectors + outer product\nToday I tried to reaaaally understand what covariance is, and I think I did. This led me to covariane matrices, which led me to the expectation and variance for random vectors — basically component-wise expectation and (co)variance. This in turn led me to outer products for a quick digression.\nThat’s it, it’s not much but it’s honest work. I tried to really understand what they were and why the way it’s computed makes sense.\n\n\nZeRO3 + HSDP\nThen in the evening we had a lesson on ZeRO3 (ZeRO with sharded optimizer states, gradients and model params) and HSDP (FSDP with Hybrid Shards). Not gonna lie, it was a bit dense, and it’s only gonna increase in complexity. I’ve been naughty and did not do too much homework on these lessons, tomorrow I’ll make up for it and grind hard on ZeRO{1,2,3}/{F,H}SDP, to fully grok it.\nJuggling regular job, gym, learning/writing math for DL and S2S is intense, but I know that’s the zone where I thrive and reach my potential."
  },
  {
    "objectID": "journal/2025-09-01.html",
    "href": "journal/2025-09-01.html",
    "title": "Finished reading The Manga Guide to Linear Algebra",
    "section": "",
    "text": "I had a lot of work to do for my day job, we are trying to build and commercialize V2 of our intercom system, and I was faced with technical difficulties that left me very little time to work on math / ML. I still found the time to finish The Manga Guide to Linear Algebra, and learned a few important concepts and techniques!\n\nStill, I did a few things\nTIL about [subspaces], [linear spans], the [rank] of a matrix and a technique to find it via Gaussian Eliminination — as usual, linear algebra appears to be the art of applying Gaussian Elimination correctly —, [Eigenvalues] and [Eigenvectors]! That was still packed!\nI skimmed through the materials for Scratch To Scale, the course I’m following by Scott Mueller, beginning tomorrow."
  },
  {
    "objectID": "journal/2025-09-22.html",
    "href": "journal/2025-09-22.html",
    "title": "Regular job + slight progress on ZeRO-2/3",
    "section": "",
    "text": "Regular Job\nWe’ve had some issues with our SIM provider resulting in intermittent loss of network on our edge devices, so I’m working on issues to mitigate and enhance our resilience infra.\nAs a result of developping a new product from scratch in-house, we’ve worked on a design overhaul of the entire intercom system, which I’ve already fully implemented. But we’ve decided to ship this update to our existing system. That’s running on a completely different tech stack, so I have to duplicate the work and effort, leading to little time for ML.\n\n\nZeRO-2/3\nNot much to add. I’ve begun writing my toy FSDP implementation, not much done yet. I did hit a wall trying to run the same script on my macbook pro and on Modal. torchrun would fail with cryptic issues about IPv6, here’s the line to run the script working:\nGLOO_SOCKET_FAMILY=inet PYTORCH_ENABLE_MPS_FALLBACK=1 uv run -m torch.distributed.run --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29500 --nproc_per_node=2 &lt;script.py&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Theo POMIES",
    "section": "",
    "text": "I’m Théo, a graduate of EPITECH and currently CTO of a tech startup, Kivala. My background is in AI, large language models, backend development, and data engineering. Over the past years, I’ve worked on building robust backends with Python and FastAPI, designing automation pipelines, and deploying AI systems ranging from custom LLMs to retrieval-augmented generation (RAG) architectures.\nMy technical work usually revolves around:"
  },
  {
    "objectID": "index.html#transition-to-mle-applied-research",
    "href": "index.html#transition-to-mle-applied-research",
    "title": "Theo POMIES",
    "section": "Transition to MLE & Applied Research",
    "text": "Transition to MLE & Applied Research\nI’m now in the process of transitioning towards machine learning engineering and applied research. This blog is where I’ll share my journey — notes from my learning process, experiments, and articles about the concepts and techniques I find valuable along the way.\nIt’s part notebook, part knowledge base, and part exploration. My goal is to document both the practice of building systems and the research side of AI that makes them possible."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "rss\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "journal/2025-09-19.html",
    "href": "journal/2025-09-19.html",
    "title": "DataLoader Workshop and some more ZeRO research",
    "section": "",
    "text": "Light work today on the ML side, big day at regular job, lots to do, it do be like that sometimes, and it’s important to keep my priorities straight and aligned. I have the remember the opportunity that I have beign CTO and partner at Kivala, I’ve cried in gratitude, I’ve prayed for this opportunity, Theo from 2 years wanted to be where I am, so even though I feel shiny object syndrom about AI/ML and the field’s pillars are super interesting, I should not forget that I’m still a value-creation machine and I should first and foremost build Kivala, and sell it for a hefty price.\n\nDataLoader Workshop\nStill, in the evening I went through the newly uploaded DataLoader workshop, once again as part of Scratch To Scale.\nFairly interesting but I didn’t learn much, I already had a strong understanding on the inner workings of a DataLoader and how to write a distributed one. Basically we just pull data index by rank so each rank gets its own mini-batch and there’s no intersection.\n\n\nZeRO\nFinally still on the ZeRO side, I’ve explored better ways to implement it, better ways to wrap the model. I still feel like the gap between ease of toy implementation and production implementation are huge and I’m not sure I’ll be able to build a production-ready, generci and flexible, ZeRO implementation. But I can say for sure that now I think I really do understand the algorithm much better than I did a few days ago."
  },
  {
    "objectID": "journal/2025-09-23.html",
    "href": "journal/2025-09-23.html",
    "title": "PP and TP with Scrach to Scale",
    "section": "",
    "text": "Pipeline Parallelism\nPipeline Parallelism is a parallelism strategy that consists in distributing the model along layers on different ranks, and have the forward pass go sequentialy through the ranks. Think of it as having 1+ layers on each GPU, input in the first, get the activations, put them through the second etc.\nThe trick now is orchestration. A naive implementation would be to process the full forward, and then the full backward (GPipe). But you incur a lot of idle time (Bubble).\nOne solution is “1 forward 1 backward” (1f1b), where you interleave the forward and backward passes. This reminded me of DeepSeek’s Open Source Week where they released DualPipe\n\n\nTensor Parallelism\nTP is an intra-layer parallelism strategy that consists of sharding a layer/tensor across different ranks. This makes sense when you think of matmul, you can split a tensor, perform two matmuls (split_n @ input) and concatenate (for column parallel) or add (for row parallel) the results to recover the original output .\nTP is useful when the model is pretty wide AND you have NVLink. Without NVLink you get drowned in communications and the gains vanish."
  },
  {
    "objectID": "journal/index.html",
    "href": "journal/index.html",
    "title": "Journal",
    "section": "",
    "text": "rss\n\n\n\n\n\n\n\n\n\nCatching up on classes, DiLoCo, Decentralized Training and Expert Parallelism\n\n\n\nmle\n\npython\n\n\n\nToday I took the saturday to catch up on Thu and Fri’s lessons, missed because of how busy I’ve been\n\n\n\n\n\nSep 27, 2025\n\n2 min\n\n\n\n\n\n\nAsync Tensor Parallelism with Less Wright + Efficient Strategies for Distributed Inference with Marc Sun\n\n\n\nmle\n\npython\n\n\n\nTwo super interesting guest lectures from Scratch To Scale, on Async TP and Distributed Inference\n\n\n\n\n\nSep 24, 2025\n\n1 min\n\n\n\n\n\n\nPP and TP with Scrach to Scale\n\n\n\nmle\n\npython\n\n\n\nLearning about Pipeline Paralellism and Tensor Parallelism\n\n\n\n\n\nSep 23, 2025\n\n1 min\n\n\n\n\n\n\nRegular job + slight progress on ZeRO-2/3\n\n\n\nmle\n\npython\n\n\n\nFixing prod issues at Kivala, working on a design overhaul of the product + progress on ZeRO-2 and 3\n\n\n\n\n\nSep 22, 2025\n\n1 min\n\n\n\n\n\n\nDataLoader Workshop and some more ZeRO research\n\n\n\nmle\n\npython\n\n\n\nWent through the new DataLoader (Sharding) Workshop for S2S and searched more techniques to make my ZeRO implementation better\n\n\n\n\n\nSep 19, 2025\n\n2 min\n\n\n\n\n\n\nZeRO-2 + Arctic Sequence Length Training presentation by Tunji Ruwase from Snowflake\n\n\n\nmle\n\npython\n\n\n\nWriting the backbone for ZeRO-2 and an isntructive call with Tunji Ruwase from Snowflake introducing Arctic Sequence Length Training as part of S2S\n\n\n\n\n\nSep 18, 2025\n\n1 min\n\n\n\n\n\n\nImplementing ZeRO-1 + Lectures on DTensor/DeviceMesh and Parallel Processing\n\n\n\nmle\n\npython\n\n\n\nImplemented DP and ZeRO1 from scratch in a Modal notebook followed by two guest lectures, DTensor/DeviceMesh and Parallel Processing, as part of S2S\n\n\n\n\n\nSep 17, 2025\n\n2 min\n\n\n\n\n\n\nSome more probability & statistics and a ZeRO3 + HSDP lesson (S2S)\n\n\n\nmath\n\nmle\n\npython\n\n\n\nWrote about expectation/variance for random vectors and covariance matrix, followed by the S2S course on ZeRO3 and HSDP\n\n\n\n\n\nSep 16, 2025\n\n1 min\n\n\n\n\n\n\nReviewing Daniel Han’s guest lecture + Backprop Ninja\n\n\n\nmath\n\nmle\n\n\n\nReviewing Daniel Han’s triton kernel lecture and going through backprop ninja (again) by Andrej Karpathy\n\n\n\n\n\nSep 15, 2025\n\n2 min\n\n\n\n\n\n\nConnectionism and Cursor articles\n\n\n\nmath\n\nmle\n\n\n\nRead Thinking Machines’s first blog article + cursor’s article on online RL training for Cursor Tab\n\n\n\n\n\nSep 13, 2025\n\n2 min\n\n\n\n\n\n\nFinished eigendecomposition notes + Jacobian\n\n\n\nmath\n\n\n\nWrote some notes on eigendecomposition and the Jacobian matrix\n\n\n\n\n\nSep 12, 2025\n\n1 min\n\n\n\n\n\n\nWriting triton kernels with Daniel Han from Unsloth + linear algebra\n\n\n\nmle\n\nmath\n\n\n\nA guest speaker session with Daniel Han on writig triton kernel (S2S), a light mode to this website and new linear algebra notes\n\n\n\n\n\nSep 11, 2025\n\n1 min\n\n\n\n\n\n\nFP8 Training with Phuc Nguyen (HF)\n\n\n\nmle\n\n\n\nEvening guest speaker: FP8 Training with Phuc (S2S)\n\n\n\n\n\nSep 10, 2025\n\n1 min\n\n\n\n\n\n\nZeRO / FSDP with Sylvain Gugger and Scott Mueller\n\n\n\npython\n\nmle\n\n\n\nEvening study session for Scratch To Scale (S2S)\n\n\n\n\n\nSep 9, 2025\n\n1 min\n\n\n\n\n\n\nMarimo Workshop + Integrals refresher\n\n\n\npython\n\nmath\n\n\n\nA superb Marimo workshop with Vincent Warmerdam (S2S) and binge watching Essence Of Calculus (3b1b)\n\n\n\n\n\nSep 8, 2025\n\n1 min\n\n\n\n\n\n\nRay / Anyscale workshop with S2S\n\n\n\npython\n\nmle\n\n\n\nAn enlighting workshop to learn Ray with Robert Nishihara of Anyscale\n\n\n\n\n\nSep 5, 2025\n\n2 min\n\n\n\n\n\n\nGoing through DDP with S2S\n\n\n\npython\n\nmle\n\nmath\n\n\n\nFinishing the preliminaries for D2L and quick evening study session on DDP with torch.distributed primitives\n\n\n\n\n\nSep 4, 2025\n\n2 min\n\n\n\n\n\n\nFireside chat QA with Yuxiang Wei from Meta FAIR with S2S\n\n\n\nmle\n\npython\n\n\n\nQuestions surrounding distributed training at FAIR\n\n\n\n\n\nSep 3, 2025\n\n1 min\n\n\n\n\n\n\nStarting Scratch To Scale (S2S) and Dive into Deep Learning (D2L)\n\n\n\nmath\n\npython\n\nmle\n\n\n\nLearning about distributed training on GPUs (S2S) and preliminaries for D2L\n\n\n\n\n\nSep 2, 2025\n\n1 min\n\n\n\n\n\n\nFinished reading The Manga Guide to Linear Algebra\n\n\n\nmath\n\n\n\nLearning more about Linear Transformations, Eigen{vectors, values} and Diagonalization\n\n\n\n\n\nSep 1, 2025\n\n1 min\n\n\n\n\n\n\nStarted reading The Manga Guide to Linear Algebra\n\n\n\nmath\n\n\n\n(Re-)Developping intuition around Linear Algebra\n\n\n\n\n\nAug 29, 2025\n\n2 min\n\n\n\n\n\n\nSetting up this blog!\n\n\n\npython\n\n\n\nToday I decided I would convert my old Next.js blog to Notebooks + Quarto and document my learning process\n\n\n\n\n\nAug 28, 2025\n\n1 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "journal/2025-09-03.html",
    "href": "journal/2025-09-03.html",
    "title": "Fireside chat QA with Yuxiang Wei from Meta FAIR with S2S",
    "section": "",
    "text": "Well today I woke up at 5am to go on-site at my startup in Paris (I live 500km away, in Bordeaux) and got home at around 7pm, so not much free time to study, so I just read a bit in the train and ML related stuff was just the evening fireside chat.\n\nFireside chat with Yuxiang Wei from Meta FAIR (S2S)\nAs part of the course (conference) Scratch To Scale, we have a lot of very qualified engineers and researchers from top labs / companies dedicating time for guest lectures or QA sessions.\nToday we had the chance to listen to Yuxiang Wei — of Meta FAIR, of the SWE-RL paper — and ask him loads of questions. We discussed topics such as {pre, mid, post}-training, training infrastructure, the advent of RL and its impacts on said infrastructure and more."
  },
  {
    "objectID": "journal/2025-09-09.html",
    "href": "journal/2025-09-09.html",
    "title": "ZeRO / FSDP with Sylvain Gugger and Scott Mueller",
    "section": "",
    "text": "ZeRO / FSDP\nTonight we had a superb lesson — very dense — by Sylvain Gugger{_target=blank} on ZeRO, followed by a code dive-in with Scott. Overall takeaway (more detailed in my notes): * adam is stateful: has states, so ~4x model size in total to store - ZeRO: Zero Redudency Optimizer -&gt; sharding optimizer state: each gpu updates a subset of the models params then they share it all together all_gather - ZeRO2: Also sharding gradients - ZeRO3 == FSDP (PyTorch version): also sharding the model!\nZeRO is NOT A PARALLELISM strategy, it’s a modeling one. Think parallelism = more throughput, modeling strategy \\(\\approx\\) memory optimizations"
  },
  {
    "objectID": "journal/2025-09-11.html",
    "href": "journal/2025-09-11.html",
    "title": "Writing triton kernels with Daniel Han from Unsloth + linear algebra",
    "section": "",
    "text": "Linear algebra notes\nToday I wrote some more linear algebra notes, on linear dependence, rank, basis and dimension. In the process I also wrote about vector spaces. I really begin to feel the value of these notes, both as an external memory to refer to later, but also as a learning process allowing me to really understand to summarize and re-explain.\n\n\nDaniel Han\nAs part of S2S we had a guest lecture by Daniel Han from Unsloth on Triton and custom kernels. It was super interesting because it was working from first principles, and that’s the way I work. Instead of diving into triton’s DSL etc, he took a pen and a stack of paper and wrote neural network graphs of computations, deriving them by hand and explaining how custom training kernels were written as a way to speed-up backprop by using derivation tricks, using our knowledge of calculus, caching computations and results to achieve a better result than a torch.compile or autograd.\n\n\nLight mode\nI also took way too much time to get an OK-tier light mode for this website."
  },
  {
    "objectID": "journal/2025-08-29.html",
    "href": "journal/2025-08-29.html",
    "title": "Started reading The Manga Guide to Linear Algebra",
    "section": "",
    "text": "Yes, this might sound weird if you don’t know the concept, but hear me out. After seing it praised on Twitter/X by an anon, I got myself a copy of The Manga Guide to Linear Algebra.\nIt’s pretty good if you want to quickly get up to speed, develop intuition, but does not go too deep into formal proofs.\nYou might not know me, but before going to a Software Engineering School, I studied math for 2 years in Uni, and dropped out because it was too “theorical”, not concrete enough for me, I needed to grok the formulas, to visualize, feel the proofs naturally and see the applications. Sadly, partly due to how Uni works — partly due to playing League of Legends too much — I did not get the level of understanding required for my attention to stick.\nNow that I’ve become a SWE, I’ve seen the practical uses of such Math, and I now find myself needing more of it! So, step by step I’ll try to go from intuition and basics (this book) -&gt; implementing existing algorithms -&gt; proofs and more advanced formulas and theorem (some other textbook like Linear Algebra Done Right) -&gt; understanding ML architecture choices -&gt; being able to invent/discover new arch and techniques?\n\n\n\nBeing somewhat familiar with Linear Algebra and computer science, I already knew a few things, so I read pretty fast through the first bits.\nThe first thing I learned were the techniques for computing the determinant for matrices of dimensions 4 and above. There’s a technique based on the generalisation of the formula \\(\\det(\\mathbf{A}) = a_{11}a_{22} - a_{12}a_{21}\\).\nAlong with the interpretation/intuition for the determinant (consolidated by 3b1b).\nBut there’s also an algorithm (LU decomposition) used in computer science based on a set of rules for determinant and gaussian reduction."
  },
  {
    "objectID": "journal/2025-08-29.html#the-manga-guide-to-linear-algebra",
    "href": "journal/2025-08-29.html#the-manga-guide-to-linear-algebra",
    "title": "Started reading The Manga Guide to Linear Algebra",
    "section": "",
    "text": "Yes, this might sound weird if you don’t know the concept, but hear me out. After seing it praised on Twitter/X by an anon, I got myself a copy of The Manga Guide to Linear Algebra.\nIt’s pretty good if you want to quickly get up to speed, develop intuition, but does not go too deep into formal proofs.\nYou might not know me, but before going to a Software Engineering School, I studied math for 2 years in Uni, and dropped out because it was too “theorical”, not concrete enough for me, I needed to grok the formulas, to visualize, feel the proofs naturally and see the applications. Sadly, partly due to how Uni works — partly due to playing League of Legends too much — I did not get the level of understanding required for my attention to stick.\nNow that I’ve become a SWE, I’ve seen the practical uses of such Math, and I now find myself needing more of it! So, step by step I’ll try to go from intuition and basics (this book) -&gt; implementing existing algorithms -&gt; proofs and more advanced formulas and theorem (some other textbook like Linear Algebra Done Right) -&gt; understanding ML architecture choices -&gt; being able to invent/discover new arch and techniques?\n\n\n\nBeing somewhat familiar with Linear Algebra and computer science, I already knew a few things, so I read pretty fast through the first bits.\nThe first thing I learned were the techniques for computing the determinant for matrices of dimensions 4 and above. There’s a technique based on the generalisation of the formula \\(\\det(\\mathbf{A}) = a_{11}a_{22} - a_{12}a_{21}\\).\nAlong with the interpretation/intuition for the determinant (consolidated by 3b1b).\nBut there’s also an algorithm (LU decomposition) used in computer science based on a set of rules for determinant and gaussian reduction."
  },
  {
    "objectID": "journal/2025-09-17.html",
    "href": "journal/2025-09-17.html",
    "title": "Implementing ZeRO-1 + Lectures on DTensor/DeviceMesh and Parallel Processing",
    "section": "",
    "text": "Like I said yesterday, I had been a bit passive during the course (Scratch To Scale), not missing a class, going through the material, but not implementing it from scratch. This was for multiple reasons, including overload (doing a lot lately) but that’s no excuse. (Reading this tweet was a much needed kick in the butt)\n\nImplementing DP and ZeRO-1 from scratch\nSo I took the class replays, the UltraScale Playbook, the paper, blogs etc. and tried to really understand the precise interweaving of computations and communications. At some point it just clicked, like really, not the first level like “ok I get it”, but the last “ok, I get it now”. But then you write the PyTorch code, and the little details bite you. I had the general picture, now I had to do the grunt work. I made a DP wrapper for a model, handling gradient sync, then I wrote a ZeRO-1 Optimizer wrapper. I did what seemed obvious from what I understood: try to shard the optimizer states. For that I wanted to del useless_state_on_this_rank. Except states are lazy. So I just removed the optimizers pointers to some of the model params, as a result the optimizer simply doesn’t create states for them since it ignores their existence.\nI’ll link a notebook once I’ve done ZeRO-2 and 3\n\n\nDTensor and DeviceMesh\nMostly 🤯. Wanchao Liang from Thinking Machines, author of PyTorch’s DTensor and TorchTitan, gave us a lecture on DTensor. It was dense, mindblowing, and intense since I follow this cohort from France, so lessons are in the evening.\nBasically, if I had to explain what I understood, DTensors are a syntactic sugar over Distributed Tensors and paralllel operations on them. Using specs, we can explain how we want to distribute a tensor, sharding it, replicating it, or representing it as a partial-tensor, pending a reduction.\n\n\nParallel Processing on Modal\nFinally we had a great lecture/demo by Charles Frye on the history of computation and parallel programming, followed by demonstrations of how to run distributed/parallel programs on Modal.\nI’ve been using Modal to run my notebooks so far, so it’s great to see how we can run scripts/jobs on it too!"
  },
  {
    "objectID": "journal/2025-09-08.html",
    "href": "journal/2025-09-08.html",
    "title": "Marimo Workshop + Integrals refresher",
    "section": "",
    "text": "Binge watched Essence of Calculus\nAfter digging back into probabilities, I realized how important it was that I not only know integrals and formulas, but also that I completely grok and am able to derive these formulas from first principles. So I re-watched the entire Essence of Calculus series by the incredible 3b1b / Grant Sanderson.\nAnd grokking I did, between the videos and my old high-school and college lessons somewhere hidden deep inside my mind, I feel a renewed understanding of integration, so I wrote notes for the future me that will obviously forget again.\n\n\nSome more Linear Algebra\nSome research into the basics — first principles always wins — led me to learn about Elementary Matrices. I’d never given more thought to Gaussian Elimination that just using the row operations to achieve my goal. But thinking about elementary matrices and their properties, made things like LU decomposition click even more.\n\n\nMarimo Workshop\nTo finish off this awesome day of learning, we had a workshop with Vincent Warmerdam from the Marimo team. I love marimo and I loved learning more about it’s capabilities and ways to hack (with / at) it."
  },
  {
    "objectID": "journal/2025-09-10.html",
    "href": "journal/2025-09-10.html",
    "title": "FP8 Training with Phuc Nguyen (HF)",
    "section": "",
    "text": "Big day at work today, so I only got the time for the evening call and some time to re-read yesterday’s lecture (ZeRO/FSDP)\n\nFP8\nPhuc gave us a nice presentation on how and why to train FP8 precision models. Why do it? To speed up training. Issues: the model diverges suuuuuper fast under full FP8 regime, need to be careful. Solution: mixed precision training. We then saw a quick overview of how frontier labs do it (DeepSeek, Meta etc) and frameworks for it (torch/oa, torchtitan)"
  },
  {
    "objectID": "journal/2025-08-28.html",
    "href": "journal/2025-08-28.html",
    "title": "Setting up this blog!",
    "section": "",
    "text": "Today’s the day!\nI’ve decided to document my learning process to “transition” from SWE / using AI \\(\\longrightarrow\\) MLE/Applied Research/ building and training AI.\nAfter experimenting with marimo, loving it and struggling to make a blog out of marimo notebooks and experiments, I’ve stumbled upon this article by Jeremy Howard encouraging me to write my blog via notebooks + Quarto.\nIt also happens that I had just purchased the Scratch To Scale course/conference by the brilliant Zach Mueller and his blog was also using Quarto — which I would soon learn was no coincidence, Zach and Jeremy were closed and Zach contributed to Jeremy’s framework, fast.ai.\nSo there it is, I intend to document my learning process, render notebooks of experiments, store knowledge, formulas I’ve finally understood even if I’m the last person on earth to do so, bookmarks, reading notes etc."
  },
  {
    "objectID": "journal/2025-08-28.html#this-blog",
    "href": "journal/2025-08-28.html#this-blog",
    "title": "Setting up this blog!",
    "section": "",
    "text": "Today’s the day!\nI’ve decided to document my learning process to “transition” from SWE / using AI \\(\\longrightarrow\\) MLE/Applied Research/ building and training AI.\nAfter experimenting with marimo, loving it and struggling to make a blog out of marimo notebooks and experiments, I’ve stumbled upon this article by Jeremy Howard encouraging me to write my blog via notebooks + Quarto.\nIt also happens that I had just purchased the Scratch To Scale course/conference by the brilliant Zach Mueller and his blog was also using Quarto — which I would soon learn was no coincidence, Zach and Jeremy were closed and Zach contributed to Jeremy’s framework, fast.ai.\nSo there it is, I intend to document my learning process, render notebooks of experiments, store knowledge, formulas I’ve finally understood even if I’m the last person on earth to do so, bookmarks, reading notes etc."
  },
  {
    "objectID": "notebooks/test.html",
    "href": "notebooks/test.html",
    "title": "Theo Pomies",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\n\ndef is_leaf(module: nn.Module):\n    return len(list(module.modules())) == 1\n\n\nmodel = nn.Sequential(\n    nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10)), nn.Linear(10, 10)\n)\n\n\nfor module in model.modules():\n    print(module, is_leaf(module))\n\nSequential(\n  (0): Sequential(\n    (0): Linear(in_features=10, out_features=10, bias=True)\n    (1): Linear(in_features=10, out_features=10, bias=True)\n  )\n  (1): Linear(in_features=10, out_features=10, bias=True)\n) False\nSequential(\n  (0): Linear(in_features=10, out_features=10, bias=True)\n  (1): Linear(in_features=10, out_features=10, bias=True)\n) False\nLinear(in_features=10, out_features=10, bias=True) True\nLinear(in_features=10, out_features=10, bias=True) True\nLinear(in_features=10, out_features=10, bias=True) True\n\n\n\n[*[*nn.Linear(10, 10).modules()][0].modules()]\n\n[Linear(in_features=10, out_features=10, bias=True)]"
  },
  {
    "objectID": "notes/typescript.html",
    "href": "notes/typescript.html",
    "title": "On Typescript",
    "section": "",
    "text": "Let your types be flexible\nLet your constraints be rigid"
  },
  {
    "objectID": "notes/typescript.html#first-principles",
    "href": "notes/typescript.html#first-principles",
    "title": "On Typescript",
    "section": "",
    "text": "Let your types be flexible\nLet your constraints be rigid"
  },
  {
    "objectID": "notes/typescript.html#tldr-rules",
    "href": "notes/typescript.html#tldr-rules",
    "title": "On Typescript",
    "section": "TL;DR Rules",
    "text": "TL;DR Rules\n\nUse unknown instead of any, then use type narrowing to get the correct type.\nUse type over interface, unless you actually need to reach for an interface or need to express objects/class inheritance.\nAvoid using as to assert types, most of the time you actually want to narrow the type with checks (if/else).\nUse array.at(index) instead of array[index] unless array is a tuple (fixed size array).\nNEVER use TS specifics (enum, private in constructor, etc.)."
  },
  {
    "objectID": "notes/typescript.html#recommandations",
    "href": "notes/typescript.html#recommandations",
    "title": "On Typescript",
    "section": "Recommandations",
    "text": "Recommandations\n\nUse satisfies to check if an object fits a type but not erase the type.\nUse as const whenever possible. (Immutable data, enum-like objects, etc.)\nDefine (and export) types where they are consumed, and import them from other files if needed."
  },
  {
    "objectID": "notes/typescript.html#explanations",
    "href": "notes/typescript.html#explanations",
    "title": "On Typescript",
    "section": "Explanations",
    "text": "Explanations\n\nNarrowing over using as\nSuppose you have a function that takes a number\nfunction double(a: number) {\n    return a * 2;\n}\nAnd you have a variable that could be a number or a string\nfunction getNumberOrString(): number | string {\n    return Math.random() &gt; 0.5 ? 1 : \"1\";\n}\nconst a: number | string = getNumberOrString();\nTypescript will allow you to use as to assert the variable to a number (this is one of the ways that TypeScript is not sound)\nconst result: number = double(a as number);\nBut this not correct/sound at runtime!\nThe correct way to do this is to narrow the type with a check (if/else/early return).\nif (typeof a === \"number\") {\n    const result = double(a);\n}"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "rss\n\n\n\n\n\n\n\n\n\nProbability, Statistics and Information Theory\n\n\n\nmath\n\n\n\nSmall Probability, Statistics & Information Theory cheatsheet.\n\n\n\n\n\nSep 30, 2025\n\n\n\n\n\n\n\nDistributed Training\n\n\n\nmle\n\npython\n\n\n\nDistributed training study notes and algorithms.\n\n\n\n\n\nSep 17, 2025\n\n\n\n\n\n\n\nLinear Algebra\n\n\n\nmath\n\n\n\nAll things Linear Algebra, matrices, vectors etc.\n\n\n\n\n\nSep 12, 2025\n\n\n\n\n\n\n\nCalculus\n\n\n\nmath\n\n\n\nAll things calculus, differential or integral.\n\n\n\n\n\nSep 12, 2025\n\n\n\n\n\n\n\nPyTorch\n\n\n\npython\n\nmle\n\n\n\ntorch cheatsheet, common methods or tricks.\n\n\n\n\n\nSep 4, 2025\n\n\n\n\n\n\n\nOn Typescript\n\n\n\ntypescript\n\n\n\nA few notes on Typescript for my future self.\n\n\n\n\n\nFeb 15, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/probability.html",
    "href": "notes/probability.html",
    "title": "Probability, Statistics and Information Theory",
    "section": "",
    "text": "When studying probability, we are performing experiments, random trials or observations. The set of all possible outcomes of this experiment is \\(\\mathcal{\\Omega}\\) (or \\(\\mathcal{S}\\)). eg. When rolling a die, \\(\\mathcal{\\Omega} = \\{1,2,3,4,5,6\\}\\).\nWe can group these outcomes into events — \\(\\mathcal{E} \\subseteq \\mathcal{\\Omega}\\). eg. The event \\(\\mathcal{E} = \\{\\)die shows an even number\\(\\} = \\{2, 4, 6\\}\\). Whenever the outcome \\(z\\) of the random experiment satisfies \\(z \\in \\mathcal{E}\\), the event \\(\\mathcal{E}\\) has occurred. Multiple events can occur from the same outcome, say we have \\(\\mathcal{A} = \\{3, 6\\}\\) “the result is divisible by 3” and \\(\\mathcal{B} = \\{2, 4, 6\\}\\). \\(z = 6\\) satisfies both \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\).\n\n\n\nThe probability function maps events onto a real value \\(P\\colon \\mathcal{E} \\subseteq \\mathcal{\\Omega} \\to [0, 1]\\).\n\\(\\operatorname{P}(\\mathcal{E})\\) is the probability associated with event \\(\\mathcal{E}\\).\n\n\n\n\\(\\operatorname{P}(\\mathcal{E}) \\geq 0\\)\n\\(\\operatorname{P}(\\mathcal{\\Omega}) = 1, \\operatorname{P}(\\mathcal{\\emptyset}) = 0\\)\n\\(\\operatorname{P}(\\mathcal{A} \\cup \\mathcal{B}) = \\operatorname{P}(\\mathcal{A}) + \\operatorname{P}(\\mathcal{B}) - \\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B})\\)\n\\(\\operatorname{P}(\\bigcup_{i=1}^{\\infty} \\mathcal{A}_i) = \\sum_{i=1}^{\\infty} \\operatorname{P}(\\mathcal{A}_i), \\quad \\mathcal{A}_i \\cap \\mathcal{A}_j = \\emptyset\\: \\text{for all}\\: i \\neq j\\) (= if all events \\(\\mathcal{A}_i\\) are mutually exclusive)\n\\(\\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B}) = \\operatorname{P}(\\mathcal{A} \\mid \\mathcal{B})\\operatorname{P}(\\mathcal{B})\\)\n\\(\\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B}) = \\operatorname{P}(\\mathcal{A})\\operatorname{P}(\\mathcal{B}) \\iff \\mathcal{A} \\perp \\mathcal{B}\\) (eg. 2 fair dice rolls)\n\\(\\mathcal{A} \\perp \\mathcal{B} \\iff \\operatorname{P}(\\mathcal{A} \\mid \\mathcal{B}) = \\operatorname{P}(\\mathcal{A})\\)\n\n\n\n\n\nA random variable \\(X\\) is a measurable function (mapping) \\(X \\colon \\mathcal{\\Omega} \\to \\mathcal{E}\\) from a sample space \\(\\mathcal{\\Omega}\\) as a set of possible outcomes to a measurable space \\(\\mathcal{E}\\).\nThe probability that \\(X\\) takes on a value in a measurable set \\(\\mathcal{S} \\in \\mathcal{E}\\) is written as \\[\n\\operatorname{P}(X \\in \\mathcal{S}) = \\operatorname{P}(\\{\\omega \\in \\mathcal{\\Omega} \\mid X(\\omega) \\in \\mathcal{S}\\})\n\\]\nThe probability that \\(X\\) takes a discrete value \\(v\\), denoted \\(X = v\\), is \\(\\operatorname{P}(X=v)\\).\nExpressions like \\(X = v\\) or \\(X \\geq v\\) define events, i.e., subsets of \\(\\Omega\\) whose probability can be measured.\nRandom variables allow us to go from outcomes to values, like \\(X(\\omega) = \\omega\\), the random variable that associates to each die its value (identity function). This is also an example of a discrete random variable.\nWhen \\(X\\) is continuous, it doesn’t make sense to have events like \\(X = v\\) (and \\(\\operatorname{P}(X = v) = 0\\)); rather we use \\(v \\leq X \\leq w\\) and probability densities. An example would be the height of a population. Probabilities are described via a probability density function \\(p_X(x)\\), with \\[\n\\operatorname{P}(v \\le X \\le w) = \\int_v^w p_X(x)\\,dx\n\\]\nWe denote the probability distribution of \\(X\\) as \\(\\operatorname{P}(X)\\) (strictly speaking \\(P_X\\), but we often write \\(P(X)\\) for convenience).\n\n\n\n\n\n\nNote\n\n\n\nWhen the measurable space \\(\\mathcal{E}\\) is multi-dimensional, like \\(\\mathbb{R}^m\\), we call the random variable \\(\\mathbf{X} \\in \\mathbb{R}^m\\) a random vector.\n\n\n\n\n\\(\\operatorname{P}(A = a, B = b)\\) is the joint probability of \\(A = a\\) and \\(B = b\\) (it’s the intersection of the events \\(A = a\\) and \\(B = b\\)). Equivalently it’s \\(\\operatorname{P}(\\{A = a\\} \\cap \\{B = b\\})\\), with an overloaded notation, the joint probability distribution becomes \\(\\operatorname{P}(A, B)\\)\nObviously \\[ \\operatorname{P}(A = a, B = b) \\leq \\operatorname{P}(A=a) \\quad \\text{and} \\quad \\operatorname{P}(A = a, B = b) \\leq \\operatorname{P}(B=b) \\]\nAlso, we can marginalize \\[ \\operatorname{P}(A = a) = \\sum_v \\operatorname{P}(A = a, B = v) \\]\nBecause \\(A = a\\) and \\(B = b\\) are events, \\[\\begin{aligned}\n\\operatorname{P}(A = a, B = b) & = \\operatorname{P}(A = a \\mid B = b)\\operatorname{P}(B = b) \\\\\n\\iff \\operatorname{P}(A = a \\mid B = b) & = \\operatorname{P}(A = a, B = b)/\\operatorname{P}(B = b)\n\\end{aligned}\\]\n\n\n\n\nFrom the properties and definitions above, we can derive the following formula\n\\[ \\overbrace{\\operatorname{P}(A \\mid B)}^{\\text{posterior probability}} = \\dfrac{\\overbrace{\\operatorname{P}(B \\mid A)}^{\\text{likelihood}}\\overbrace{\\operatorname{P}(A)}^{\\text{prior}}}{\\underbrace{\\operatorname{P}(B)}_{\\text{observation}}} \\]\n\nprior/hypothesis: our estimate or current belief about the probability of \\(A\\)\nobservation/marginal likelihood/evidence: the evidence or observations we’ve made regarding \\(B\\)\nlikelihood: a measure of how compatible our hypothesis is with our observation\n\nA simplified version is \\(\\operatorname{P}(A \\mid B) \\propto \\operatorname{P}(B \\mid A)\\operatorname{P}(A)\\)\n\n\n\nThe expectation (or expected value) is the weighted average of the values of \\(X\\).\nDiscrete case:\n\\[ \\operatorname{E}[X] = \\operatorname{E}_{X \\sim P}[X] = \\sum_x x\\operatorname{P}(X=x) \\]\nContinuous case:\n\\[\\operatorname{E}[X] = \\int_{-\\infty}^{\\infty} x p(x) \\;dx \\]\nTo follow mathematical notation, sometimes we use \\(\\mu\\) to denote this average.\n\n\n\nLinearity: \\(\\operatorname{E}[\\alpha A + B] = \\alpha \\operatorname{E}[A] + \\operatorname{E}[B]\\)\nEquality: \\(X = Y \\; \\text{a.s.} \\implies \\operatorname{E}[X] = \\operatorname{E}[Y]\\)\nConstants: \\(X = c \\implies \\operatorname{E}[X] = c\\)\nTower property: \\(\\operatorname{E}[\\operatorname{E}[X]] = \\operatorname{E}[X]\\)\n\n\n\n\nFor a vector-valued random variable — ie. the random vector \\(\\mathbf{X} \\in \\mathbb{R}^n\\), we have \\(\\mathbf{\\mu} = \\operatorname{E}_{\\mathbf{X} \\sim P}[\\mathbf{X}]\\) with \\(\\mu_i = \\operatorname{E}_{\\mathbf{X} \\sim P}[x_i]\\) — the expectation of \\(\\mathbf{X}\\) is a vector of the expectations of each element \\(x_i\\) of \\(\\mathbf{X}\\).\n\n\n\n\nThe variance is a measure of dispersion, it quantifies how much values deviate from their expectation, on average. The variance is the expectation of the squared difference between the values and the expected value.\n\\[ \\operatorname{Var}(X) = \\operatorname{E}[(X - \\operatorname{E}[X])^2] = \\operatorname{E}[X^2] - (\\operatorname{E}[X])^2 \\]\nBecause\n\\[ \\operatorname{E}[X^2 - 2X\\operatorname{E}[X] + \\operatorname{E}[X]^2] = \\operatorname{E}[X^2] - 2(\\operatorname{E}[X])^2 + (\\operatorname{E}[X])^2 \\]\n\n\nFor a random vector \\(\\mathbf{X}\\), we store the pairwise variances of elements and covariances in a covariance matrix (aka. auto-covariance matrix or variance matrix) noted \\(\\mathbf{\\Sigma}\\) or \\(K_{\\mathbf{x}\\mathbf{x}}\\) or \\(\\operatorname{Cov}_{\\mathbf{x} \\sim P}\\), defined as\n\\[ \\mathbf{\\Sigma} = \\operatorname{E}_{\\mathbf{X} \\sim P}[(\\mathbf{X} - \\mathbf{\\mu})(\\mathbf{X} - \\mathbf{\\mu})^\\top] \\] \\[\\begin{aligned}\n\\mathbf{\\Sigma} = K_{\\mathbf{X}\\mathbf{Y}}\n& = \\operatorname{E}_{\\mathbf{X} \\sim P}[(\\mathbf{X} - \\mathbf{\\mu})(\\mathbf{X} - \\mathbf{\\mu})^\\top] \\\\\n& = \\operatorname{E}[\\mathbf{X}\\mathbf{X}^\\top] - \\operatorname{E}[\\mathbf{X}]\\operatorname{E}[\\mathbf{X}]^\\top\n\\end{aligned}\\]\n\n\n\n\n\n\nNote\n\n\n\nEach entry \\(\\Sigma_{i, j} = \\operatorname{Cov}(X_i, X_j)\\) — see covariance), and by definition, for diagonal entries \\(\\Sigma_{i, i} = \\operatorname{Cov}(X_i, X_i) = \\operatorname{Var}(X_i)\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe have the following property when applying a linear transformation represented by the appropriately dimensioned matrix \\(\\mathbf{A}\\)\n\\[\n\\begin{aligned}\n\\operatorname{Cov}(\\mathbf{AX}, \\mathbf{AX}) & = \\operatorname{E}[(\\mathbf{AX} - \\operatorname{E}[\\mathbf{AX}])(\\mathbf{AX} - \\operatorname{E}[\\mathbf{AX}])^\\top] \\\\\n& = \\operatorname{E}[\\mathbf{AX}(\\mathbf{AX})^\\top] - \\operatorname{E}[\\mathbf{AX}]\\operatorname{E}[(\\mathbf{AX})^\\top] \\\\\n& = \\operatorname{E}[\\mathbf{AX}\\mathbf{X}^\\top\\mathbf{A}^\\top] - \\operatorname{E}[\\mathbf{AX}]\\operatorname{E}[\\mathbf{X}^\\top\\mathbf{A}^\\top] \\\\\n& = \\mathbf{A}\\operatorname{E}[\\mathbf{X}\\mathbf{X}^\\top]\\mathbf{A}^\\top - \\mathbf{A}\\operatorname{E}[\\mathbf{X}]\\operatorname{E}[\\mathbf{X}^\\top]\\mathbf{A}^\\top \\\\\n& = \\mathbf{A}(\\operatorname{E}[\\mathbf{X}\\mathbf{X}^\\top] - \\operatorname{E}[\\mathbf{X}]\\operatorname{E}[\\mathbf{X}^\\top])\\mathbf{A}^\\top \\\\\n& = \\mathbf{A}\\mathbf{\\Sigma}\\mathbf{A}^\\top\n\\end{aligned}\n\\]\nAs a result of the linearity of expectation\n\n\n\n\n\n\nBecause the variance is a squared difference, we can take its square root to get the standard deviation which has the benefit of being in the same unit as our random variable.\n\\[ \\operatorname{Var}(X) = \\sigma^2_X \\iff \\sigma_X = \\sqrt{\\operatorname{Var}(X)} \\]\n\n\n\nCovariance is a measure of the joint variability of two random variables.\n\\[ \\operatorname{Cov}(X, Y) = \\operatorname{E}[(X - \\operatorname{E}[X])(Y - \\operatorname{E}[Y])] \\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\operatorname{Cov}(X, X) = \\operatorname{E}[(X - \\operatorname{E}[X])^2] = \\operatorname{Var}(X)\\)\n\n\n\n\nFor random vectors \\(\\mathbf{X} \\in \\mathbb{R}^m\\), \\(\\mathbf{Y} \\in \\mathbb{R}^n\\), the covariance matrix is a matrix \\(K_{\\mathbf{X}\\mathbf{Y}}\\) defined as\n\\[\\begin{aligned}\nK_{\\mathbf{X}\\mathbf{Y}} & = \\operatorname{E}[(\\mathbf{X} - \\operatorname{E}[\\mathbf{X}])(\\mathbf{Y} - \\operatorname{E}[\\mathbf{Y}])^\\top] \\\\\n& = \\operatorname{E}[\\mathbf{X}\\mathbf{Y}^\\top] - \\operatorname{E}[\\mathbf{X}]\\operatorname{E}[\\mathbf{Y}]^\\top\n\\end{aligned}\\]\nWe have \\(\\operatorname{Cov}(X_i, Y_j) = K_{X_iY_j} = \\operatorname{E}[(X_i - \\operatorname{E}[X_i])(Y_j - \\operatorname{E}[Y_j])]\\) found at index \\((i, j)\\) in \\(K_{\\mathbf{X}\\mathbf{Y}}\\)\nIf \\(\\mathbf{X} = \\mathbf{Y}\\) this is the auto-covariance matrix or variance matrix of this random vector \\(\\mathbf{X}\\)\nIf \\(\\mathbf{X} \\neq \\mathbf{Y}\\) this is the cross-covariance matrix of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\)\n\n\n\n\n\nConsidering model parameters \\(\\boldsymbol{\\theta}\\) and data examples \\(X\\), the goal of Machine Learning is to find \\(\\boldsymbol{\\theta}\\) such that \\[\\mathop{\\mathrm{argmax}} P(\\boldsymbol{\\theta}\\mid X)\\]\nBy Bayes’ theorem\n\\[ \\mathop{\\mathrm{argmax}} P(\\boldsymbol{\\theta}\\mid X) = \\mathop{\\mathrm{argmax}} \\dfrac{P(X \\mid \\boldsymbol{\\theta})P(\\boldsymbol{\\theta})}{P(X)} \\]\nHowever \\(P(X)\\) and \\(P(\\boldsymbol{\\theta})\\) can be dropped because \\(P(X)\\) is independent on \\(\\boldsymbol{\\theta}\\), and we have no prior information or “belief” on the best parameters \\(\\boldsymbol{\\theta}\\), so \\(P(\\boldsymbol{\\theta})\\) uninformative.\nHence, our best parameter estimation is the argument \\(\\boldsymbol{\\theta}\\) maximizing the likelihood (probability of seeing data \\(X\\) knowing parameters \\(\\boldsymbol{\\theta}\\)):\n\\[ \\hat{\\boldsymbol{\\theta}} = \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}}\\, P(\\boldsymbol{\\theta}\\mid X) = \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}}\\, P(X \\mid \\boldsymbol{\\theta}) \\]\n\n\n\n\n\n\nNoteNegative Log-Likelihood\n\n\n\nIn practice, we often use Negative Log-Likelihood in ML. The Log part comes from numerical stability and transforming products into sums. Imagine computing the likelihood for billions of data points, it would require too much precision for fp32. But the log-likelihood fits easily and precisely.\nThe negative part comes from loss/cost functions, which we want to minimize, not maximize. Because log is continuous and increasing we can transform the maximization problem into a minimization one by taking \\(-P(X \\mid \\boldsymbol{\\theta})\\)."
  },
  {
    "objectID": "notes/probability.html#definitions-formulas",
    "href": "notes/probability.html#definitions-formulas",
    "title": "Probability, Statistics and Information Theory",
    "section": "",
    "text": "When studying probability, we are performing experiments, random trials or observations. The set of all possible outcomes of this experiment is \\(\\mathcal{\\Omega}\\) (or \\(\\mathcal{S}\\)). eg. When rolling a die, \\(\\mathcal{\\Omega} = \\{1,2,3,4,5,6\\}\\).\nWe can group these outcomes into events — \\(\\mathcal{E} \\subseteq \\mathcal{\\Omega}\\). eg. The event \\(\\mathcal{E} = \\{\\)die shows an even number\\(\\} = \\{2, 4, 6\\}\\). Whenever the outcome \\(z\\) of the random experiment satisfies \\(z \\in \\mathcal{E}\\), the event \\(\\mathcal{E}\\) has occurred. Multiple events can occur from the same outcome, say we have \\(\\mathcal{A} = \\{3, 6\\}\\) “the result is divisible by 3” and \\(\\mathcal{B} = \\{2, 4, 6\\}\\). \\(z = 6\\) satisfies both \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\).\n\n\n\nThe probability function maps events onto a real value \\(P\\colon \\mathcal{E} \\subseteq \\mathcal{\\Omega} \\to [0, 1]\\).\n\\(\\operatorname{P}(\\mathcal{E})\\) is the probability associated with event \\(\\mathcal{E}\\).\n\n\n\n\\(\\operatorname{P}(\\mathcal{E}) \\geq 0\\)\n\\(\\operatorname{P}(\\mathcal{\\Omega}) = 1, \\operatorname{P}(\\mathcal{\\emptyset}) = 0\\)\n\\(\\operatorname{P}(\\mathcal{A} \\cup \\mathcal{B}) = \\operatorname{P}(\\mathcal{A}) + \\operatorname{P}(\\mathcal{B}) - \\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B})\\)\n\\(\\operatorname{P}(\\bigcup_{i=1}^{\\infty} \\mathcal{A}_i) = \\sum_{i=1}^{\\infty} \\operatorname{P}(\\mathcal{A}_i), \\quad \\mathcal{A}_i \\cap \\mathcal{A}_j = \\emptyset\\: \\text{for all}\\: i \\neq j\\) (= if all events \\(\\mathcal{A}_i\\) are mutually exclusive)\n\\(\\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B}) = \\operatorname{P}(\\mathcal{A} \\mid \\mathcal{B})\\operatorname{P}(\\mathcal{B})\\)\n\\(\\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B}) = \\operatorname{P}(\\mathcal{A})\\operatorname{P}(\\mathcal{B}) \\iff \\mathcal{A} \\perp \\mathcal{B}\\) (eg. 2 fair dice rolls)\n\\(\\mathcal{A} \\perp \\mathcal{B} \\iff \\operatorname{P}(\\mathcal{A} \\mid \\mathcal{B}) = \\operatorname{P}(\\mathcal{A})\\)\n\n\n\n\n\nA random variable \\(X\\) is a measurable function (mapping) \\(X \\colon \\mathcal{\\Omega} \\to \\mathcal{E}\\) from a sample space \\(\\mathcal{\\Omega}\\) as a set of possible outcomes to a measurable space \\(\\mathcal{E}\\).\nThe probability that \\(X\\) takes on a value in a measurable set \\(\\mathcal{S} \\in \\mathcal{E}\\) is written as \\[\n\\operatorname{P}(X \\in \\mathcal{S}) = \\operatorname{P}(\\{\\omega \\in \\mathcal{\\Omega} \\mid X(\\omega) \\in \\mathcal{S}\\})\n\\]\nThe probability that \\(X\\) takes a discrete value \\(v\\), denoted \\(X = v\\), is \\(\\operatorname{P}(X=v)\\).\nExpressions like \\(X = v\\) or \\(X \\geq v\\) define events, i.e., subsets of \\(\\Omega\\) whose probability can be measured.\nRandom variables allow us to go from outcomes to values, like \\(X(\\omega) = \\omega\\), the random variable that associates to each die its value (identity function). This is also an example of a discrete random variable.\nWhen \\(X\\) is continuous, it doesn’t make sense to have events like \\(X = v\\) (and \\(\\operatorname{P}(X = v) = 0\\)); rather we use \\(v \\leq X \\leq w\\) and probability densities. An example would be the height of a population. Probabilities are described via a probability density function \\(p_X(x)\\), with \\[\n\\operatorname{P}(v \\le X \\le w) = \\int_v^w p_X(x)\\,dx\n\\]\nWe denote the probability distribution of \\(X\\) as \\(\\operatorname{P}(X)\\) (strictly speaking \\(P_X\\), but we often write \\(P(X)\\) for convenience).\n\n\n\n\n\n\nNote\n\n\n\nWhen the measurable space \\(\\mathcal{E}\\) is multi-dimensional, like \\(\\mathbb{R}^m\\), we call the random variable \\(\\mathbf{X} \\in \\mathbb{R}^m\\) a random vector.\n\n\n\n\n\\(\\operatorname{P}(A = a, B = b)\\) is the joint probability of \\(A = a\\) and \\(B = b\\) (it’s the intersection of the events \\(A = a\\) and \\(B = b\\)). Equivalently it’s \\(\\operatorname{P}(\\{A = a\\} \\cap \\{B = b\\})\\), with an overloaded notation, the joint probability distribution becomes \\(\\operatorname{P}(A, B)\\)\nObviously \\[ \\operatorname{P}(A = a, B = b) \\leq \\operatorname{P}(A=a) \\quad \\text{and} \\quad \\operatorname{P}(A = a, B = b) \\leq \\operatorname{P}(B=b) \\]\nAlso, we can marginalize \\[ \\operatorname{P}(A = a) = \\sum_v \\operatorname{P}(A = a, B = v) \\]\nBecause \\(A = a\\) and \\(B = b\\) are events, \\[\\begin{aligned}\n\\operatorname{P}(A = a, B = b) & = \\operatorname{P}(A = a \\mid B = b)\\operatorname{P}(B = b) \\\\\n\\iff \\operatorname{P}(A = a \\mid B = b) & = \\operatorname{P}(A = a, B = b)/\\operatorname{P}(B = b)\n\\end{aligned}\\]\n\n\n\n\nFrom the properties and definitions above, we can derive the following formula\n\\[ \\overbrace{\\operatorname{P}(A \\mid B)}^{\\text{posterior probability}} = \\dfrac{\\overbrace{\\operatorname{P}(B \\mid A)}^{\\text{likelihood}}\\overbrace{\\operatorname{P}(A)}^{\\text{prior}}}{\\underbrace{\\operatorname{P}(B)}_{\\text{observation}}} \\]\n\nprior/hypothesis: our estimate or current belief about the probability of \\(A\\)\nobservation/marginal likelihood/evidence: the evidence or observations we’ve made regarding \\(B\\)\nlikelihood: a measure of how compatible our hypothesis is with our observation\n\nA simplified version is \\(\\operatorname{P}(A \\mid B) \\propto \\operatorname{P}(B \\mid A)\\operatorname{P}(A)\\)\n\n\n\nThe expectation (or expected value) is the weighted average of the values of \\(X\\).\nDiscrete case:\n\\[ \\operatorname{E}[X] = \\operatorname{E}_{X \\sim P}[X] = \\sum_x x\\operatorname{P}(X=x) \\]\nContinuous case:\n\\[\\operatorname{E}[X] = \\int_{-\\infty}^{\\infty} x p(x) \\;dx \\]\nTo follow mathematical notation, sometimes we use \\(\\mu\\) to denote this average.\n\n\n\nLinearity: \\(\\operatorname{E}[\\alpha A + B] = \\alpha \\operatorname{E}[A] + \\operatorname{E}[B]\\)\nEquality: \\(X = Y \\; \\text{a.s.} \\implies \\operatorname{E}[X] = \\operatorname{E}[Y]\\)\nConstants: \\(X = c \\implies \\operatorname{E}[X] = c\\)\nTower property: \\(\\operatorname{E}[\\operatorname{E}[X]] = \\operatorname{E}[X]\\)\n\n\n\n\nFor a vector-valued random variable — ie. the random vector \\(\\mathbf{X} \\in \\mathbb{R}^n\\), we have \\(\\mathbf{\\mu} = \\operatorname{E}_{\\mathbf{X} \\sim P}[\\mathbf{X}]\\) with \\(\\mu_i = \\operatorname{E}_{\\mathbf{X} \\sim P}[x_i]\\) — the expectation of \\(\\mathbf{X}\\) is a vector of the expectations of each element \\(x_i\\) of \\(\\mathbf{X}\\).\n\n\n\n\nThe variance is a measure of dispersion, it quantifies how much values deviate from their expectation, on average. The variance is the expectation of the squared difference between the values and the expected value.\n\\[ \\operatorname{Var}(X) = \\operatorname{E}[(X - \\operatorname{E}[X])^2] = \\operatorname{E}[X^2] - (\\operatorname{E}[X])^2 \\]\nBecause\n\\[ \\operatorname{E}[X^2 - 2X\\operatorname{E}[X] + \\operatorname{E}[X]^2] = \\operatorname{E}[X^2] - 2(\\operatorname{E}[X])^2 + (\\operatorname{E}[X])^2 \\]\n\n\nFor a random vector \\(\\mathbf{X}\\), we store the pairwise variances of elements and covariances in a covariance matrix (aka. auto-covariance matrix or variance matrix) noted \\(\\mathbf{\\Sigma}\\) or \\(K_{\\mathbf{x}\\mathbf{x}}\\) or \\(\\operatorname{Cov}_{\\mathbf{x} \\sim P}\\), defined as\n\\[ \\mathbf{\\Sigma} = \\operatorname{E}_{\\mathbf{X} \\sim P}[(\\mathbf{X} - \\mathbf{\\mu})(\\mathbf{X} - \\mathbf{\\mu})^\\top] \\] \\[\\begin{aligned}\n\\mathbf{\\Sigma} = K_{\\mathbf{X}\\mathbf{Y}}\n& = \\operatorname{E}_{\\mathbf{X} \\sim P}[(\\mathbf{X} - \\mathbf{\\mu})(\\mathbf{X} - \\mathbf{\\mu})^\\top] \\\\\n& = \\operatorname{E}[\\mathbf{X}\\mathbf{X}^\\top] - \\operatorname{E}[\\mathbf{X}]\\operatorname{E}[\\mathbf{X}]^\\top\n\\end{aligned}\\]\n\n\n\n\n\n\nNote\n\n\n\nEach entry \\(\\Sigma_{i, j} = \\operatorname{Cov}(X_i, X_j)\\) — see covariance), and by definition, for diagonal entries \\(\\Sigma_{i, i} = \\operatorname{Cov}(X_i, X_i) = \\operatorname{Var}(X_i)\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe have the following property when applying a linear transformation represented by the appropriately dimensioned matrix \\(\\mathbf{A}\\)\n\\[\n\\begin{aligned}\n\\operatorname{Cov}(\\mathbf{AX}, \\mathbf{AX}) & = \\operatorname{E}[(\\mathbf{AX} - \\operatorname{E}[\\mathbf{AX}])(\\mathbf{AX} - \\operatorname{E}[\\mathbf{AX}])^\\top] \\\\\n& = \\operatorname{E}[\\mathbf{AX}(\\mathbf{AX})^\\top] - \\operatorname{E}[\\mathbf{AX}]\\operatorname{E}[(\\mathbf{AX})^\\top] \\\\\n& = \\operatorname{E}[\\mathbf{AX}\\mathbf{X}^\\top\\mathbf{A}^\\top] - \\operatorname{E}[\\mathbf{AX}]\\operatorname{E}[\\mathbf{X}^\\top\\mathbf{A}^\\top] \\\\\n& = \\mathbf{A}\\operatorname{E}[\\mathbf{X}\\mathbf{X}^\\top]\\mathbf{A}^\\top - \\mathbf{A}\\operatorname{E}[\\mathbf{X}]\\operatorname{E}[\\mathbf{X}^\\top]\\mathbf{A}^\\top \\\\\n& = \\mathbf{A}(\\operatorname{E}[\\mathbf{X}\\mathbf{X}^\\top] - \\operatorname{E}[\\mathbf{X}]\\operatorname{E}[\\mathbf{X}^\\top])\\mathbf{A}^\\top \\\\\n& = \\mathbf{A}\\mathbf{\\Sigma}\\mathbf{A}^\\top\n\\end{aligned}\n\\]\nAs a result of the linearity of expectation\n\n\n\n\n\n\nBecause the variance is a squared difference, we can take its square root to get the standard deviation which has the benefit of being in the same unit as our random variable.\n\\[ \\operatorname{Var}(X) = \\sigma^2_X \\iff \\sigma_X = \\sqrt{\\operatorname{Var}(X)} \\]\n\n\n\nCovariance is a measure of the joint variability of two random variables.\n\\[ \\operatorname{Cov}(X, Y) = \\operatorname{E}[(X - \\operatorname{E}[X])(Y - \\operatorname{E}[Y])] \\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\operatorname{Cov}(X, X) = \\operatorname{E}[(X - \\operatorname{E}[X])^2] = \\operatorname{Var}(X)\\)\n\n\n\n\nFor random vectors \\(\\mathbf{X} \\in \\mathbb{R}^m\\), \\(\\mathbf{Y} \\in \\mathbb{R}^n\\), the covariance matrix is a matrix \\(K_{\\mathbf{X}\\mathbf{Y}}\\) defined as\n\\[\\begin{aligned}\nK_{\\mathbf{X}\\mathbf{Y}} & = \\operatorname{E}[(\\mathbf{X} - \\operatorname{E}[\\mathbf{X}])(\\mathbf{Y} - \\operatorname{E}[\\mathbf{Y}])^\\top] \\\\\n& = \\operatorname{E}[\\mathbf{X}\\mathbf{Y}^\\top] - \\operatorname{E}[\\mathbf{X}]\\operatorname{E}[\\mathbf{Y}]^\\top\n\\end{aligned}\\]\nWe have \\(\\operatorname{Cov}(X_i, Y_j) = K_{X_iY_j} = \\operatorname{E}[(X_i - \\operatorname{E}[X_i])(Y_j - \\operatorname{E}[Y_j])]\\) found at index \\((i, j)\\) in \\(K_{\\mathbf{X}\\mathbf{Y}}\\)\nIf \\(\\mathbf{X} = \\mathbf{Y}\\) this is the auto-covariance matrix or variance matrix of this random vector \\(\\mathbf{X}\\)\nIf \\(\\mathbf{X} \\neq \\mathbf{Y}\\) this is the cross-covariance matrix of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\)\n\n\n\n\n\nConsidering model parameters \\(\\boldsymbol{\\theta}\\) and data examples \\(X\\), the goal of Machine Learning is to find \\(\\boldsymbol{\\theta}\\) such that \\[\\mathop{\\mathrm{argmax}} P(\\boldsymbol{\\theta}\\mid X)\\]\nBy Bayes’ theorem\n\\[ \\mathop{\\mathrm{argmax}} P(\\boldsymbol{\\theta}\\mid X) = \\mathop{\\mathrm{argmax}} \\dfrac{P(X \\mid \\boldsymbol{\\theta})P(\\boldsymbol{\\theta})}{P(X)} \\]\nHowever \\(P(X)\\) and \\(P(\\boldsymbol{\\theta})\\) can be dropped because \\(P(X)\\) is independent on \\(\\boldsymbol{\\theta}\\), and we have no prior information or “belief” on the best parameters \\(\\boldsymbol{\\theta}\\), so \\(P(\\boldsymbol{\\theta})\\) uninformative.\nHence, our best parameter estimation is the argument \\(\\boldsymbol{\\theta}\\) maximizing the likelihood (probability of seeing data \\(X\\) knowing parameters \\(\\boldsymbol{\\theta}\\)):\n\\[ \\hat{\\boldsymbol{\\theta}} = \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}}\\, P(\\boldsymbol{\\theta}\\mid X) = \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}}\\, P(X \\mid \\boldsymbol{\\theta}) \\]\n\n\n\n\n\n\nNoteNegative Log-Likelihood\n\n\n\nIn practice, we often use Negative Log-Likelihood in ML. The Log part comes from numerical stability and transforming products into sums. Imagine computing the likelihood for billions of data points, it would require too much precision for fp32. But the log-likelihood fits easily and precisely.\nThe negative part comes from loss/cost functions, which we want to minimize, not maximize. Because log is continuous and increasing we can transform the maximization problem into a minimization one by taking \\(-P(X \\mid \\boldsymbol{\\theta})\\)."
  },
  {
    "objectID": "notes/probability.html#proofs",
    "href": "notes/probability.html#proofs",
    "title": "Probability, Statistics and Information Theory",
    "section": "Proofs",
    "text": "Proofs\nLater!"
  },
  {
    "objectID": "notes/probability.html#notation",
    "href": "notes/probability.html#notation",
    "title": "Probability, Statistics and Information Theory",
    "section": "Notation",
    "text": "Notation\n\n\\(\\mathcal{X}\\): a set\n\\(\\{a, b, c\\}\\): a set, with its elements\n\\(\\emptyset\\): the empty set\n\\(\\mathcal{A} \\subset \\mathcal{B}\\), \\(\\mathcal{A} \\subsetneq \\mathcal{B}\\): \\(\\mathcal{A}\\) is a proper/strict subset of \\(\\mathcal{B}\\)\n\\(\\mathcal{A} \\subseteq \\mathcal{B}\\): \\(\\mathcal{A}\\) is a subest of \\(\\mathcal{B}\\)\n\\(\\mathcal{A} \\cap \\mathcal{B}\\): the intersection of sets \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) — “\\(\\mathcal{A}\\) and \\(\\mathcal{B}\\)”\n\\(\\mathcal{A} \\cup \\mathcal{B}\\): the union of sets \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) — “\\(\\mathcal{A}\\) or \\(\\mathcal{B}\\)”\n\\(\\mathcal{A} \\setminus \\mathcal{B}\\): set subtraction of \\(\\mathcal{B}\\) from \\(\\mathcal{A}\\), elements from \\(\\mathcal{A}\\) but not in \\(\\mathcal{B}\\)\n\\(\\mathcal{S}\\), \\(\\mathcal{\\Omega}\\): the sample space / universe (the set of all possible outcomes)\n\\(|\\mathcal{X}|\\): the cardinality of set \\(\\mathcal{X}\\) (its number of events)\n\\(X\\): a random variable\n\\(\\mathbf{X}\\): a random vector\n\\(P\\): a probability distribution\n\\(X \\sim P\\): the random variable \\(X\\) follows the probability distribution \\(P\\)\n\\(a \\propto b\\): \\(a\\) is proportional to \\(b\\), eg. \\(a = kb\\)\n\\(\\operatorname{P}(\\cdot)\\): the probability function, maps events to their probability and random variables to their probability distributions\n\\(\\operatorname{P}(X)\\): depending on the context, a probability distribution or the probability of any \\(X=x\\), meaning the formula is true for any value\n\\(\\operatorname{P}(X=x)\\): the probability assigned to the event where random variable \\(X\\) takes value \\(x\\)\n\\(\\operatorname{P}(X \\mid Y)\\): the conditional probability distribution of \\(X\\) given \\(Y\\)\n\\(\\operatorname{p}(\\cdot)\\): a probability density function (PDF) associated with distribution \\(P\\)\n\\(\\operatorname{E}[X]\\): expectation of a random variable \\(X\\)\n\\(X \\perp Y\\): random variables \\(X\\) and \\(Y\\) are independent\n\\(X \\perp Y \\mid Z\\): random variables \\(X\\) and \\(Y\\) are conditionally independent given \\(Z\\)\n\\(\\sigma_X\\): standard deviation of random variable \\(X\\)\n\\(\\operatorname{Var}(X)\\): variance of random variable \\(X\\), equal to \\(\\sigma^2_X\\)\n\\(\\operatorname{Cov}(X, Y)\\): covariance of random variables \\(X\\) and \\(Y\\)\n\\(\\operatorname{\\rho}(X, Y)\\): the Pearson correlation coefficient between \\(X\\) and \\(Y\\), equals \\(\\frac{\\operatorname{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\)\n\\(\\operatorname{H}(X)\\): entropy of random variable \\(X\\)\n\\(D_{\\operatorname{KL}}(P\\|Q)\\): the KL-divergence (or relative entropy) from distribution \\(Q\\) to distribution \\(P\\)"
  }
]