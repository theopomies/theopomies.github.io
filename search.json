[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Theo POMIES",
    "section": "",
    "text": "I’m Théo, a graduate of EPITECH and currently CTO of a tech startup, Kivala. My background is in AI, large language models, backend development, and data engineering. Over the past years, I’ve worked on building robust backends with Python and FastAPI, designing automation pipelines, and deploying AI systems ranging from custom LLMs to retrieval-augmented generation (RAG) architectures.\nMy technical work usually revolves around:"
  },
  {
    "objectID": "index.html#transition-to-mle-applied-research",
    "href": "index.html#transition-to-mle-applied-research",
    "title": "Theo POMIES",
    "section": "Transition to MLE & Applied Research",
    "text": "Transition to MLE & Applied Research\nI’m now in the process of transitioning towards machine learning engineering and applied research. This blog is where I’ll share my journey — notes from my learning process, experiments, and articles about the concepts and techniques I find valuable along the way.\nIt’s part notebook, part knowledge base, and part exploration. My goal is to document both the practice of building systems and the research side of AI that makes them possible.\n\nprint(\"Hello, World!\")  # Stdout = cyan\n\"Hello, World!\"  # Output display = primary\n\nHello, World!\n\n\n'Hello, World!'"
  },
  {
    "objectID": "notes/typescript.html",
    "href": "notes/typescript.html",
    "title": "On Typescript",
    "section": "",
    "text": "Let your types be flexible\nLet your constraints be rigid"
  },
  {
    "objectID": "notes/typescript.html#first-principles",
    "href": "notes/typescript.html#first-principles",
    "title": "On Typescript",
    "section": "",
    "text": "Let your types be flexible\nLet your constraints be rigid"
  },
  {
    "objectID": "notes/typescript.html#tldr-rules",
    "href": "notes/typescript.html#tldr-rules",
    "title": "On Typescript",
    "section": "TL;DR Rules",
    "text": "TL;DR Rules\n\nUse unknown instead of any, then use type narrowing to get the correct type.\nUse type over interface, unless you actually need to reach for an interface or need to express objects/class inheritance.\nAvoid using as to assert types, most of the time you actually want to narrow the type with checks (if/else).\nUse array.at(index) instead of array[index] unless array is a tuple (fixed size array).\nNEVER use TS specifics (enum, private in constructor, etc.)."
  },
  {
    "objectID": "notes/typescript.html#recommandations",
    "href": "notes/typescript.html#recommandations",
    "title": "On Typescript",
    "section": "Recommandations",
    "text": "Recommandations\n\nUse satisfies to check if an object fits a type but not erase the type.\nUse as const whenever possible. (Immutable data, enum-like objects, etc.)\nDefine (and export) types where they are consumed, and import them from other files if needed."
  },
  {
    "objectID": "notes/typescript.html#explanations",
    "href": "notes/typescript.html#explanations",
    "title": "On Typescript",
    "section": "Explanations",
    "text": "Explanations\n\nNarrowing over using as\nSuppose you have a function that takes a number\nfunction double(a: number) {\n    return a * 2;\n}\nAnd you have a variable that could be a number or a string\nfunction getNumberOrString(): number | string {\n    return Math.random() &gt; 0.5 ? 1 : \"1\";\n}\nconst a: number | string = getNumberOrString();\nTypescript will allow you to use as to assert the variable to a number (this is one of the ways that TypeScript is not sound)\nconst result: number = double(a as number);\nBut this not correct/sound at runtime!\nThe correct way to do this is to narrow the type with a check (if/else/early return).\nif (typeof a === \"number\") {\n    const result = double(a);\n}"
  },
  {
    "objectID": "journal/index.html",
    "href": "journal/index.html",
    "title": "Journal",
    "section": "",
    "text": "rss\n\n\n\n\n\n\n\n\n\nStarted reading The Manga Guide to Linear Algebra\n\n\n\npython\n\nmath\n\n\n\n(Re-)Developping intuition around Linear Algebra\n\n\n\n\n\nAug 29, 2025\n\n\n\n\n\n\n\nSetting up this blog!\n\n\n\npython\n\n\n\nToday I decided I would convert my old Next.js blog to Notebooks + Quarto and document my learning process\n\n\n\n\n\nAug 28, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "journal/2025-08-28.html",
    "href": "journal/2025-08-28.html",
    "title": "Setting up this blog!",
    "section": "",
    "text": "Today’s the day!\nI’ve decided to document my learning process to “transition” from SWE / using AI \\(\\longrightarrow\\) MLE/Applied Research/ building and training AI.\nAfter experimenting with marimo, loving it and struggling to make a blog out of marimo notebooks and experiments, I’ve stumbled upon this article by Jeremy Howard encouraging me to write my blog via notebooks + Quarto.\nIt also happens that I had just purchased the Scratch To Scale course/conference by the brilliant Zach Mueller and his blog was also using Quarto — which I would soon learn was no coincidence, Zach and Jeremy were closed and Zach contributed to Jeremy’s framework, fast.ai.\nSo there it is, I intend to document my learning process, render notebooks of experiments, store knowledge, formulas I’ve finally understood even if I’m the last person on earth to do so, bookmarks, reading notes etc."
  },
  {
    "objectID": "journal/2025-08-28.html#this-blog",
    "href": "journal/2025-08-28.html#this-blog",
    "title": "Setting up this blog!",
    "section": "",
    "text": "Today’s the day!\nI’ve decided to document my learning process to “transition” from SWE / using AI \\(\\longrightarrow\\) MLE/Applied Research/ building and training AI.\nAfter experimenting with marimo, loving it and struggling to make a blog out of marimo notebooks and experiments, I’ve stumbled upon this article by Jeremy Howard encouraging me to write my blog via notebooks + Quarto.\nIt also happens that I had just purchased the Scratch To Scale course/conference by the brilliant Zach Mueller and his blog was also using Quarto — which I would soon learn was no coincidence, Zach and Jeremy were closed and Zach contributed to Jeremy’s framework, fast.ai.\nSo there it is, I intend to document my learning process, render notebooks of experiments, store knowledge, formulas I’ve finally understood even if I’m the last person on earth to do so, bookmarks, reading notes etc."
  },
  {
    "objectID": "journal/2025-08-29.html",
    "href": "journal/2025-08-29.html",
    "title": "Started reading The Manga Guide to Linear Algebra",
    "section": "",
    "text": "Yes, this might sound weird if you don’t know the concept, but hear me out. After seing it praised on Twitter/X by an anon, I got myself a copy of The Manga Guide to Linear Algebra.\nIt’s pretty good if you want to quickly get up to speed, develop intuition, but does not go too deep into formal proofs.\nYou might not know me, but before going to a Software Engineering School, I studied math for 2 years in Uni, and dropped out because it was too “theorical”, not concrete enough for me, I needed to grok the formulas, to visualize, feel the proofs naturally and see the applications. Sadly, partly due to how Uni works — partly due to playing League of Legends too much — I did not get the level of understanding required for my attention to stick.\nNow that I’ve become a SWE, I’ve seen the practical uses of such Math, and I now find myself needing more of it! So, step by step I’ll try to go from intuition and basics (this book) -&gt; implementing existing algorithms -&gt; proofs and more advanced formulas and theorem (some other textbook like Linear Algebra Done Right) -&gt; understanding ML architecture choices -&gt; being able to invent/discover new arch and techniques?\n\n\n\nBeing somewhat familiar with Linear Algebra and computer science, I already knew a few things, so I read pretty fast through the first bits.\nThe first thing I learned were the techniques for computing the determinant for matrices of dimensions 4 and above. There’s a technique based on the generalisation of the formula \\(\\det(\\mathbf{A}) = a_{11}a_{22} - a_{12}a_{21}\\).\nAlong with the interpretation/intuition for the determinant (consolidated by 3b1b).\nBut there’s also an algorithm (LU decomposition) used in computer science based on a set of rules for determinant and gaussian reduction."
  },
  {
    "objectID": "journal/2025-08-29.html#the-manga-guide-to-linear-algebra",
    "href": "journal/2025-08-29.html#the-manga-guide-to-linear-algebra",
    "title": "Started reading The Manga Guide to Linear Algebra",
    "section": "",
    "text": "Yes, this might sound weird if you don’t know the concept, but hear me out. After seing it praised on Twitter/X by an anon, I got myself a copy of The Manga Guide to Linear Algebra.\nIt’s pretty good if you want to quickly get up to speed, develop intuition, but does not go too deep into formal proofs.\nYou might not know me, but before going to a Software Engineering School, I studied math for 2 years in Uni, and dropped out because it was too “theorical”, not concrete enough for me, I needed to grok the formulas, to visualize, feel the proofs naturally and see the applications. Sadly, partly due to how Uni works — partly due to playing League of Legends too much — I did not get the level of understanding required for my attention to stick.\nNow that I’ve become a SWE, I’ve seen the practical uses of such Math, and I now find myself needing more of it! So, step by step I’ll try to go from intuition and basics (this book) -&gt; implementing existing algorithms -&gt; proofs and more advanced formulas and theorem (some other textbook like Linear Algebra Done Right) -&gt; understanding ML architecture choices -&gt; being able to invent/discover new arch and techniques?\n\n\n\nBeing somewhat familiar with Linear Algebra and computer science, I already knew a few things, so I read pretty fast through the first bits.\nThe first thing I learned were the techniques for computing the determinant for matrices of dimensions 4 and above. There’s a technique based on the generalisation of the formula \\(\\det(\\mathbf{A}) = a_{11}a_{22} - a_{12}a_{21}\\).\nAlong with the interpretation/intuition for the determinant (consolidated by 3b1b).\nBut there’s also an algorithm (LU decomposition) used in computer science based on a set of rules for determinant and gaussian reduction."
  },
  {
    "objectID": "notes/linear-algebra.html",
    "href": "notes/linear-algebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "\\[ \\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i} u_i v_i \\]\n\nu, v = torch.arange(3), torch.arange(3)  # torch.tensor([0, 1, 2])\ntorch.dot(u, v)  # u @ v\n\ntensor(5)\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{u})_{i} = \\mathbf{a}_{i,:} \\cdot \\mathbf{u} = \\sum_{j} a_{i,j} u_j  \\]\n\nA, u = torch.arange(6).reshape(3, 2), torch.arange(2)\nA @ u\n\ntensor([1, 3, 5])\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{B})_{i,j} = \\mathbf{a}_{i,:} \\cdot \\mathbf{b}_{:, j} = \\sum_{k} a_{i,k} b_{k,j} \\]\n\nU, V = torch.arange(6).reshape(3, 2), torch.arange(6).reshape(2, 3)\nU @ V\n\ntensor([[ 3,  4,  5],\n        [ 9, 14, 19],\n        [15, 24, 33]])\n\n\n\n\n\n\\[ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I},\\quad \\mathbf{A} \\in \\mathbb{R}^{n \\times n} \\] Note: Might not exist if \\(\\mathbf{A}\\) is not invertible, that is \\(\\det(\\mathbf{A}) = 0\\)\n\nA = torch.randn(3, 3, dtype=torch.double)\nA_i = A.inverse()\nI = torch.eye(3, dtype=torch.double)\ntorch.allclose(A @ A_i, I), torch.allclose(A_i @ A, I)\n\n(True, True)\n\n\n\n\n\n\n\n\\[ \\det(\\mathbf{A}^\\top) = \\det(\\mathbf{A}) \\] \\[ \\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A})\\det(\\mathbf{B}) \\]\n\n\n\n\\[ A = \\operatorname{diag}(a_1, \\dots, a_n), \\quad \\det(A) = \\prod_{i=1}^n a_i \\]\n\n# using I as a mask\nI = torch.eye(3, dtype=torch.float32)\nA = torch.arange(1, 10, dtype=torch.float32).reshape(3, 3) * I  # A = diag(1, 5, 9)\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\\[ \\det(T) = \\prod_{i} t_{ii}, \\quad \\text{where $T$ is triangular} \\]\n\nA = torch.triu(torch.arange(1, 10, dtype=torch.float32).reshape(3, 3))\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}\\: (= \\mathbf{A}\\mathbf{A}^\\top) \\]\nInterstingly,\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^{-1} = \\mathbf{A}^\\top \\]"
  },
  {
    "objectID": "notes/linear-algebra.html#formulas",
    "href": "notes/linear-algebra.html#formulas",
    "title": "Linear Algebra",
    "section": "",
    "text": "\\[ \\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i} u_i v_i \\]\n\nu, v = torch.arange(3), torch.arange(3)  # torch.tensor([0, 1, 2])\ntorch.dot(u, v)  # u @ v\n\ntensor(5)\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{u})_{i} = \\mathbf{a}_{i,:} \\cdot \\mathbf{u} = \\sum_{j} a_{i,j} u_j  \\]\n\nA, u = torch.arange(6).reshape(3, 2), torch.arange(2)\nA @ u\n\ntensor([1, 3, 5])\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{B})_{i,j} = \\mathbf{a}_{i,:} \\cdot \\mathbf{b}_{:, j} = \\sum_{k} a_{i,k} b_{k,j} \\]\n\nU, V = torch.arange(6).reshape(3, 2), torch.arange(6).reshape(2, 3)\nU @ V\n\ntensor([[ 3,  4,  5],\n        [ 9, 14, 19],\n        [15, 24, 33]])\n\n\n\n\n\n\\[ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I},\\quad \\mathbf{A} \\in \\mathbb{R}^{n \\times n} \\] Note: Might not exist if \\(\\mathbf{A}\\) is not invertible, that is \\(\\det(\\mathbf{A}) = 0\\)\n\nA = torch.randn(3, 3, dtype=torch.double)\nA_i = A.inverse()\nI = torch.eye(3, dtype=torch.double)\ntorch.allclose(A @ A_i, I), torch.allclose(A_i @ A, I)\n\n(True, True)\n\n\n\n\n\n\n\n\\[ \\det(\\mathbf{A}^\\top) = \\det(\\mathbf{A}) \\] \\[ \\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A})\\det(\\mathbf{B}) \\]\n\n\n\n\\[ A = \\operatorname{diag}(a_1, \\dots, a_n), \\quad \\det(A) = \\prod_{i=1}^n a_i \\]\n\n# using I as a mask\nI = torch.eye(3, dtype=torch.float32)\nA = torch.arange(1, 10, dtype=torch.float32).reshape(3, 3) * I  # A = diag(1, 5, 9)\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\\[ \\det(T) = \\prod_{i} t_{ii}, \\quad \\text{where $T$ is triangular} \\]\n\nA = torch.triu(torch.arange(1, 10, dtype=torch.float32).reshape(3, 3))\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}\\: (= \\mathbf{A}\\mathbf{A}^\\top) \\]\nInterstingly,\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^{-1} = \\mathbf{A}^\\top \\]"
  },
  {
    "objectID": "notes/linear-algebra.html#notes",
    "href": "notes/linear-algebra.html#notes",
    "title": "Linear Algebra",
    "section": "Notes",
    "text": "Notes\n\nVectors\nIn linear algebra, vectors are column vectors by default, so \\[ \\mathbf{u} \\in \\mathbb{R}^{n \\times 1}, \\quad \\mathbf{u} = \\begin{bmatrix} u_1 \\\\ \\vdots \\\\ u_n \\end{bmatrix} \\]\nIt follows that when doing a Matrix-Vector product, the matrix is on the left \\(\\mathbf{A}\\mathbf{u}\\).\n\n\nOrder of operations\nSimilarly, when we say “we apply a linear transformation \\(\\mathbf{A}\\) to \\(\\mathbf{B}\\)”, we mean \\(\\mathbf{A}\\mathbf{B}\\).\n\n\nDeterminant\nI think of the determinant of \\(\\mathbf{A}\\) as the scaling factor of the linear transformation represented by \\(\\mathbf{A}\\).\nThis explains why a matrix whose determinant is 0 is not invertible: it “collapses”, and two images might have the same original input \\(\\mathbf{x}\\)"
  },
  {
    "objectID": "notes/linear-algebra.html#proofs",
    "href": "notes/linear-algebra.html#proofs",
    "title": "Linear Algebra",
    "section": "Proofs",
    "text": "Proofs\nLater!"
  },
  {
    "objectID": "notes/linear-algebra.html#algorithms",
    "href": "notes/linear-algebra.html#algorithms",
    "title": "Linear Algebra",
    "section": "Algorithms",
    "text": "Algorithms\n\nGaussian Elimination\nGaussian elimination — or row reduction – is an algorithm for solving systems of linear equations, based on the following operations:\n\nScaling a row\nSwapping two rows\nAdding a multiple of one row to another\n\n\n\nLU decomposition (or LU factorization)\nComputing the determinant of a Matrix is not trivial at first glance.\nBut consider the following fact:\n\nwe know how to easily compute the determinant of a triangular matrix\nwe know how to easily compute the determinant of a product of matrices\n\nKnowing that, we want to find a representation of our original matrix \\(\\mathbf{A}\\) that involves an Upper Triangular Matrix \\(\\mathbf{U}\\), and one or more other matrices whose determinant is known or trivial to compute, as \\(\\mathbf{P}\\mathbf{A} = \\mathbf{L}\\mathbf{U}\\)\nTo go from \\(\\mathbf{A}\\) to \\(\\mathbf{U}\\) we’ll use Gaussian Elimination, \\(\\mathbf{P}\\) tracks our permutations (row swaps) and \\(\\mathbf{L}\\) tracks our row operations (row additions).\nNow, because \\(\\mathbf{P}\\) is orthogonal (yes, since its the identity matrix with row swaps, when performing \\(\\mathbf{P}^\\top\\mathbf{P}\\), the ones in the rows meet the ones in the columns at the diagonal, zeros everywhere else, so we get \\(\\mathbf{P}^\\top\\mathbf{P} = \\mathbf{I}\\)), we then have \\[ \\mathbf{P}\\mathbf{A} = \\mathbf{L}\\mathbf{U} \\implies \\mathbf{A} = \\mathbf{P}^{-1}\\mathbf{L}\\mathbf{U} = \\mathbf{P}^\\top\\mathbf{L}\\mathbf{U} \\]\nFinally, this means that \\[ \\det(\\mathbf{A}) = \\det(\\mathbf{P}^\\top) \\cdot \\det(\\mathbf{L}) \\cdot \\det(\\mathbf{U}) = \\det(\\mathbf{P}) \\cdot \\det(\\mathbf{L}) \\cdot \\det(\\mathbf{U}) \\]\nNow I’m not gonna prove that, but:\n\n\\(\\det(\\mathbf{P}) = (-1)^{\\#swaps}\\)\n\\(\\det(\\mathbf{L}) = 1\\) (because when adding rows, we never modify the original identity’s diagonal, so the diagonal is full of ones, and since L is a Lower Triangular Matrix, the determinant is 1)\n\nNow, if we just keep track of row swaps, we can easily compute \\(\\det(\\mathbf{A})\\)!"
  },
  {
    "objectID": "notes/linear-algebra.html#notation",
    "href": "notes/linear-algebra.html#notation",
    "title": "Linear Algebra",
    "section": "Notation",
    "text": "Notation\n\n\\(x\\): a scalar\n\\(\\mathbf{x}\\): a vector\n\\(\\mathbf{X}\\): a matrix\n\\(x_i\\): the \\(i^\\textrm{th}\\) element of vector \\(\\mathbf{x}\\)\n\\(x_{i,j}\\): the element of matrix \\(\\mathbf{X}\\) at row \\(i\\) and column \\(j\\)\n\\(\\mathbf{x}_{i, :}\\): the \\(i^\\textrm{th}\\) row-vector of \\(\\mathbf{X}\\)\n\\(\\mathbf{x}_{:,j}\\): the \\(j^\\textrm{th}\\) column-vector of \\(\\mathbf{X}\\)\n\\(\\operatorname{diag}(a_1, \\dots, a_n)\\): a diagonal matrix\n\\(\\mathbf{I}\\): the indentity matrix\n\\(\\mathbf{A}^{-1}\\): the inverse of A"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "rss\n\n\n\n\n\n\n\n\n\nLinear Algebra\n\n\n\nmath\n\n\n\nNotes, formulas and proofs for me to reference later. This is an evolving document, so it might at times be incomplete\n\n\n\n\n\nAug 29, 2025\n\n4 min\n\n\n\n\n\n\nOn Typescript\n\n\n\ntypescript\n\n\n\nA few notes on Typescript for my future self.\n\n\n\n\n\nFeb 14, 2025\n\n2 min\n\n\n\n\nNo matching items"
  }
]