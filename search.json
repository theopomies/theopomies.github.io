[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Theo POMIES",
    "section": "",
    "text": "I’m Théo, a graduate of EPITECH and currently CTO of a tech startup, Kivala. My background is in AI, large language models, backend development, and data engineering. Over the past years, I’ve worked on building robust backends with Python and FastAPI, designing automation pipelines, and deploying AI systems ranging from custom LLMs to retrieval-augmented generation (RAG) architectures.\nMy technical work usually revolves around:"
  },
  {
    "objectID": "index.html#transition-to-mle-applied-research",
    "href": "index.html#transition-to-mle-applied-research",
    "title": "Theo POMIES",
    "section": "Transition to MLE & Applied Research",
    "text": "Transition to MLE & Applied Research\nI’m now in the process of transitioning towards machine learning engineering and applied research. This blog is where I’ll share my journey — notes from my learning process, experiments, and articles about the concepts and techniques I find valuable along the way.\nIt’s part notebook, part knowledge base, and part exploration. My goal is to document both the practice of building systems and the research side of AI that makes them possible."
  },
  {
    "objectID": "notes/typescript.html",
    "href": "notes/typescript.html",
    "title": "On Typescript",
    "section": "",
    "text": "Let your types be flexible\nLet your constraints be rigid"
  },
  {
    "objectID": "notes/typescript.html#first-principles",
    "href": "notes/typescript.html#first-principles",
    "title": "On Typescript",
    "section": "",
    "text": "Let your types be flexible\nLet your constraints be rigid"
  },
  {
    "objectID": "notes/typescript.html#tldr-rules",
    "href": "notes/typescript.html#tldr-rules",
    "title": "On Typescript",
    "section": "TL;DR Rules",
    "text": "TL;DR Rules\n\nUse unknown instead of any, then use type narrowing to get the correct type.\nUse type over interface, unless you actually need to reach for an interface or need to express objects/class inheritance.\nAvoid using as to assert types, most of the time you actually want to narrow the type with checks (if/else).\nUse array.at(index) instead of array[index] unless array is a tuple (fixed size array).\nNEVER use TS specifics (enum, private in constructor, etc.)."
  },
  {
    "objectID": "notes/typescript.html#recommandations",
    "href": "notes/typescript.html#recommandations",
    "title": "On Typescript",
    "section": "Recommandations",
    "text": "Recommandations\n\nUse satisfies to check if an object fits a type but not erase the type.\nUse as const whenever possible. (Immutable data, enum-like objects, etc.)\nDefine (and export) types where they are consumed, and import them from other files if needed."
  },
  {
    "objectID": "notes/typescript.html#explanations",
    "href": "notes/typescript.html#explanations",
    "title": "On Typescript",
    "section": "Explanations",
    "text": "Explanations\n\nNarrowing over using as\nSuppose you have a function that takes a number\nfunction double(a: number) {\n    return a * 2;\n}\nAnd you have a variable that could be a number or a string\nfunction getNumberOrString(): number | string {\n    return Math.random() &gt; 0.5 ? 1 : \"1\";\n}\nconst a: number | string = getNumberOrString();\nTypescript will allow you to use as to assert the variable to a number (this is one of the ways that TypeScript is not sound)\nconst result: number = double(a as number);\nBut this not correct/sound at runtime!\nThe correct way to do this is to narrow the type with a check (if/else/early return).\nif (typeof a === \"number\") {\n    const result = double(a);\n}"
  },
  {
    "objectID": "notes/calculus.html",
    "href": "notes/calculus.html",
    "title": "Calculus",
    "section": "",
    "text": "As the name states, we analyze differences in the output(s) of a function based on differences in the input(s) \\[ \\dfrac{dy}{dx} = \\lim_{h \\rightarrow 0} \\dfrac{f(x + h) - f(x)}{h} \\]\nA function is said to be differentiable at \\(x\\) if this limit exists, and differentiable on an interval if it exists at any \\(x\\) in this interval.\n\n\nFrom that we can get common derivatives\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}C & = 0 && \\text{for any constant C} \\\\\n    \\dfrac{d}{dx}x^n & = nx^{n - 1} && \\text{for n} \\neq 0 \\\\\n    \\dfrac{d}{dx}e^x & = e^x \\\\\n    \\dfrac{d}{dx}\\ln x & = x^{-1} \\\\\n    \\dfrac{d}{dx}\\cos x & = -\\sin x \\\\\n    \\dfrac{d}{dx}\\sin x & = \\cos x \\\\\n\\end{aligned} \\]\nFrom these it becomes trivial to derive \\(\\tan\\), \\(\\sec\\), \\(\\csc\\) and \\(\\cot\\).\n\n\n\nFinally a composition of differentiable functions is also differentiable, so we have the following rules that allow us to derive almost any function:\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}Cf(x) & = C\\dfrac{d}{dx}f(x) && \\text{Constant multiple rule} \\\\\n    \\dfrac{d}{dx}[f(x) + g(x)] & = \\dfrac{d}{dx}f(x) + \\dfrac{d}{dx}g(x) && \\text{Sum rule} \\\\\n    \\dfrac{d}{dx}[f(x)g(x)] & = \\dfrac{d}{dx}f(x)g(x) + f(x)\\dfrac{d}{dx}g(x) && \\text{Product rule} \\\\\n    \\dfrac{dy}{dx} & = \\dfrac{dy}{dz}\\dfrac{dz}{dx} = \\dfrac{\\frac{dy}{dz}}{\\frac{dx}{dz}} && \\text{Chain rule} \\\\\n\\end{aligned} \\]\nFrom these we can easily find a Quotient Rule, a Power Rule and a Reciprocal Rule:\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}\\dfrac{f(x)}{g(x)} & = \\dfrac{\\frac{d}{dx}f(x)g(x) - f(x)\\frac{d}{dx}g(x)}{g(x)^2} && \\text{Quotient rule} \\\\\n    \\dfrac{d}{dx}f(x)^n & = nf(x)^{n-1}\\dfrac{d}{dx}f(x) && \\text{Power rule} \\\\\n    \\dfrac{dy}{dx}\\dfrac{1}{f(x)} & = -\\dfrac{\\frac{d}{dx}f(x)}{f(x)^2} && \\text{Reciprocal rule} \\\\\n\\end{aligned} \\]\n\n\n\n\nVery similar, but now our function takes a vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\) as input and returns a scalar \\(y \\in \\mathbb{R}\\) — or even a vector \\(\\mathbf{y} \\in \\mathbb{R}^m\\).\nTo paraphrase D2L because their explanation is perfect:\nLet \\(y = f(x_1, x_2, \\ldots, x_n)\\) be a function with \\(n\\) variables. The partial derivative of \\(y\\) with respect to its \\(i^\\textrm{th}\\) parameter \\(x_i\\) is\n\\[ \\dfrac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.\\]\nIf \\(f(\\mathbf{x}) = y\\), a scalar, we collect/concatenate all the partial derivatives to obtain the gradient of \\(y\\) with respect to \\(\\mathbf{x}\\) \\[ \\nabla_{\\mathbf{x}}f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial y}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial y}{\\partial x_n} \\end{bmatrix} \\]\n\n\nThe following rules come straight from D2L:\n\nFor all \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) we have \\(\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top\\) and \\(\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}\\).\nFor square matrices \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) we have that \\(\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}\\) and in particular \\(\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}\\).\n\nThen the chain rule states that\n\\[\\frac{\\partial y}{\\partial x_{i}} = \\frac{\\partial y}{\\partial u_{1}} \\frac{\\partial u_{1}}{\\partial x_{i}} + \\frac{\\partial y}{\\partial u_{2}} \\frac{\\partial u_{2}}{\\partial x_{i}} + \\ldots + \\frac{\\partial y}{\\partial u_{m}} \\frac{\\partial u_{m}}{\\partial x_{i}} \\ \\textrm{ and so } \\ \\nabla_{\\mathbf{x}} y =  \\mathbf{A} \\nabla_{\\mathbf{u}} y,\\]\nwhere \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times m}\\) is a matrix that contains the derivative of vector \\(\\mathbf{u}\\) with respect to vector \\(\\mathbf{x}\\)."
  },
  {
    "objectID": "notes/calculus.html#definitions-formulas",
    "href": "notes/calculus.html#definitions-formulas",
    "title": "Calculus",
    "section": "",
    "text": "As the name states, we analyze differences in the output(s) of a function based on differences in the input(s) \\[ \\dfrac{dy}{dx} = \\lim_{h \\rightarrow 0} \\dfrac{f(x + h) - f(x)}{h} \\]\nA function is said to be differentiable at \\(x\\) if this limit exists, and differentiable on an interval if it exists at any \\(x\\) in this interval.\n\n\nFrom that we can get common derivatives\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}C & = 0 && \\text{for any constant C} \\\\\n    \\dfrac{d}{dx}x^n & = nx^{n - 1} && \\text{for n} \\neq 0 \\\\\n    \\dfrac{d}{dx}e^x & = e^x \\\\\n    \\dfrac{d}{dx}\\ln x & = x^{-1} \\\\\n    \\dfrac{d}{dx}\\cos x & = -\\sin x \\\\\n    \\dfrac{d}{dx}\\sin x & = \\cos x \\\\\n\\end{aligned} \\]\nFrom these it becomes trivial to derive \\(\\tan\\), \\(\\sec\\), \\(\\csc\\) and \\(\\cot\\).\n\n\n\nFinally a composition of differentiable functions is also differentiable, so we have the following rules that allow us to derive almost any function:\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}Cf(x) & = C\\dfrac{d}{dx}f(x) && \\text{Constant multiple rule} \\\\\n    \\dfrac{d}{dx}[f(x) + g(x)] & = \\dfrac{d}{dx}f(x) + \\dfrac{d}{dx}g(x) && \\text{Sum rule} \\\\\n    \\dfrac{d}{dx}[f(x)g(x)] & = \\dfrac{d}{dx}f(x)g(x) + f(x)\\dfrac{d}{dx}g(x) && \\text{Product rule} \\\\\n    \\dfrac{dy}{dx} & = \\dfrac{dy}{dz}\\dfrac{dz}{dx} = \\dfrac{\\frac{dy}{dz}}{\\frac{dx}{dz}} && \\text{Chain rule} \\\\\n\\end{aligned} \\]\nFrom these we can easily find a Quotient Rule, a Power Rule and a Reciprocal Rule:\n\\[ \\begin{aligned}\n    \\dfrac{d}{dx}\\dfrac{f(x)}{g(x)} & = \\dfrac{\\frac{d}{dx}f(x)g(x) - f(x)\\frac{d}{dx}g(x)}{g(x)^2} && \\text{Quotient rule} \\\\\n    \\dfrac{d}{dx}f(x)^n & = nf(x)^{n-1}\\dfrac{d}{dx}f(x) && \\text{Power rule} \\\\\n    \\dfrac{dy}{dx}\\dfrac{1}{f(x)} & = -\\dfrac{\\frac{d}{dx}f(x)}{f(x)^2} && \\text{Reciprocal rule} \\\\\n\\end{aligned} \\]\n\n\n\n\nVery similar, but now our function takes a vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\) as input and returns a scalar \\(y \\in \\mathbb{R}\\) — or even a vector \\(\\mathbf{y} \\in \\mathbb{R}^m\\).\nTo paraphrase D2L because their explanation is perfect:\nLet \\(y = f(x_1, x_2, \\ldots, x_n)\\) be a function with \\(n\\) variables. The partial derivative of \\(y\\) with respect to its \\(i^\\textrm{th}\\) parameter \\(x_i\\) is\n\\[ \\dfrac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.\\]\nIf \\(f(\\mathbf{x}) = y\\), a scalar, we collect/concatenate all the partial derivatives to obtain the gradient of \\(y\\) with respect to \\(\\mathbf{x}\\) \\[ \\nabla_{\\mathbf{x}}f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial y}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial y}{\\partial x_n} \\end{bmatrix} \\]\n\n\nThe following rules come straight from D2L:\n\nFor all \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) we have \\(\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top\\) and \\(\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}\\).\nFor square matrices \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) we have that \\(\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}\\) and in particular \\(\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}\\).\n\nThen the chain rule states that\n\\[\\frac{\\partial y}{\\partial x_{i}} = \\frac{\\partial y}{\\partial u_{1}} \\frac{\\partial u_{1}}{\\partial x_{i}} + \\frac{\\partial y}{\\partial u_{2}} \\frac{\\partial u_{2}}{\\partial x_{i}} + \\ldots + \\frac{\\partial y}{\\partial u_{m}} \\frac{\\partial u_{m}}{\\partial x_{i}} \\ \\textrm{ and so } \\ \\nabla_{\\mathbf{x}} y =  \\mathbf{A} \\nabla_{\\mathbf{u}} y,\\]\nwhere \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times m}\\) is a matrix that contains the derivative of vector \\(\\mathbf{u}\\) with respect to vector \\(\\mathbf{x}\\)."
  },
  {
    "objectID": "notes/calculus.html#notes",
    "href": "notes/calculus.html#notes",
    "title": "Calculus",
    "section": "Notes",
    "text": "Notes\nBecause of the definition of derivative as a rate of change, this is possible \\(\\dfrac{dy}{dx} = \\dfrac{1}{\\frac{dx}{dy}}\\)"
  },
  {
    "objectID": "notes/calculus.html#proofs",
    "href": "notes/calculus.html#proofs",
    "title": "Calculus",
    "section": "Proofs",
    "text": "Proofs\nLater!"
  },
  {
    "objectID": "notes/calculus.html#notation",
    "href": "notes/calculus.html#notation",
    "title": "Calculus",
    "section": "Notation",
    "text": "Notation\n\n\\(f(\\cdot)\\): a function\n\\(\\dfrac{dy}{dx}\\): derivative of \\(y\\) with respect to \\(x\\)\n\\(\\dfrac{\\partial y}{\\partial x}\\): partial derivative of \\(y\\) with respect to \\(x\\)\n\\(\\nabla_{\\mathbf{x}} y\\): gradient of \\(y\\) with respect to \\(\\mathbf{x}\\)\n\\(\\int_a^b f(x) \\;dx\\): definite integral of \\(f\\) from \\(a\\) to \\(b\\) with respect to \\(x\\)\n\\(\\int f(x) \\;dx\\): indefinite integral of \\(f\\) with respect to \\(x\\)"
  },
  {
    "objectID": "notes/distributed-training.html",
    "href": "notes/distributed-training.html",
    "title": "Distributed Training",
    "section": "",
    "text": "send and recv to send or receive a tensor synchronously — from/to a single rank.\nAnd their async counterparts, isend and irecv.\n\nrank = dist.get_rank()\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\nif rank == 0:\n    request = dist.isend(tensor, 1)\n    ...\n    # can do something else, like more sends for example!\n    ...\n    request.wait() # now block until it's been fulfilled\nelif rank == 1:\n    dist.recv(tensor, 0) # recv is synchronous, so it will block until tensor is fully received\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n\n\n\n\nCollective operations allow communication (data-transfer) from All-&gt;Point, Point-&gt;All and All-&gt;All.\n\n\n\n\nBroadcast (torch.distributed.broadcast(tensor, src, ...)) allows a rank to broadcast a tensor to the whole group.\n\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\ndist.broadcast(tensor, src=0)\n\nprint(f\"After: {tensor})\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 2 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n\n\n\n\nScatter (torch.distributed.scatter(tensor, scatter_list, src, ...)) allows us to scatter — split and broadcast different chunks of — a tensor from a rank to the whole group.\n\ntensor = torch.zeros(3)\nscatter_list = [torch.arange(3 * i, 3 * i + 3) if rank == 0 else torch.zeros(3) for i in range(world_size)]\n\nprint(f\"Scatter list: {scatter_list}\")\nprint(f\"Before: {tensor}\")\n\ndist.scatter(tensor, scatter_list, src=0)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nScatter list: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nScatter list: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([3, 4, 5])\n\n\n\n\n\n\n\n\nReduce (torch.distributed.reduce(tensor, dst, op, ...)) performs a reduction operation (N-&gt;1, eg. sum, max, min, prod, …) and the dst rank receives the result.\n\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 4, 5])\n\n\n\n\n\nGather (torch.distributed.gather(tensor, gather_list, dst, ...)) gathers — pulls — a tensor, of the same size, from every rank and stores them in a list in a single rank.\n\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.gather(tensor, gather_list, dst=0)\n\nprint(f\"After: {gather_list}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([3, 4, 5])\n\n========== rank 1 ==========\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nAfter: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\n\n\n\n\n\n\n\n\nAll-Reduce (torch.distributed.all_reduce(tensor, op, ...)) performs a reduction operation, like reduce, but every rank receives the result — rather than a single one with reduce. Think of it as reduce + broadcast — though it is optimized by techniques like ring-reduce.\n\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 5, 7])\n\n\n\n\n\nAll-Gather (torch.distributed.all_gather(tensor, gather_list, ...)) gathers — pulls — a tensor, of the same size, from every rank and stores them in a list in every rank. Think of it as running gather on all ranks.\n\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.all_gather(tensor, gather_list)\n\nprint(f\"After: {gather_list}\")\n\n\n\n========== rank 2 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n\n\n\n\n\nReduce-Scatter (torch.distributed.reduce_scatter(output_tensor, input_list, op, ...)) performs a reduction operation — like other reduce functions — and scatters the resulting tensor. Think of it like reduce + scatter. Note: it needs len(input_list) == world_size and every tensor in input_list to have the same shape of output_tensor.\n\ntensor = torch.zeros(2)\nscatter_list = [torch.tensor([(rank + 1) * i for i in range(1, 3)]) ** (j + 1) for j in range(3)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {scatter_list}\")\n\ndist.reduce_scatter(tensor, scatter_list, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([1, 2]), tensor([1, 4]), tensor([1, 8])]\nAfter: tensor([ 6, 12])\n\n========== rank 1 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([2, 4]), tensor([ 4, 16]), tensor([ 8, 64])]\nAfter: tensor([14, 56])\n\n========== rank 2 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([3, 6]), tensor([ 9, 36]), tensor([ 27, 216])]\nAfter: tensor([ 36, 288])\n\n\n\n\n\n\n\n\nBarrier (torch.distributed.barrier(...)) synchronizes all ranks (processes), waiting for all of them to reach the barrier. (think of .join() for threads or processes)"
  },
  {
    "objectID": "notes/distributed-training.html#functions-methods",
    "href": "notes/distributed-training.html#functions-methods",
    "title": "Distributed Training",
    "section": "",
    "text": "send and recv to send or receive a tensor synchronously — from/to a single rank.\nAnd their async counterparts, isend and irecv.\n\nrank = dist.get_rank()\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\nif rank == 0:\n    request = dist.isend(tensor, 1)\n    ...\n    # can do something else, like more sends for example!\n    ...\n    request.wait() # now block until it's been fulfilled\nelif rank == 1:\n    dist.recv(tensor, 0) # recv is synchronous, so it will block until tensor is fully received\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n\n\n\n\nCollective operations allow communication (data-transfer) from All-&gt;Point, Point-&gt;All and All-&gt;All.\n\n\n\n\nBroadcast (torch.distributed.broadcast(tensor, src, ...)) allows a rank to broadcast a tensor to the whole group.\n\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\ndist.broadcast(tensor, src=0)\n\nprint(f\"After: {tensor})\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 2 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n\n\n\n\nScatter (torch.distributed.scatter(tensor, scatter_list, src, ...)) allows us to scatter — split and broadcast different chunks of — a tensor from a rank to the whole group.\n\ntensor = torch.zeros(3)\nscatter_list = [torch.arange(3 * i, 3 * i + 3) if rank == 0 else torch.zeros(3) for i in range(world_size)]\n\nprint(f\"Scatter list: {scatter_list}\")\nprint(f\"Before: {tensor}\")\n\ndist.scatter(tensor, scatter_list, src=0)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nScatter list: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nScatter list: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([3, 4, 5])\n\n\n\n\n\n\n\n\nReduce (torch.distributed.reduce(tensor, dst, op, ...)) performs a reduction operation (N-&gt;1, eg. sum, max, min, prod, …) and the dst rank receives the result.\n\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 4, 5])\n\n\n\n\n\nGather (torch.distributed.gather(tensor, gather_list, dst, ...)) gathers — pulls — a tensor, of the same size, from every rank and stores them in a list in a single rank.\n\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.gather(tensor, gather_list, dst=0)\n\nprint(f\"After: {gather_list}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([3, 4, 5])\n\n========== rank 1 ==========\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nAfter: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\n\n\n\n\n\n\n\n\nAll-Reduce (torch.distributed.all_reduce(tensor, op, ...)) performs a reduction operation, like reduce, but every rank receives the result — rather than a single one with reduce. Think of it as reduce + broadcast — though it is optimized by techniques like ring-reduce.\n\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 5, 7])\n\n\n\n\n\nAll-Gather (torch.distributed.all_gather(tensor, gather_list, ...)) gathers — pulls — a tensor, of the same size, from every rank and stores them in a list in every rank. Think of it as running gather on all ranks.\n\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.all_gather(tensor, gather_list)\n\nprint(f\"After: {gather_list}\")\n\n\n\n========== rank 2 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n\n\n\n\n\nReduce-Scatter (torch.distributed.reduce_scatter(output_tensor, input_list, op, ...)) performs a reduction operation — like other reduce functions — and scatters the resulting tensor. Think of it like reduce + scatter. Note: it needs len(input_list) == world_size and every tensor in input_list to have the same shape of output_tensor.\n\ntensor = torch.zeros(2)\nscatter_list = [torch.tensor([(rank + 1) * i for i in range(1, 3)]) ** (j + 1) for j in range(3)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {scatter_list}\")\n\ndist.reduce_scatter(tensor, scatter_list, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n\n\n\n========== rank 0 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([1, 2]), tensor([1, 4]), tensor([1, 8])]\nAfter: tensor([ 6, 12])\n\n========== rank 1 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([2, 4]), tensor([ 4, 16]), tensor([ 8, 64])]\nAfter: tensor([14, 56])\n\n========== rank 2 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([3, 6]), tensor([ 9, 36]), tensor([ 27, 216])]\nAfter: tensor([ 36, 288])\n\n\n\n\n\n\n\n\nBarrier (torch.distributed.barrier(...)) synchronizes all ranks (processes), waiting for all of them to reach the barrier. (think of .join() for threads or processes)"
  },
  {
    "objectID": "notes/distributed-training.html#algorithms-techniques",
    "href": "notes/distributed-training.html#algorithms-techniques",
    "title": "Distributed Training",
    "section": "Algorithms / Techniques",
    "text": "Algorithms / Techniques\n\nDDP — Distributed Data Parallelism\nDistributed Data Parallelism is a type of parallelism where each rank loads a copy — replica — of the model, after each optimizer step they always all have the same parameters, they are replicants. Each rank then trains on a different mini-batch (hence the importance of data sharding). We then average the gradients (all_reducesum + division by world_size), perform a step of gradient descent, rinse and repeat. If we can use this, we should, it has the least amount of overhead, but it requires that the model + optimizer states all fit in the device’s VRAM.\nNote: the difference between DDP and DP is that DDP uses processes to avoid the GIL and DP uses threads. Do not use DP, only DDP.\nclass SimpleDataParaellism():\n    def __init__(self, model):\n        self.model = model\n\n        for param in model.parameters():\n            rank_0_params = param.data.clone()\n            dist.broadcast(rank_0_params, src=0)\n            assert torch.equal(param.data, rank_0_params), \"Parameters mismatch at initialization\"\n\n    def sync_grad(self):\n        for param in model.parameters():\n            dist.all_reduce(param.grad, op=dist.ReduceOp.SUM)\n            param.grad /= dist.get_world_size()\n\n\nData Sharding\nData Sharding is the process of sharding — splitting — the dataset / dataloader so that each rank only pulls their own unique mini-batches of the training data. This avoids duplicates and is more commucation / memory efficient that duplicating the same full dataset on every rank. To do this with torch, setup the DataLoader with sampler=[instance of DistributedSampler]."
  },
  {
    "objectID": "notes/distributed-training.html#notes",
    "href": "notes/distributed-training.html#notes",
    "title": "Distributed Training",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "notes/distributed-training.html#terminology",
    "href": "notes/distributed-training.html#terminology",
    "title": "Distributed Training",
    "section": "Terminology",
    "text": "Terminology\n\ndevice: Hardware unit — GPU \"cuda:0\", CPU \"cpu\" etc. that’s where tensors and computations live\nnode: Phyisical machine/server (or VPS whatever) that has 1+ devices\nprocess: Python process/worker, executing a copy of the code/script — often on a single device (GPU)\nrank: ID of a process — often that maps to a single device. rank without qualifiers is global rank\nworld: Set of all processes part of our current distributed job\nglobal rank, world rank: rank across all processes/nodes. note: collective operations take the global rank (or just rank) as input for src/dst\nlocal rank: rank within a single node (node not group). note: device takes the local rank \"cuda:{local_rank}\"\ngroup: subset of processes (1+ nodes) that we’ve grouped for sub-communications. note: we still use global rank for intra-group communication."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "rss\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "journal/2025-08-29.html",
    "href": "journal/2025-08-29.html",
    "title": "Started reading The Manga Guide to Linear Algebra",
    "section": "",
    "text": "Yes, this might sound weird if you don’t know the concept, but hear me out. After seing it praised on Twitter/X by an anon, I got myself a copy of The Manga Guide to Linear Algebra.\nIt’s pretty good if you want to quickly get up to speed, develop intuition, but does not go too deep into formal proofs.\nYou might not know me, but before going to a Software Engineering School, I studied math for 2 years in Uni, and dropped out because it was too “theorical”, not concrete enough for me, I needed to grok the formulas, to visualize, feel the proofs naturally and see the applications. Sadly, partly due to how Uni works — partly due to playing League of Legends too much — I did not get the level of understanding required for my attention to stick.\nNow that I’ve become a SWE, I’ve seen the practical uses of such Math, and I now find myself needing more of it! So, step by step I’ll try to go from intuition and basics (this book) -&gt; implementing existing algorithms -&gt; proofs and more advanced formulas and theorem (some other textbook like Linear Algebra Done Right) -&gt; understanding ML architecture choices -&gt; being able to invent/discover new arch and techniques?\n\n\n\nBeing somewhat familiar with Linear Algebra and computer science, I already knew a few things, so I read pretty fast through the first bits.\nThe first thing I learned were the techniques for computing the determinant for matrices of dimensions 4 and above. There’s a technique based on the generalisation of the formula \\(\\det(\\mathbf{A}) = a_{11}a_{22} - a_{12}a_{21}\\).\nAlong with the interpretation/intuition for the determinant (consolidated by 3b1b).\nBut there’s also an algorithm (LU decomposition) used in computer science based on a set of rules for determinant and gaussian reduction."
  },
  {
    "objectID": "journal/2025-08-29.html#the-manga-guide-to-linear-algebra",
    "href": "journal/2025-08-29.html#the-manga-guide-to-linear-algebra",
    "title": "Started reading The Manga Guide to Linear Algebra",
    "section": "",
    "text": "Yes, this might sound weird if you don’t know the concept, but hear me out. After seing it praised on Twitter/X by an anon, I got myself a copy of The Manga Guide to Linear Algebra.\nIt’s pretty good if you want to quickly get up to speed, develop intuition, but does not go too deep into formal proofs.\nYou might not know me, but before going to a Software Engineering School, I studied math for 2 years in Uni, and dropped out because it was too “theorical”, not concrete enough for me, I needed to grok the formulas, to visualize, feel the proofs naturally and see the applications. Sadly, partly due to how Uni works — partly due to playing League of Legends too much — I did not get the level of understanding required for my attention to stick.\nNow that I’ve become a SWE, I’ve seen the practical uses of such Math, and I now find myself needing more of it! So, step by step I’ll try to go from intuition and basics (this book) -&gt; implementing existing algorithms -&gt; proofs and more advanced formulas and theorem (some other textbook like Linear Algebra Done Right) -&gt; understanding ML architecture choices -&gt; being able to invent/discover new arch and techniques?\n\n\n\nBeing somewhat familiar with Linear Algebra and computer science, I already knew a few things, so I read pretty fast through the first bits.\nThe first thing I learned were the techniques for computing the determinant for matrices of dimensions 4 and above. There’s a technique based on the generalisation of the formula \\(\\det(\\mathbf{A}) = a_{11}a_{22} - a_{12}a_{21}\\).\nAlong with the interpretation/intuition for the determinant (consolidated by 3b1b).\nBut there’s also an algorithm (LU decomposition) used in computer science based on a set of rules for determinant and gaussian reduction."
  },
  {
    "objectID": "journal/index.html",
    "href": "journal/index.html",
    "title": "Journal",
    "section": "",
    "text": "rss\n\n\n\n\n\n\n\n\n\nRay / Anyscale workshop with S2S\n\n\n\npython\n\nmle\n\n\n\nAn enlighting workshop to learn Ray with Robert Nishihara of Anyscale\n\n\n\n\n\nSep 5, 2025\n\n2 min\n\n\n\n\n\n\nGoing through DDP with S2S\n\n\n\npython\n\nmle\n\nmath\n\n\n\nFinishing the preliminaries for D2L and quick evening study session on DDP with torch.distributed primitives\n\n\n\n\n\nSep 4, 2025\n\n2 min\n\n\n\n\n\n\nFireside chat QA with Yuxiang Wei from Meta FAIR with S2S\n\n\n\nmle\n\npython\n\n\n\nQuestions surrounding distributed training at FAIR\n\n\n\n\n\nSep 3, 2025\n\n1 min\n\n\n\n\n\n\nStarting Scratch To Scale (S2S) and Dive into Deep Learning (D2L)\n\n\n\nmath\n\npython\n\nmle\n\n\n\nLearning about distributed training on GPUs (S2S) and preliminaries for D2L\n\n\n\n\n\nSep 2, 2025\n\n1 min\n\n\n\n\n\n\nFinished reading The Manga Guide to Linear Algebra\n\n\n\nmath\n\n\n\nLearning more about Linear Transformations, Eigen{vectors, values} and Diagonalization\n\n\n\n\n\nSep 1, 2025\n\n1 min\n\n\n\n\n\n\nStarted reading The Manga Guide to Linear Algebra\n\n\n\nmath\n\n\n\n(Re-)Developping intuition around Linear Algebra\n\n\n\n\n\nAug 29, 2025\n\n2 min\n\n\n\n\n\n\nSetting up this blog!\n\n\n\npython\n\n\n\nToday I decided I would convert my old Next.js blog to Notebooks + Quarto and document my learning process\n\n\n\n\n\nAug 28, 2025\n\n1 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "journal/2025-08-28.html",
    "href": "journal/2025-08-28.html",
    "title": "Setting up this blog!",
    "section": "",
    "text": "Today’s the day!\nI’ve decided to document my learning process to “transition” from SWE / using AI \\(\\longrightarrow\\) MLE/Applied Research/ building and training AI.\nAfter experimenting with marimo, loving it and struggling to make a blog out of marimo notebooks and experiments, I’ve stumbled upon this article by Jeremy Howard encouraging me to write my blog via notebooks + Quarto.\nIt also happens that I had just purchased the Scratch To Scale course/conference by the brilliant Zach Mueller and his blog was also using Quarto — which I would soon learn was no coincidence, Zach and Jeremy were closed and Zach contributed to Jeremy’s framework, fast.ai.\nSo there it is, I intend to document my learning process, render notebooks of experiments, store knowledge, formulas I’ve finally understood even if I’m the last person on earth to do so, bookmarks, reading notes etc."
  },
  {
    "objectID": "journal/2025-08-28.html#this-blog",
    "href": "journal/2025-08-28.html#this-blog",
    "title": "Setting up this blog!",
    "section": "",
    "text": "Today’s the day!\nI’ve decided to document my learning process to “transition” from SWE / using AI \\(\\longrightarrow\\) MLE/Applied Research/ building and training AI.\nAfter experimenting with marimo, loving it and struggling to make a blog out of marimo notebooks and experiments, I’ve stumbled upon this article by Jeremy Howard encouraging me to write my blog via notebooks + Quarto.\nIt also happens that I had just purchased the Scratch To Scale course/conference by the brilliant Zach Mueller and his blog was also using Quarto — which I would soon learn was no coincidence, Zach and Jeremy were closed and Zach contributed to Jeremy’s framework, fast.ai.\nSo there it is, I intend to document my learning process, render notebooks of experiments, store knowledge, formulas I’ve finally understood even if I’m the last person on earth to do so, bookmarks, reading notes etc."
  },
  {
    "objectID": "journal/2025-09-05.html",
    "href": "journal/2025-09-05.html",
    "title": "Ray / Anyscale workshop with S2S",
    "section": "",
    "text": "Going down the ray rabbit hole\nToday we had a workshop on Ray x Anyscale by Robert Nishihara — co-founder and CEO of Anyscale, co-creator of Ray — himself!\nAt first I thought this was just gonna be a cloud ad. Like “look this is our platform, here’s how to use it, please do” (lol). I didn’t know Ray, since I don’t know much — yet — about distributed workloads.\nThe workshop was nice, we covered Ray Data and Ray Train, their data and training libraries. These looked like powerful stuff, but I was still cautious about it, they looked like some frameworks that were too high level, too much abstraction for me, à la fast.ai (it’s good, but too high level for me sometimes, I like the from scratch feel of some other stuff, the tweakability).\nDuring the workshop someone asked about using Ray + vLLM (LLM inference engine). I thought “one’s for training, the other for inference, I don’t see the intersection here”. Oh boy was I wrong. After seing the Anyscale employes answer, I realized I didn’t fully grasp what Ray was and what it offered. So, naturally, I started digging. Know I know that Ray is fully OSS, it’s a library for distributed pythonic applications. Ray Core is just that, and that’s already a lot, it allows us to create remote Tasks and Actors, very good parallel / distributed primitives. Then they built higher-level utilities above that, with Ray {Data, Train, Tune and Serve}.\nI had badly misjuged it, I feel like we’re gonna become very close friends.\nBefore I use a lib I like to know what it does and how it does it, but once I’ll have built a minimal toy version, it’s gonna be you and me buddy.\n\n\nLapace Expansion\nTo finish the day, I wrote a quick Laplace Expansion function in DeepML."
  },
  {
    "objectID": "journal/2025-09-04.html",
    "href": "journal/2025-09-04.html",
    "title": "Going through DDP with S2S",
    "section": "",
    "text": "Finishing the preliminaries for D2L\nWell, finally I’m done with the “catching up” for D2L, with a good refresher on Probabilities. Since I studied stats and probabilities at university, I didn’t learn anything new here, but it’s always good to refresh some knowledge, and re-derive formulas, to have them in mind and grok or accept them as true. Like the classic Bayes Theorem, I always accepted the formula as a way to flip conditional probabilities, but rarely as a way of updating a prior.\n\n\nEvening study session on DDP\nWe discussed the different types of parallelism (for ML distributed training):\n\n(Distributed) Data Parallelism (DDP): Each rank loads a copy — replica — of the model, after each optimizer step they always all have the same parameters, they are replicants. Each rank then trains on a different mini-batch (hence the importance of data sharding). We then average the gradients, perform a step of gradient descent, rinse and repeat. If we can use this, we should, it has the least amount of overhead, but it requires that the model + optimizer states all fit in the device’s VRAM.\nPipeline Parallelism (PP): We split the model across different ranks without splitting the layers (so we split along the layers). That is inter-layer parallelism. An exaggeration would be having a model with 2 hidden layers and 1 output layer split across 3 GPUs.\nTensor Parallelism (TP): We split the layers of the model across different ranks, that’s intra-layer parallelism. This could be useful if some layers are so large they don’t even fit in a single device. To reduce overhead, it’s advisable that even if a layer is split across different ranks, all parts remain on the same node. (See terminology)\n\nThere’s also\n\nExpert Parallelism (EP): For Mixture-of-Experts (MoE) we can split the experts on different devices.\n\nWe didn’t discuss much this last one but I knew about it already and searched a bit to know more about it.\nLast but not least we discussed the issue of DataLoaders / Samplers, and the importance of data sharding. They are a mean of “splitting” efficiently our dataset so that each rank sees a different mini-batch of data. Also, each process only loads its shard so it’s more memory/communication efficient."
  },
  {
    "objectID": "journal/2025-09-01.html",
    "href": "journal/2025-09-01.html",
    "title": "Finished reading The Manga Guide to Linear Algebra",
    "section": "",
    "text": "I had a lot of work to do for my day job, we are trying to build and commercialize V2 of our intercom system, and I was faced with technical difficulties that left me very little time to work on math / ML. I still found the time to finish The Manga Guide to Linear Algebra, and learned a few important concepts and techniques!\n\nStill, I did a few things\nTIL about [subspaces], [linear spans], the [rank] of a matrix and a technique to find it via Gaussian Eliminination — as usual, linear algebra appears to be the art of applying Gaussian Elimination correctly —, [Eigenvalues] and [Eigenvectors]! That was still packed!\nI skimmed through the materials for Scratch To Scale, the course I’m following by Scott Mueller, beginning tomorrow."
  },
  {
    "objectID": "journal/2025-09-03.html",
    "href": "journal/2025-09-03.html",
    "title": "Fireside chat QA with Yuxiang Wei from Meta FAIR with S2S",
    "section": "",
    "text": "Well today I woke up at 5am to go on-site at my startup in Paris (I live 500km away, in Bordeaux) and got home at around 7pm, so not much free time to study, so I just read a bit in the train and ML related stuff was just the evening fireside chat.\n\nFireside chat with Yuxiang Wei from Meta FAIR (S2S)\nAs part of the course (conference) Scratch To Scale, we have a lot of very qualified engineers and researchers from top labs / companies dedicating time for guest lectures or QA sessions.\nToday we had the chance to listen to Yuxiang Wei — of Meta FAIR, of the SWE-RL paper — and ask him loads of questions. We discussed topics such as {pre, mid, post}-training, training infrastructure, the advent of RL and its impacts on said infrastructure and more."
  },
  {
    "objectID": "journal/2025-09-02.html",
    "href": "journal/2025-09-02.html",
    "title": "Starting Scratch To Scale (S2S) and Dive into Deep Learning (D2L)",
    "section": "",
    "text": "Today I revised the basics of calculus on D2L, and ended the day with the first “lesson” on Distributed Training (S2S) by Zach Mueller.\n\nCalculus\nI worked a bit on calculus, and there’s always something to learn, even we you go as far back as the high-school level stuff. Small example, it’s just today that I realized that \\(\\dfrac{dx}{dy} = \\dfrac{1}{\\frac{dy}{dx}}\\) (cf. the definition of derivative as a limit).\nI also got my hands back into multivariate calculus and learning useful identities.\n\n\nDistributed Training (S2S)\nFinally, I finished the day learning the basics of distributed/parallel processing/training on GPUs (using torch.distributed, we’re not yet at the triton or CUDA level, but someday we’ll be there, just watch).\nWe went from the primitives — (i)send and (i)recv — to the collective operations — reduce, all_reduce, scatter, reduce_scatter, broadcast, barrier, all2all, gather, all_gather. I can now much more easily conceive how distributed training algorithms work.\nI learned a few distributed training concepts, such as the rank.\nI concluded the day by running my first notebooks accelerated by more than 1 GPU on Modal. I’d done some lightly GPU accelerated stuff on Kaggle, but now I could grasp how to do stuff with multiple GPUs."
  },
  {
    "objectID": "notes/probability.html",
    "href": "notes/probability.html",
    "title": "Probability and Information Theory",
    "section": "",
    "text": "When studying probability, we are performing experiments, random trials or observations. The set of all possible outcomes of this experiment is \\(\\mathcal{\\Omega}\\) (or \\(\\mathcal{S}\\)). eg. When rolling a die, \\(\\mathcal{\\Omega} = \\{1,2,3,4,5,6\\}\\).\nWe can group these outcomes into events — \\(\\mathcal{E} \\subseteq \\mathcal{\\Omega}\\). eg. The event \\(\\mathcal{E} = \\{die shows an even number\\} = \\{2, 4, 6\\}\\). Whenever the outcome \\(z\\) of the random experiment satisfies \\(z \\in \\mathcal{E}\\), the event \\(\\mathcal{E}\\) has occurred. Multiple events can occur from the same outcome, say we have \\(\\mathcal{A} = \\{3, 6\\}\\) “the result is divisible by 3” and \\(\\mathcal{B} = \\{2, 4, 6\\}\\). \\(z = 6\\) satisfies both \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\).\n\n\n\nThe probability function maps events onto a real value \\(P\\colon \\mathcal{E} \\subseteq \\mathcal{\\Omega} \\to [0, 1]\\).\n\\(\\operatorname{P}(\\mathcal{E})\\) is the probability associated with event \\(\\mathcal{E}\\).\n\n\n\n\\(\\operatorname{P}(\\mathcal{E}) \\geq 0\\)\n\\(\\operatorname{P}(\\mathcal{\\Omega}) = 1, \\operatorname{P}(\\mathcal{\\emptyset}) = 0\\)\n\\(\\operatorname{P}(\\mathcal{A} \\cup \\mathcal{B}) = \\operatorname{P}(\\mathcal{A}) + \\operatorname{P}(\\mathcal{B}) - \\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B})\\)\n\\(\\operatorname{P}(\\bigcup_{i=1}^{\\infty} \\mathcal{A}_i) = \\sum_{i=1}^{\\infty} \\operatorname{P}(\\mathcal{A}_i), \\quad \\mathcal{A}_i \\cap \\mathcal{A}_j = \\emptyset\\: \\text{for all}\\: i \\neq j\\) (= if all events \\(\\mathcal{A}_i\\) are mutually exclusive)\n\\(\\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B}) = \\operatorname{P}(\\mathcal{A} \\mid \\mathcal{B})\\operatorname{P}(\\mathcal{B})\\)\n\\(\\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B}) = \\operatorname{P}(\\mathcal{A})\\operatorname{P}(\\mathcal{B}) \\iff \\mathcal{A} \\perp \\mathcal{B}\\) (eg. 2 fair dice rolls)\n\\(\\mathcal{A} \\perp \\mathcal{B} \\iff \\operatorname{P}(\\mathcal{A} \\mid \\mathcal{B}) = \\operatorname{P}(\\mathcal{A})\\)\n\n\n\n\n\nA random variable \\(X\\) is a measurable function (mapping) \\(X \\colon \\mathcal{\\Omega} \\to \\mathcal{E}\\) from a sample space \\(\\mathcal{\\Omega}\\) as a set of possible outcomes to a measurable space \\(\\mathcal{E}\\).\nThe probability that \\(X\\) takes on a value in a measurable set \\(\\mathcal{S} \\in \\mathcal{E}\\) is written as \\(\\operatorname{P}(X \\in \\mathcal{S}) = \\operatorname{P}(\\{\\omega \\in \\mathcal{\\Omega} \\mid X(\\omega) \\in \\mathcal{S}\\})\\)\nThe probability that \\(X\\) takes discrete value \\(v\\), denoted \\(X = v\\), is \\(\\operatorname{P}(X=v)\\).\n\\(X = v\\) or \\(X \\geq v\\) are events.\nRandom variables allow us to go from outcomes to values, like \\(X(\\omega) = \\omega\\) the random variable that associates to each die its value (identity function). This is also an example of a discrete random variable.\nWhen \\(X\\) is continuous it doesn’t make sense to have events like \\(X = v\\) (and \\(\\operatorname{P}(X = v) = 0\\)), rather we use \\(v \\leq X \\leq w\\) and probability densities. An example would be the height of a population.\nWe note \\(\\operatorname{P}(X)\\) the probability distribution of X. (Abuse of notation: strictly \\(P_X\\) is the distribution of \\(X\\), but we’ll often just write \\(P(X)\\))\n\n\n\\(\\operatorname{P}(A = a, B = b)\\) is the joint probability of \\(A = a\\) and \\(B = b\\) (it’s the intersection of the events \\(A = a\\) and \\(B = b\\)). Equivalently it’s \\(\\operatorname{P}(\\{A = a\\} \\cap \\{B = b\\})\\), with an overloaded notation, the joint probability distribution becomes \\(\\operatorname{P}(A, B)\\)\nObviously \\[ \\operatorname{P}(A = a, B = b) \\leq \\operatorname{P}(A=a) \\quad \\text{and} \\quad \\operatorname{P}(A = a, B = b) \\leq \\operatorname{P}(B=b) \\]\nAlso, we can marginalize \\[ \\operatorname{P}(A = a) = \\sum_v \\operatorname{P}(A = a, B = v) \\]\nBecause \\(A = a\\) and \\(B = b\\) are events, \\[\\begin{aligned}\n\\operatorname{P}(A = a, B = b) & = \\operatorname{P}(A = a \\mid B = b)\\operatorname{P}(B = b) \\\\\n\\iff \\operatorname{P}(A = a \\mid B = b) & = \\operatorname{P}(A = a, B = b)/\\operatorname{P}(B = b)\n\\end{aligned}\\]\n\n\n\n\nFrom the properties and definitions above, we can derive the following formula\n\\[ \\overbrace{\\operatorname{P}(A \\mid B)}^{\\text{posterior probability}} = \\dfrac{\\overbrace{\\operatorname{P}(B \\mid A)}^{\\text{likelihood}}\\overbrace{\\operatorname{P}(A)}^{\\text{prior}}}{\\underbrace{\\operatorname{P}(B)}_{\\text{observation}}} \\]\n\nprior/hypothesis: our estimate or current belief about the probability of \\(A\\)\nobservation/marginal likelihood/evidence: the evidence or observations we’ve made regarding \\(B\\)\nlikelihood: a measure of how compatible our hypothesis is with our observation\n\nA simplified version is \\(\\operatorname{P}(A \\mid B) \\propto \\operatorname{P}(B \\mid A)\\operatorname{P}(A)\\)\n\n\n\n\n\nThe expectation (or expected value) is the weighted average of the values of \\(X\\).\nDiscrete case:\n\\[ \\operatorname{E}[X] = \\operatorname{E}_{x \\sim P}[X] = \\sum_x x\\operatorname{P}(X=x) \\]\nContinuous case:\n\\[\\operatorname{E}[X] = \\int_{-\\infty}^{\\infty} x f(x) \\;dx \\]\nTo follow mathematical notation, sometimes we use \\(\\mu\\) to denote this average.\n\n\n\nThe variance is a measure of dispersion, it quantifies how much do values vary relative to the expectation (average) on average. The variance is the expectation of the squared difference between the values and the expected value.\n\\[ \\operatorname{Var}(X) = \\operatorname{E}[(X - \\operatorname{E}[X])^2] = \\operatorname{E}[X^2] - (\\operatorname{E}[X])^2 \\]\nBecause\n\\[ \\operatorname{E}[X^2 - 2X\\operatorname{E}[X] + \\operatorname{E}[X]^2] = \\operatorname{E}[X^2] - 2(\\operatorname{E}[X])^2 + (\\operatorname{E}[X])^2 \\]\n\n\n\nBecause the variance is a squared difference, we can take its square root to get the standard deviation which has the benefit of being in the same unit as our random variable.\n\\[ \\operatorname{Var}(X) = \\sigma^2_X \\iff \\sigma_X = \\sqrt{\\operatorname{Var}(X)} \\]\n\n\n\nTODO once I understand it fully enough to explain it."
  },
  {
    "objectID": "notes/probability.html#definitions-formulas",
    "href": "notes/probability.html#definitions-formulas",
    "title": "Probability and Information Theory",
    "section": "",
    "text": "When studying probability, we are performing experiments, random trials or observations. The set of all possible outcomes of this experiment is \\(\\mathcal{\\Omega}\\) (or \\(\\mathcal{S}\\)). eg. When rolling a die, \\(\\mathcal{\\Omega} = \\{1,2,3,4,5,6\\}\\).\nWe can group these outcomes into events — \\(\\mathcal{E} \\subseteq \\mathcal{\\Omega}\\). eg. The event \\(\\mathcal{E} = \\{die shows an even number\\} = \\{2, 4, 6\\}\\). Whenever the outcome \\(z\\) of the random experiment satisfies \\(z \\in \\mathcal{E}\\), the event \\(\\mathcal{E}\\) has occurred. Multiple events can occur from the same outcome, say we have \\(\\mathcal{A} = \\{3, 6\\}\\) “the result is divisible by 3” and \\(\\mathcal{B} = \\{2, 4, 6\\}\\). \\(z = 6\\) satisfies both \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\).\n\n\n\nThe probability function maps events onto a real value \\(P\\colon \\mathcal{E} \\subseteq \\mathcal{\\Omega} \\to [0, 1]\\).\n\\(\\operatorname{P}(\\mathcal{E})\\) is the probability associated with event \\(\\mathcal{E}\\).\n\n\n\n\\(\\operatorname{P}(\\mathcal{E}) \\geq 0\\)\n\\(\\operatorname{P}(\\mathcal{\\Omega}) = 1, \\operatorname{P}(\\mathcal{\\emptyset}) = 0\\)\n\\(\\operatorname{P}(\\mathcal{A} \\cup \\mathcal{B}) = \\operatorname{P}(\\mathcal{A}) + \\operatorname{P}(\\mathcal{B}) - \\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B})\\)\n\\(\\operatorname{P}(\\bigcup_{i=1}^{\\infty} \\mathcal{A}_i) = \\sum_{i=1}^{\\infty} \\operatorname{P}(\\mathcal{A}_i), \\quad \\mathcal{A}_i \\cap \\mathcal{A}_j = \\emptyset\\: \\text{for all}\\: i \\neq j\\) (= if all events \\(\\mathcal{A}_i\\) are mutually exclusive)\n\\(\\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B}) = \\operatorname{P}(\\mathcal{A} \\mid \\mathcal{B})\\operatorname{P}(\\mathcal{B})\\)\n\\(\\operatorname{P}(\\mathcal{A} \\cap \\mathcal{B}) = \\operatorname{P}(\\mathcal{A})\\operatorname{P}(\\mathcal{B}) \\iff \\mathcal{A} \\perp \\mathcal{B}\\) (eg. 2 fair dice rolls)\n\\(\\mathcal{A} \\perp \\mathcal{B} \\iff \\operatorname{P}(\\mathcal{A} \\mid \\mathcal{B}) = \\operatorname{P}(\\mathcal{A})\\)\n\n\n\n\n\nA random variable \\(X\\) is a measurable function (mapping) \\(X \\colon \\mathcal{\\Omega} \\to \\mathcal{E}\\) from a sample space \\(\\mathcal{\\Omega}\\) as a set of possible outcomes to a measurable space \\(\\mathcal{E}\\).\nThe probability that \\(X\\) takes on a value in a measurable set \\(\\mathcal{S} \\in \\mathcal{E}\\) is written as \\(\\operatorname{P}(X \\in \\mathcal{S}) = \\operatorname{P}(\\{\\omega \\in \\mathcal{\\Omega} \\mid X(\\omega) \\in \\mathcal{S}\\})\\)\nThe probability that \\(X\\) takes discrete value \\(v\\), denoted \\(X = v\\), is \\(\\operatorname{P}(X=v)\\).\n\\(X = v\\) or \\(X \\geq v\\) are events.\nRandom variables allow us to go from outcomes to values, like \\(X(\\omega) = \\omega\\) the random variable that associates to each die its value (identity function). This is also an example of a discrete random variable.\nWhen \\(X\\) is continuous it doesn’t make sense to have events like \\(X = v\\) (and \\(\\operatorname{P}(X = v) = 0\\)), rather we use \\(v \\leq X \\leq w\\) and probability densities. An example would be the height of a population.\nWe note \\(\\operatorname{P}(X)\\) the probability distribution of X. (Abuse of notation: strictly \\(P_X\\) is the distribution of \\(X\\), but we’ll often just write \\(P(X)\\))\n\n\n\\(\\operatorname{P}(A = a, B = b)\\) is the joint probability of \\(A = a\\) and \\(B = b\\) (it’s the intersection of the events \\(A = a\\) and \\(B = b\\)). Equivalently it’s \\(\\operatorname{P}(\\{A = a\\} \\cap \\{B = b\\})\\), with an overloaded notation, the joint probability distribution becomes \\(\\operatorname{P}(A, B)\\)\nObviously \\[ \\operatorname{P}(A = a, B = b) \\leq \\operatorname{P}(A=a) \\quad \\text{and} \\quad \\operatorname{P}(A = a, B = b) \\leq \\operatorname{P}(B=b) \\]\nAlso, we can marginalize \\[ \\operatorname{P}(A = a) = \\sum_v \\operatorname{P}(A = a, B = v) \\]\nBecause \\(A = a\\) and \\(B = b\\) are events, \\[\\begin{aligned}\n\\operatorname{P}(A = a, B = b) & = \\operatorname{P}(A = a \\mid B = b)\\operatorname{P}(B = b) \\\\\n\\iff \\operatorname{P}(A = a \\mid B = b) & = \\operatorname{P}(A = a, B = b)/\\operatorname{P}(B = b)\n\\end{aligned}\\]\n\n\n\n\nFrom the properties and definitions above, we can derive the following formula\n\\[ \\overbrace{\\operatorname{P}(A \\mid B)}^{\\text{posterior probability}} = \\dfrac{\\overbrace{\\operatorname{P}(B \\mid A)}^{\\text{likelihood}}\\overbrace{\\operatorname{P}(A)}^{\\text{prior}}}{\\underbrace{\\operatorname{P}(B)}_{\\text{observation}}} \\]\n\nprior/hypothesis: our estimate or current belief about the probability of \\(A\\)\nobservation/marginal likelihood/evidence: the evidence or observations we’ve made regarding \\(B\\)\nlikelihood: a measure of how compatible our hypothesis is with our observation\n\nA simplified version is \\(\\operatorname{P}(A \\mid B) \\propto \\operatorname{P}(B \\mid A)\\operatorname{P}(A)\\)\n\n\n\n\n\nThe expectation (or expected value) is the weighted average of the values of \\(X\\).\nDiscrete case:\n\\[ \\operatorname{E}[X] = \\operatorname{E}_{x \\sim P}[X] = \\sum_x x\\operatorname{P}(X=x) \\]\nContinuous case:\n\\[\\operatorname{E}[X] = \\int_{-\\infty}^{\\infty} x f(x) \\;dx \\]\nTo follow mathematical notation, sometimes we use \\(\\mu\\) to denote this average.\n\n\n\nThe variance is a measure of dispersion, it quantifies how much do values vary relative to the expectation (average) on average. The variance is the expectation of the squared difference between the values and the expected value.\n\\[ \\operatorname{Var}(X) = \\operatorname{E}[(X - \\operatorname{E}[X])^2] = \\operatorname{E}[X^2] - (\\operatorname{E}[X])^2 \\]\nBecause\n\\[ \\operatorname{E}[X^2 - 2X\\operatorname{E}[X] + \\operatorname{E}[X]^2] = \\operatorname{E}[X^2] - 2(\\operatorname{E}[X])^2 + (\\operatorname{E}[X])^2 \\]\n\n\n\nBecause the variance is a squared difference, we can take its square root to get the standard deviation which has the benefit of being in the same unit as our random variable.\n\\[ \\operatorname{Var}(X) = \\sigma^2_X \\iff \\sigma_X = \\sqrt{\\operatorname{Var}(X)} \\]\n\n\n\nTODO once I understand it fully enough to explain it."
  },
  {
    "objectID": "notes/probability.html#notes",
    "href": "notes/probability.html#notes",
    "title": "Probability and Information Theory",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "notes/probability.html#proofs",
    "href": "notes/probability.html#proofs",
    "title": "Probability and Information Theory",
    "section": "Proofs",
    "text": "Proofs\nLater!"
  },
  {
    "objectID": "notes/probability.html#algorithms",
    "href": "notes/probability.html#algorithms",
    "title": "Probability and Information Theory",
    "section": "Algorithms",
    "text": "Algorithms"
  },
  {
    "objectID": "notes/probability.html#notation",
    "href": "notes/probability.html#notation",
    "title": "Probability and Information Theory",
    "section": "Notation",
    "text": "Notation\n\n\\(\\mathcal{X}\\): a set\n\\(\\{a, b, c\\}\\): a set, with its elements\n\\(\\emptyset\\): the empty set\n\\(\\mathcal{A} \\subset \\mathcal{B}\\), \\(\\mathcal{A} \\subsetneq \\mathcal{B}\\): \\(\\mathcal{A}\\) is a proper/strict subset of \\(\\mathcal{B}\\)\n\\(\\mathcal{A} \\subseteq \\mathcal{B}\\): \\(\\mathcal{A}\\) is a subest of \\(\\mathcal{B}\\)\n\\(\\mathcal{A} \\cap \\mathcal{B}\\): the intersection of sets \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) — “\\(\\mathcal{A}\\) and \\(\\mathcal{B}\\)”\n\\(\\mathcal{A} \\cup \\mathcal{B}\\): the union of sets \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) — “\\(\\mathcal{A}\\) or \\(\\mathcal{B}\\)”\n\\(\\mathcal{A} \\setminus \\mathcal{B}\\): set subtraction of \\(\\mathcal{B}\\) from \\(\\mathcal{A}\\), elements from \\(\\mathcal{A}\\) but not in \\(\\mathcal{B}\\)\n\\(\\mathcal{S}\\), \\(\\mathcal{\\Omega}\\): the sample space / universe (the set of all possible outcomes)\n\\(|\\mathcal{X}|\\): the cardinality of set \\(\\mathcal{X}\\) (its number of events)\n\\(X\\): a random variable\n\\(P\\): a probability distribution\n\\(X \\sim P\\): the random variable \\(X\\) follows the probability distribution \\(P\\)\n\\(a \\propto b\\): \\(a\\) is proportional to \\(b\\), eg. \\(a = kb\\)\n\\(\\operatorname{P}(\\cdot)\\): the probability function, maps events to their probability and random variables to their probability distributions\n\\(\\operatorname{P}(X)\\): depending on the context, a probability distribution or the probability of any \\(X=x\\), meaning the formula is true for any value\n\\(\\operatorname{P}(X=x)\\): the probability assigned to the event where random variable \\(X\\) takes value \\(x\\)\n\\(\\operatorname{P}(X \\mid Y)\\): the conditional probability distribution of \\(X\\) given \\(Y\\)\n\\(\\operatorname{p}(\\cdot)\\): a probability density function (PDF) associated with distribution \\(P\\)\n\\(\\operatorname{E}[X]\\): expectation of a random variable \\(X\\)\n\\(X \\perp Y\\): random variables \\(X\\) and \\(Y\\) are independent\n\\(X \\perp Y \\mid Z\\): random variables \\(X\\) and \\(Y\\) are conditionally independent given \\(Z\\)\n\\(\\sigma_X\\): standard deviation of random variable \\(X\\)\n\\(\\textrm{Var}(X)\\): variance of random variable \\(X\\), equal to \\(\\sigma^2_X\\)\n\\(\\textrm{Cov}(X, Y)\\): covariance of random variables \\(X\\) and \\(Y\\)\n\\(\\operatorname{\\rho}(X, Y)\\): the Pearson correlation coefficient between \\(X\\) and \\(Y\\), equals \\(\\frac{\\textrm{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\)\n\\(\\operatorname{H}(X)\\): entropy of random variable \\(X\\)\n\\(D_{\\textrm{KL}}(P\\|Q)\\): the KL-divergence (or relative entropy) from distribution \\(Q\\) to distribution \\(P\\)"
  },
  {
    "objectID": "notes/pytorch.html#notes",
    "href": "notes/pytorch.html#notes",
    "title": "PyTorch",
    "section": "Notes",
    "text": "Notes\nLater!"
  },
  {
    "objectID": "notes/pytorch.html#terminology",
    "href": "notes/pytorch.html#terminology",
    "title": "PyTorch",
    "section": "Terminology",
    "text": "Terminology"
  },
  {
    "objectID": "notes/linear-algebra.html",
    "href": "notes/linear-algebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "\\[ \\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i} u_i v_i \\]\n\nu, v = torch.arange(3), torch.arange(3)  # torch.tensor([0, 1, 2])\ntorch.dot(u, v)  # u @ v\n\ntensor(5)\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{u})_{i} = \\mathbf{a}_{i,:} \\cdot \\mathbf{u} = \\sum_{j} a_{i,j} u_j  \\]\n\nA, u = torch.arange(6).reshape(3, 2), torch.arange(2)\nA @ u\n\ntensor([1, 3, 5])\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{B})_{i,j} = \\mathbf{a}_{i,:} \\cdot \\mathbf{b}_{:, j} = \\sum_{k} a_{i,k} b_{k,j} \\]\n\nU, V = torch.arange(6).reshape(3, 2), torch.arange(6).reshape(2, 3)\nU @ V\n\ntensor([[ 3,  4,  5],\n        [ 9, 14, 19],\n        [15, 24, 33]])\n\n\n\n\n\n\\[ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I},\\quad \\mathbf{A} \\in \\mathbb{R}^{n \\times n} \\] Note: Might not exist if \\(\\mathbf{A}\\) is not invertible, that is \\(\\det(\\mathbf{A}) = 0\\)\n\nA = torch.randn(3, 3, dtype=torch.double)\nA_i = A.inverse()\nI = torch.eye(3, dtype=torch.double)\ntorch.allclose(A @ A_i, I), torch.allclose(A_i @ A, I)\n\n(True, True)\n\n\n\n\n\n\n\n\\[ \\det(\\mathbf{A}^\\top) = \\det(\\mathbf{A}) \\] \\[ \\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A})\\det(\\mathbf{B}) \\]\n\n\n\n\\[ A = \\operatorname{diag}(a_1, \\dots, a_n), \\quad \\det(A) = \\prod_{i=1}^n a_i \\]\n\n# using I as a mask\nI = torch.eye(3, dtype=torch.float32)\nA = torch.arange(1, 10, dtype=torch.float32).reshape(3, 3) * I  # A = diag(1, 5, 9)\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\\[ \\det(T) = \\prod_{i} t_{ii}, \\quad \\text{where $T$ is triangular} \\]\n\nA = torch.triu(torch.arange(1, 10, dtype=torch.float32).reshape(3, 3))\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}\\: (= \\mathbf{A}\\mathbf{A}^\\top) \\]\nInterstingly,\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^{-1} = \\mathbf{A}^\\top \\]"
  },
  {
    "objectID": "notes/linear-algebra.html#definitions-formulas",
    "href": "notes/linear-algebra.html#definitions-formulas",
    "title": "Linear Algebra",
    "section": "",
    "text": "\\[ \\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i} u_i v_i \\]\n\nu, v = torch.arange(3), torch.arange(3)  # torch.tensor([0, 1, 2])\ntorch.dot(u, v)  # u @ v\n\ntensor(5)\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{u})_{i} = \\mathbf{a}_{i,:} \\cdot \\mathbf{u} = \\sum_{j} a_{i,j} u_j  \\]\n\nA, u = torch.arange(6).reshape(3, 2), torch.arange(2)\nA @ u\n\ntensor([1, 3, 5])\n\n\n\n\n\n\\[ (\\mathbf{A}\\mathbf{B})_{i,j} = \\mathbf{a}_{i,:} \\cdot \\mathbf{b}_{:, j} = \\sum_{k} a_{i,k} b_{k,j} \\]\n\nU, V = torch.arange(6).reshape(3, 2), torch.arange(6).reshape(2, 3)\nU @ V\n\ntensor([[ 3,  4,  5],\n        [ 9, 14, 19],\n        [15, 24, 33]])\n\n\n\n\n\n\\[ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I},\\quad \\mathbf{A} \\in \\mathbb{R}^{n \\times n} \\] Note: Might not exist if \\(\\mathbf{A}\\) is not invertible, that is \\(\\det(\\mathbf{A}) = 0\\)\n\nA = torch.randn(3, 3, dtype=torch.double)\nA_i = A.inverse()\nI = torch.eye(3, dtype=torch.double)\ntorch.allclose(A @ A_i, I), torch.allclose(A_i @ A, I)\n\n(True, True)\n\n\n\n\n\n\n\n\\[ \\det(\\mathbf{A}^\\top) = \\det(\\mathbf{A}) \\] \\[ \\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A})\\det(\\mathbf{B}) \\]\n\n\n\n\\[ A = \\operatorname{diag}(a_1, \\dots, a_n), \\quad \\det(A) = \\prod_{i=1}^n a_i \\]\n\n# using I as a mask\nI = torch.eye(3, dtype=torch.float32)\nA = torch.arange(1, 10, dtype=torch.float32).reshape(3, 3) * I  # A = diag(1, 5, 9)\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\\[ \\det(T) = \\prod_{i} t_{ii}, \\quad \\text{where $T$ is triangular} \\]\n\nA = torch.triu(torch.arange(1, 10, dtype=torch.float32).reshape(3, 3))\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n\n(tensor(45.), tensor(True))\n\n\n\n\n\n\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}\\: (= \\mathbf{A}\\mathbf{A}^\\top) \\]\nInterstingly,\n\\[ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^{-1} = \\mathbf{A}^\\top \\]"
  },
  {
    "objectID": "notes/linear-algebra.html#notes",
    "href": "notes/linear-algebra.html#notes",
    "title": "Linear Algebra",
    "section": "Notes",
    "text": "Notes\n\nVectors\nIn linear algebra, vectors are column vectors by default, so \\[ \\mathbf{u} \\in \\mathbb{R}^{n \\times 1}, \\quad \\mathbf{u} = \\begin{bmatrix} u_1 \\\\ \\vdots \\\\ u_n \\end{bmatrix} \\]\nIt follows that when doing a Matrix-Vector product, the matrix is on the left \\(\\mathbf{A}\\mathbf{u}\\).\n\n\nOrder of operations\nSimilarly, when we say “we apply a linear transformation \\(\\mathbf{A}\\) to \\(\\mathbf{B}\\)”, we mean \\(\\mathbf{A}\\mathbf{B}\\).\n\n\nDeterminant\nI think of the determinant of \\(\\mathbf{A}\\) as the scaling factor of the linear transformation represented by \\(\\mathbf{A}\\).\nThis explains why a matrix whose determinant is 0 is not invertible: it “collapses”, and two images might have the same original input \\(\\mathbf{x}\\)"
  },
  {
    "objectID": "notes/linear-algebra.html#proofs",
    "href": "notes/linear-algebra.html#proofs",
    "title": "Linear Algebra",
    "section": "Proofs",
    "text": "Proofs\nLater!"
  },
  {
    "objectID": "notes/linear-algebra.html#algorithms",
    "href": "notes/linear-algebra.html#algorithms",
    "title": "Linear Algebra",
    "section": "Algorithms",
    "text": "Algorithms\n\nGaussian Elimination\nGaussian elimination — or row reduction – is an algorithm for solving systems of linear equations, based on the following operations:\n\nScaling a row\nSwapping two rows\nAdding a multiple of one row to another\n\n\n\nLU decomposition (or LU factorization)\nComputing the determinant of a Matrix is not trivial at first glance.\nBut consider the following facts:\n\nwe know how to easily compute the determinant of a triangular matrix\nwe know how to easily compute the determinant of a product of matrices\n\nKnowing that, we want to find a representation of our original matrix \\(\\mathbf{A}\\) that involves an Upper Triangular Matrix \\(\\mathbf{U}\\), and one or more other matrices whose determinant is known or trivial to compute, as \\(\\mathbf{P}\\mathbf{A} = \\mathbf{L}\\mathbf{U}\\)\nTo go from \\(\\mathbf{A}\\) to \\(\\mathbf{U}\\) we’ll use Gaussian Elimination, \\(\\mathbf{P}\\) tracks our permutations (row swaps) and \\(\\mathbf{L}\\) tracks our row operations (row additions).\nNow, because \\(\\mathbf{P}\\) is orthogonal (yes, since its the identity matrix with row swaps, when performing \\(\\mathbf{P}^\\top\\mathbf{P}\\), the ones in the rows meet the ones in the columns at the diagonal, zeros everywhere else, so we get \\(\\mathbf{P}^\\top\\mathbf{P} = \\mathbf{I}\\)), we then have \\[ \\mathbf{P}\\mathbf{A} = \\mathbf{L}\\mathbf{U} \\implies \\mathbf{A} = \\mathbf{P}^{-1}\\mathbf{L}\\mathbf{U} = \\mathbf{P}^\\top\\mathbf{L}\\mathbf{U} \\]\nFinally, this means that \\[ \\det(\\mathbf{A}) = \\det(\\mathbf{P}^\\top) \\cdot \\det(\\mathbf{L}) \\cdot \\det(\\mathbf{U}) = \\det(\\mathbf{P}) \\cdot \\det(\\mathbf{L}) \\cdot \\det(\\mathbf{U}) \\]\nNow I’m not gonna prove that, but:\n\n\\(\\det(\\mathbf{P}) = (-1)^{\\#swaps}\\)\n\\(\\det(\\mathbf{L}) = 1\\) (because when adding rows, we never modify the original identity’s diagonal, so the diagonal is full of ones, and since L is a Lower Triangular Matrix, the determinant is 1)\n\nNow, if we just keep track of row swaps, we can easily compute \\(\\det(\\mathbf{A})\\)!"
  },
  {
    "objectID": "notes/linear-algebra.html#notation",
    "href": "notes/linear-algebra.html#notation",
    "title": "Linear Algebra",
    "section": "Notation",
    "text": "Notation\n\n\\(x\\): a scalar\n\\(\\mathbf{x}\\): a vector\n\\(\\mathbf{X}\\): a matrix\n\\(x_i\\): the \\(i^\\textrm{th}\\) element of vector \\(\\mathbf{x}\\)\n\\(x_{i,j}\\): the element of matrix \\(\\mathbf{X}\\) at row \\(i\\) and column \\(j\\)\n\\(\\mathbf{x}_{i, :}\\): the \\(i^\\textrm{th}\\) row-vector of \\(\\mathbf{X}\\)\n\\(\\mathbf{x}_{:,j}\\): the \\(j^\\textrm{th}\\) column-vector of \\(\\mathbf{X}\\)\n\\(\\operatorname{diag}(a_1, \\dots, a_n)\\): a diagonal matrix\n\\(\\mathbf{I}\\): the indentity matrix\n\\((\\cdot)^\\top\\): the transpose of a vector or matrix\n\\(\\mathbf{A}^{-1}\\): the inverse of a matrix"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "rss\n\n\n\n\n\n\n\n\n\nDistributed Training\n\n\n\nmle\n\npython\n\n\n\nDistributed training study notes and algorithms.\n\n\n\n\n\nSep 5, 2025\n\n\n\n\n\n\n\nProbability and Information Theory\n\n\n\nmath\n\n\n\nSmall Probability & Information Theory cheatsheet.\n\n\n\n\n\nSep 4, 2025\n\n\n\n\n\n\n\nPyTorch\n\n\n\npython\n\nmle\n\n\n\ntorch cheatsheet, common methods or tricks.\n\n\n\n\n\nSep 4, 2025\n\n\n\n\n\n\n\nCalculus\n\n\n\nmath\n\n\n\nAll things calculus, differential or integral.\n\n\n\n\n\nSep 4, 2025\n\n\n\n\n\n\n\nLinear Algebra\n\n\n\nmath\n\n\n\nAll things Linear Algebra, matrices, vectors etc.\n\n\n\n\n\nSep 4, 2025\n\n\n\n\n\n\n\nOn Typescript\n\n\n\ntypescript\n\n\n\nA few notes on Typescript for my future self.\n\n\n\n\n\nFeb 15, 2025\n\n\n\n\n\nNo matching items"
  }
]