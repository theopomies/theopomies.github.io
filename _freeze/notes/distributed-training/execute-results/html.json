{
  "hash": "3078be4d78cb1853b5bdb3eba2eb95d1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Distributed Training\nauthor: Theo POMIES\ndate: 2025-09-02\ndate-modified: 2025-09-05\ndescription: Distributed training study notes and algorithms.\ncategories: [mle, python]\n---\n\n\n\n## Functions / Methods\n### Basics (Point-Point)\n`send` and `recv` to send or receive a tensor synchronously — from/to a single rank.\n\nAnd their async counterparts, `isend` and `irecv`.\n\n::: {#e4eb10dd .cell execution_count=2}\n``` {.python .cell-code}\nrank = dist.get_rank()\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\nif rank == 0:\n    request = dist.isend(tensor, 1)\n    ...\n    # can do something else, like more sends for example!\n    ...\n    request.wait() # now block until it's been fulfilled\nelif rank == 1:\n    dist.recv(tensor, 0) # recv is synchronous, so it will block until tensor is fully received\n\nprint(f\"After: {tensor}\")\n```\n:::\n\n\n::: {#ee8dfa26 .cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n```\n:::\n:::\n\n\n### Collective Operations\n\nCollective operations allow communication (data-transfer) from All->Point, Point->All and All->All.\n\n#### Point->All\n##### Broadcast\n\nBroadcast (`torch.distributed.broadcast(tensor, src, ...)`) allows a rank to _broadcast_ a tensor to the whole group.\n\n::: {#fb5512be .cell execution_count=4}\n``` {.python .cell-code}\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\ndist.broadcast(tensor, src=0)\n\nprint(f\"After: {tensor})\n```\n:::\n\n\n::: {#c4899e9d .cell execution_count=5}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 2 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n```\n:::\n:::\n\n\n##### Scatter\n\nScatter (`torch.distributed.scatter(tensor, scatter_list, src, ...)`) allows us to _scatter_ — split and broadcast different chunks of — a tensor from a rank to the whole group.\n\n::: {#6f436cc1 .cell execution_count=6}\n``` {.python .cell-code}\ntensor = torch.zeros(3)\nscatter_list = [torch.arange(3 * i, 3 * i + 3) if rank == 0 else torch.zeros(3) for i in range(world_size)]\n\nprint(f\"Scatter list: {scatter_list}\")\nprint(f\"Before: {tensor}\")\n\ndist.scatter(tensor, scatter_list, src=0)\n\nprint(f\"After: {tensor}\")\n```\n:::\n\n\n::: {#72e1fbc0 .cell execution_count=7}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 0 ==========\nScatter list: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nScatter list: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([3, 4, 5])\n```\n:::\n:::\n\n\n#### All->Point\n##### Reduce\n\nReduce (`torch.distributed.reduce(tensor, dst, op, ...)`) performs a reduction operation (N->1, eg. sum, max, min, prod, ...) and the `dst` rank receives the result.\n\n::: {#553fde99 .cell execution_count=8}\n``` {.python .cell-code}\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n```\n:::\n\n\n::: {#c5d6bca7 .cell execution_count=9}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 4, 5])\n```\n:::\n:::\n\n\n##### Gather\n\nGather (`torch.distributed.gather(tensor, gather_list, dst, ...)`) _gathers_ — pulls — a tensor, of the same size, from every rank and stores them in a list in a single rank.\n\n::: {#a34d2c3a .cell execution_count=10}\n``` {.python .cell-code}\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.gather(tensor, gather_list, dst=0)\n\nprint(f\"After: {gather_list}\")\n```\n:::\n\n\n::: {#9a48ba75 .cell execution_count=11}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([3, 4, 5])\n\n========== rank 1 ==========\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nAfter: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\n```\n:::\n:::\n\n\n#### All->All\n##### All-Reduce\n\nAll-Reduce (`torch.distributed.all_reduce(tensor, op, ...)`) performs a _reduction_ operation, like `reduce`, but every rank receives the result — rather than a single one with `reduce`. Think of it as `reduce` + `broadcast` — though it is optimized by techniques like ring-reduce.\n\n::: {#5f784e01 .cell execution_count=12}\n``` {.python .cell-code}\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n```\n:::\n\n\n::: {#1da72472 .cell execution_count=13}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 5, 7])\n```\n:::\n:::\n\n\n##### All-Gather\n\nAll-Gather (`torch.distributed.all_gather(tensor, gather_list, ...)`) _gathers_ — pulls — a tensor, of the same size, from every rank and stores them in a list in _every_ rank. Think of it as running `gather` on all ranks.\n\n::: {#e49bfd43 .cell execution_count=14}\n``` {.python .cell-code}\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.all_gather(tensor, gather_list)\n\nprint(f\"After: {gather_list}\")\n```\n:::\n\n\n::: {#c37b2682 .cell execution_count=15}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 2 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n```\n:::\n:::\n\n\n##### Reduce-Scatter\n\nReduce-Scatter (`torch.distributed.reduce_scatter(output_tensor, input_list, op, ...)`) performs a _reduction_ operation — like other `reduce` functions — and _scatters_ the resulting tensor. Think of it like `reduce` + `scatter`. Note: it needs `len(input_list) == world_size` and every tensor in `input_list` to have the same shape of `output_tensor`.\n\n::: {#d009b716 .cell execution_count=16}\n``` {.python .cell-code}\ntensor = torch.zeros(2)\nscatter_list = [torch.tensor([(rank + 1) * i for i in range(1, 3)]) ** (j + 1) for j in range(3)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {scatter_list}\")\n\ndist.reduce_scatter(tensor, scatter_list, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n```\n:::\n\n\n::: {#9590e441 .cell execution_count=17}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 0 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([1, 2]), tensor([1, 4]), tensor([1, 8])]\nAfter: tensor([ 6, 12])\n\n========== rank 1 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([2, 4]), tensor([ 4, 16]), tensor([ 8, 64])]\nAfter: tensor([14, 56])\n\n========== rank 2 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([3, 6]), tensor([ 9, 36]), tensor([ 27, 216])]\nAfter: tensor([ 36, 288])\n```\n:::\n:::\n\n\n#### Other\n##### Barrier\n\nBarrier (`torch.distributed.barrier(...)`) synchronizes all ranks (processes), waiting for all of them to reach the barrier. (think of `.join()` for threads or processes)\n\n## Algorithms / Techniques\n### DDP — Distributed Data Parallelism\n\n### Data Sharding\n\n## Notes\n\n## Terminology\n\n* device: Hardware unit — GPU `\"cuda:0\"`, CPU `\"cpu\"` etc. _that's where tensors and computations live_\n* node: Phyisical machine/server (or VPS whatever) that has 1+ devices\n* process: Python process/worker, executing a copy of the code/script — often on a single device (GPU)\n* rank: ID of a process — often that maps to a single device. _rank_ without qualifiers is _global rank_\n* world: Set of all processes part of our current distributed job\n* global rank, world rank: rank across all processes/nodes\n* local rank: rank within a single node (**node** _not_ group)\n* group: subset of processes (1+ nodes) that we've grouped for sub-communications. **note**: we still use _global_ rank for intra-group communication.\n\n",
    "supporting": [
      "distributed-training_files"
    ],
    "filters": [],
    "includes": {}
  }
}