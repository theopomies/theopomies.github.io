{
  "hash": "dd5b978c2180579282a82037bd80e9b4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Distributed Training\nauthor: Theo POMIES\ndate: 2025-09-02\ndate-modified: 2025-09-17\ndescription: Distributed training study notes and algorithms.\ncategories: [mle, python]\n---\n\n<!-- TODO: DiLoCo, Streaming DiLoCo -->\n\n\n\n## Functions / Methods\n### Basics (Point-Point)\n`send` and `recv` to send or receive a tensor synchronously — from/to a single rank.\n\nAnd their async counterparts, `isend` and `irecv`.\n\n::: {#3f2754a1 .cell execution_count=2}\n``` {.python .cell-code}\nrank = dist.get_rank()\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\nif rank == 0:\n    request = dist.isend(tensor, 1)\n    ...\n    # can do something else, like more sends for example!\n    ...\n    request.wait() # now block until it's been fulfilled\nelif rank == 1:\n    dist.recv(tensor, 0) # recv is synchronous, so it will block until tensor is fully received\n\nprint(f\"After: {tensor}\")\n```\n:::\n\n\n::: {#6b1d11d0 .cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n```\n:::\n:::\n\n\n### Collective Operations\n\nCollective operations allow communication (data-transfer) from All->Point, Point->All and All->All.\n\n#### Point->All\n##### Broadcast\n\nBroadcast (`torch.distributed.broadcast(tensor, src, ...)`) allows a rank to _broadcast_ a tensor to the whole group.\n\n::: {#40389502 .cell execution_count=4}\n``` {.python .cell-code}\ntensor = torch.arange(3) if rank == 0 else torch.zeros(3)\n\nprint(f\"Before: {tensor}\")\n\ndist.broadcast(tensor, src=0)\n\nprint(f\"After: {tensor})\n```\n:::\n\n\n::: {#880c6513 .cell execution_count=5}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 2 ==========\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n```\n:::\n:::\n\n\n##### Scatter\n\nScatter (`torch.distributed.scatter(tensor, scatter_list, src, ...)`) allows us to _scatter_ — split and broadcast different chunks of — a tensor from a rank to the whole group.\n\n::: {#d191e8e8 .cell execution_count=6}\n``` {.python .cell-code}\ntensor = torch.zeros(3)\nscatter_list = [torch.arange(3 * i, 3 * i + 3) if rank == 0 else torch.zeros(3) for i in range(world_size)]\n\nprint(f\"Scatter list: {scatter_list}\")\nprint(f\"Before: {tensor}\")\n\ndist.scatter(tensor, scatter_list, src=0)\n\nprint(f\"After: {tensor}\")\n```\n:::\n\n\n::: {#7e1bb653 .cell execution_count=7}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 0 ==========\nScatter list: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([0, 1, 2])\n\n========== rank 1 ==========\nScatter list: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([0., 0., 0.])\nAfter: tensor([3, 4, 5])\n```\n:::\n:::\n\n\n#### All->Point\n##### Reduce\n\nReduce (`torch.distributed.reduce(tensor, dst, op, ...)`) performs a reduction operation (N->1, eg. sum, max, min, prod, ...) and the `dst` rank receives the result.\n\n::: {#569607fe .cell execution_count=8}\n``` {.python .cell-code}\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n```\n:::\n\n\n::: {#d6f3a09a .cell execution_count=9}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 4, 5])\n```\n:::\n:::\n\n\n##### Gather\n\nGather (`torch.distributed.gather(tensor, gather_list, dst, ...)`) _gathers_ — pulls — a tensor, of the same size, from every rank and stores them in a list in a single rank.\n\n::: {#b4e60d0c .cell execution_count=10}\n``` {.python .cell-code}\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.gather(tensor, gather_list, dst=0)\n\nprint(f\"After: {gather_list}\")\n```\n:::\n\n\n::: {#6eb516c4 .cell execution_count=11}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nBefore: tensor([3, 4, 5])\n\n========== rank 1 ==========\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\nAfter: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\n```\n:::\n:::\n\n\n#### All->All\n##### All-Reduce\n\nAll-Reduce (`torch.distributed.all_reduce(tensor, op, ...)`) performs a _reduction_ operation, like `reduce`, but every rank receives the result — rather than a single one with `reduce`. Think of it as `reduce` + `broadcast` — though it is optimized by techniques like ring-reduce.\n\n::: {#6b58596c .cell execution_count=12}\n``` {.python .cell-code}\ntensor = torch.arange(3) + rank * 3\n\nprint(f\"Before: {tensor}\")\n\ndist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n```\n:::\n\n\n::: {#fb107516 .cell execution_count=13}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 0 ==========\nBefore: tensor([0, 1, 2])\nAfter: tensor([3, 5, 7])\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nAfter: tensor([3, 5, 7])\n```\n:::\n:::\n\n\n##### All-Gather\n\nAll-Gather (`torch.distributed.all_gather(tensor, gather_list, ...)`) _gathers_ — pulls — a tensor, of the same size, from every rank and stores them in a list in _every_ rank. Think of it as running `gather` on all ranks.\n\n::: {#142dc5ac .cell execution_count=14}\n``` {.python .cell-code}\ntensor = torch.arange(3) + rank * 3\ngather_list = [torch.zeros(3) for _ in range(world_size)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {gather_list}\")\n\ndist.all_gather(tensor, gather_list)\n\nprint(f\"After: {gather_list}\")\n```\n:::\n\n\n::: {#1ca801b7 .cell execution_count=15}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 2 ==========\nBefore: tensor([0, 1, 2])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n\n========== rank 1 ==========\nBefore: tensor([3, 4, 5])\nBefore: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]\nAfter: [tensor([0, 1, 2]), tensor([3, 4, 5])]\n```\n:::\n:::\n\n\n##### Reduce-Scatter\n\nReduce-Scatter (`torch.distributed.reduce_scatter(output_tensor, input_list, op, ...)`) performs a _reduction_ operation — like other `reduce` functions — and _scatters_ the resulting tensor. Think of it like `reduce` + `scatter`.\n\n:::{.callout-note}\nIt needs `len(input_list) == world_size` and every tensor in `input_list` to have the same shape of `output_tensor`.\n:::\n\n::: {#e65ffe56 .cell execution_count=16}\n``` {.python .cell-code}\ntensor = torch.zeros(2)\nscatter_list = [torch.tensor([(rank + 1) * i for i in range(1, 3)]) ** (j + 1) for j in range(3)]\n\nprint(f\"Before: {tensor}\")\nprint(f\"Before: {scatter_list}\")\n\ndist.reduce_scatter(tensor, scatter_list, op=dist.ReduceOp.SUM)\n\nprint(f\"After: {tensor}\")\n```\n:::\n\n\n::: {#08b1f7ef .cell execution_count=17}\n\n::: {.cell-output .cell-output-stdout}\n```\n========== rank 0 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([1, 2]), tensor([1, 4]), tensor([1, 8])]\nAfter: tensor([ 6, 12])\n\n========== rank 1 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([2, 4]), tensor([ 4, 16]), tensor([ 8, 64])]\nAfter: tensor([14, 56])\n\n========== rank 2 ==========\nBefore: tensor([0., 0.])\nBefore: [tensor([3, 6]), tensor([ 9, 36]), tensor([ 27, 216])]\nAfter: tensor([ 36, 288])\n```\n:::\n:::\n\n\n#### Other\n##### Barrier\n\nBarrier (`torch.distributed.barrier(...)`) synchronizes all ranks (processes), waiting for all of them to reach the barrier. (think of `.join()` for threads or processes)\n\n## Algorithms / Techniques\n\n### Data Sharding\n\nData Sharding is the process of sharding — splitting — the dataset / dataloader so that each rank only pulls their own unique mini-batches of the training data. This avoids duplicates and is more commucation / memory efficient that duplicating the same full dataset on every rank. To do this with torch, setup the `DataLoader` with `sampler=[instance of DistributedSampler]`.\n\n### Types of parallelism\n\nThe goal of parallelism is to maximize throughput and cluster utilization.\n\n<!--TODO: gradient accumulation somewhere -->\n<!--TODO: activation checkpointing -->\n<!--TODO: PP/TP/EP sections + link to journal 09-23 and 09-24 -->\n* **[Data Parallelism @sec-data-parallelism]** (**DP**): Each rank has a replica of the model — they're **replicants** — and receives a different mini-batch. After optional [Gradient Accumulation] , gradients are averaged across ranks (`all_reduce`).\n* **Pipeline Parallelism** (**PP**): The model is split along the layers. Each rank has 1+ consecutive layers of the model, and we orchestrate sequential forward/backward passes along the ranks. This is **inter-layer** parallelism.\n* **Tensor Parallelism** (**TP**): The model's layers themselves are split across ranks. We need more complex orchestration since a single tensor's values are scattered across different ranks. This is **intra-layer** parallelism.\n* **Expert Parallelism** (**EP**): A specific type of **TP** where we only split the experts of an **MoE** across ranks.\n\n::: {.callout-important}\n**ZeRO/FSDP is not a parallelism strategy in the strict sense, but a memory-optimization strategy.** It's a highly memory-efficient form DP.\n\n* **Parallelism** = distributing *computation* to increase throughput.\n* **Memory optimization** (eg. ZeRO/FSDP) = sharding model states (parameters, gradients, optimizer states) across ranks so the model fits in memory, while each rank still computes the full forward and backward pass.\n\nThus:\n\n* With **ZeRO/FSDP**, every rank executes the full network computation but stores only a shard of the model states.\n* With **TP/EP/PP**, computation itself is partitioned across ranks, and the combined work reconstructs the whole forward/backward pass.\n\nThese approaches are complementary and usually combined in large-scale training.\n:::\n\n### DDP — Distributed Data Parallelism {#sec-data-parallelism}\n\nDistributed Data Parallelism is a type of parallelism where each _rank_ loads a copy — replica — of the model, after each optimizer step they always all have the same parameters, they are **replicants**. Each rank then trains on a _different_ mini-batch (hence the importance of [data sharding](/notes/distributed-training.qmd#data-sharding)). We then average the gradients (`all_reduce` sum + division by world_size or avg operation if available), perform a step of gradient descent, rinse and repeat. If we _can_ use this, we should, it has the least amount of overhead, but it requires that the model + optimizer states all fit in the device's VRAM.\n\n:::{.callout-note}\nThe difference between DDP and DP is that DDP uses processes to avoid the GIL and DP uses threads. Do not use DP, only DDP.\n:::\n\n```python\nclass SimpleDataParaellism():\n    def __init__(self, model):\n        self.model = model\n\n        for param in model.parameters():\n            rank_0_params = param.data.clone()\n            dist.broadcast(rank_0_params, src=0)\n            assert torch.equal(param.data, rank_0_params), \"Parameters mismatch at initialization\"\n\n    def sync_grad(self):\n        for param in model.parameters():\n            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG) # only available on NCCL backend\n            # eq.\n            # dist.all_reduce(param.grad, op=dist.ReduceOp.SUM)\n            # param.grad /= dist.get_world_size()\n```\n\n:::{.callout-note}\nThe above is a Toy implementation, in reality you do **not** waste time and resources by doing a single `all_reduce` at the end. This leaves GPUs idle. You interleave computations and communications\n:::\n\n### ZeRO / FSDP {#sec-zero-fsdp}\n\n**Ze**ro **R**edudency **O**ptimizer (ZeRO) by DeepSpeed is a modeling strategy involving sharding states and parameters during training as a mean of optimizing peak memory. The core idea is that the optimizer states, gradients and/or model parameters are sharded, retrieved only when necessary for some computation, then anything we do not use anymore is discarded.\n\n**F**ully **S**harded **D**ata **P**arallelism (FSDP) is PyTorch's implementation of ZeRo.\n\n[Paper](https://huggingface.co/papers/1910.02054){target=_blank} [Article](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/){target=_blank} [FSDP Paper](https://huggingface.co/papers/2304.11277){target=_blank} [FSDP Doc](https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html){target=_blank}\n\n#### ZeRO-1\n\nZeRO stage 1 (aka. $P_{os}$) is the sharding/partitioning of **optimizer states** only. 4x memory reduction, communication volume of the same order as DP (gradient all-reduce dominates).\n\n**Forward pass**\n\n* Same as DP: each rank stores the full model parameters and runs the full forward pass.\n\n**Backward pass**\n\n* Same as DP: each rank computes all gradients locally.\n* Same as DP: gradients are averaged across ranks via `all_reduce`.\n\n:::{.callout-note}\nCan be a `reduce_scatter` too\n:::\n\n**Optimizer step**\n\n* Each rank holds the full parameters and full averaged gradients.\n* Each rank updates **only the parameter shard** corresponding to its shard of the optimizer state.\n* Updated parameter shards are then exchanged (`all_gather`) so all ranks end up with the full updated model.\n\n#### ZeRO-2\n\nZeRO stage 2 (aka. $P_{os} + P_g$) is the sharding/partitioning of optimizer states **and** **gradient**. 8x memory reduction, communication volume of the same order as DP and ZeRO-1.\n\n**Forward pass**\n\n* Same as DP: each rank stores the full model parameters and runs the full forward pass.\n\n**Backward pass**\n\n* Each rank computes gradients locally, so gradients are temporarily materialized on every rank. This means ZeRO-2 has the same _peak_ memory as ZeRO-1, but 8x lower _persistent_ memory.\n* Gradients are averaged and sharded across ranks (`reduce_scatter`) — think averaging + sending to each rank the shard of the gradients that corresponds **exactly** to its optimizer state\n\n**Optimizer step**\n\n* Each rank holds the full parameters.\n* Each rank holds **only the averaged gradients** corresponding to its shard of the optimizer state.\n* Each rank updates **only the parameter shard** corresponding to its shard of the optimizer state.\n* Updated parameter shards are then exchanged (`all_gather`) so all ranks end up with the full updated model.\n\n#### ZeRO-3\n\nZeRO stage 3 (aka. $P_{os} + P_g + P_p$) is the sharding/partitioning of optimizer states and gradient **and** **model parameters**. Memory reduction scales linearly with our parallelism degree, larger communication overhead (≈50% more than DP/ZeRO-1/2) — (need to `all_gather` and `reduce_scatter` parameters before and after every computation requiring them).\n\n(Assuming FP16 params and FP32 optimizer states)\n\n**Forward pass**\n\n* Each rank stores its shard of the model parameters.\n* Whenever a parameter is needed for computation, it is materialized (`all_gather` from its shard)\n* The computation is done\n* The local param is released/flushed (`del`/`=None`) on every rank but the one owning it\n\n**Backward pass**\n\n* Each rank runs the backward pass for its full model replica, but parameters must be all-gathered on demand.\n* Gradients are produced during backprop, then immediately reduce-scattered so only the owning rank keeps the shard.\n\n**Optimizer step**\n\n* Each rank holds **only the parameters' shard** corresponding to its shard of the gradients and optimizer state.\n* Each rank holds **only the averaged gradients' shard** corresponding to its shard of the parameters and optimizer state.\n* Each rank updates **only the parameter shard** corresponding to its shard of the optimizer state.\n* Updated parameter shards are then exchanged (`all_gather`) so all ranks end up with the full updated model.\n\n## Terminology\n\n* **device**: Hardware unit — GPU `\"cuda:0\"`, CPU `\"cpu\"` etc. **that's where tensors and computations live**\n* **node**: Phyisical machine/server (or VPS whatever) that has 1+ devices\n* **process**: Python process/worker, executing a copy of the code/script — often on a single device (GPU)\n* **rank**: ID of a process — often that maps to a single device. **rank** without qualifiers is **global rank**\n* **world**: Set of all processes part of our current distributed job\n* **global** rank, world rank: rank across all processes/nodes. **note**: collective operations take the **global rank** (or just **rank**) as input for `src`/`dst`\n* **local rank**: rank within a single node (**node** _not_ group). **note**: `device` takes the **local rank** `\"cuda:{local_rank}\"`\n* **group**: subset of processes (1+ nodes) that we've grouped for sub-communications. **note**: we still use **global rank** for intra-group communication.\n\n## Resources / References / Bookmarks\n* [HF UltraScale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook){target=_blank}\n* [torch.distributed doc](https://docs.pytorch.org/docs/stable/distributed.html){target=_blank}, RTFM\n\n",
    "supporting": [
      "distributed-training_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}