{
  "hash": "c601314aea3cfde5268e1d82a475922d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Linear Algebra \nauthor: Theo POMIES\ndate: 2025-08-29\ndate-modified: 2025-09-04\ndescription: Notes, formulas and proofs for me to reference later. This is an evolving document, so it might at times be incomplete\ncategories: [math]\n---\n\n\n\n## Formulas\n\n### Dot Product\n$$ \\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i} u_i v_i $$\n\n::: {#d77726cb .cell execution_count=2}\n``` {.python .cell-code}\nu, v = torch.arange(3), torch.arange(3)  # torch.tensor([0, 1, 2])\ntorch.dot(u, v)  # u @ v\n```\n\n::: {.cell-output .cell-output-display execution_count=90}\n```\ntensor(5)\n```\n:::\n:::\n\n\n### Matrix-Vector Product\n$$ (\\mathbf{A}\\mathbf{u})_{i} = \\mathbf{a}_{i,:} \\cdot \\mathbf{u} = \\sum_{j} a_{i,j} u_j  $$\n\n::: {#11c937bd .cell execution_count=3}\n``` {.python .cell-code}\nA, u = torch.arange(6).reshape(3, 2), torch.arange(2)\nA @ u\n```\n\n::: {.cell-output .cell-output-display execution_count=91}\n```\ntensor([1, 3, 5])\n```\n:::\n:::\n\n\n### Matrix Multiplication\n$$ (\\mathbf{A}\\mathbf{B})_{i,j} = \\mathbf{a}_{i,:} \\cdot \\mathbf{b}_{:, j} = \\sum_{k} a_{i,k} b_{k,j} $$\n\n::: {#883904bc .cell execution_count=4}\n``` {.python .cell-code}\nU, V = torch.arange(6).reshape(3, 2), torch.arange(6).reshape(2, 3)\nU @ V\n```\n\n::: {.cell-output .cell-output-display execution_count=92}\n```\ntensor([[ 3,  4,  5],\n        [ 9, 14, 19],\n        [15, 24, 33]])\n```\n:::\n:::\n\n\n### Inverse of a Square Matrix\n$$ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I},\\quad \\mathbf{A} \\in \\mathbb{R}^{n \\times n} $$\nNote: Might not exist if $\\mathbf{A}$ is not invertible, that is $\\det(\\mathbf{A}) = 0$\n\n::: {#bfc6d1c4 .cell execution_count=5}\n``` {.python .cell-code}\nA = torch.randn(3, 3, dtype=torch.double)\nA_i = A.inverse()\nI = torch.eye(3, dtype=torch.double)\ntorch.allclose(A @ A_i, I), torch.allclose(A_i @ A, I)\n```\n\n::: {.cell-output .cell-output-display execution_count=93}\n```\n(True, True)\n```\n:::\n:::\n\n\n### Determinant of a Matrix\n#### Basic identities\n$$ \\det(\\mathbf{A}^\\top) = \\det(\\mathbf{A}) $$\n$$ \\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A})\\det(\\mathbf{B}) $$\n\n#### Determinant of Diagonal Matrix\n$$ A = \\operatorname{diag}(a_1, \\dots, a_n), \\quad \\det(A) = \\prod_{i=1}^n a_i $$\n\n::: {#258847f5 .cell execution_count=6}\n``` {.python .cell-code}\n# using I as a mask\nI = torch.eye(3, dtype=torch.float32)\nA = torch.arange(1, 10, dtype=torch.float32).reshape(3, 3) * I  # A = diag(1, 5, 9)\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n```\n\n::: {.cell-output .cell-output-display execution_count=94}\n```\n(tensor(45.), tensor(True))\n```\n:::\n:::\n\n\n#### Determinant of a Triangular Matrix\n$$ \\det(T) = \\prod_{i} t_{ii}, \\quad \\text{where $T$ is triangular} $$\n\n::: {#0f70e461 .cell execution_count=7}\n``` {.python .cell-code}\nA = torch.triu(torch.arange(1, 10, dtype=torch.float32).reshape(3, 3))\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n```\n\n::: {.cell-output .cell-output-display execution_count=95}\n```\n(tensor(45.), tensor(True))\n```\n:::\n:::\n\n\n### Orthogonality\n$$ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}\\: (= \\mathbf{A}\\mathbf{A}^\\top) $$\n\nInterstingly,\n\n$$ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^{-1} = \\mathbf{A}^\\top $$\n\n\n## Notes\n### Vectors\n\nIn linear algebra, vectors are column vectors by default, so\n$$ \\mathbf{u} \\in \\mathbb{R}^{n \\times 1}, \\quad \\mathbf{u} = \\begin{bmatrix} u_1 \\\\ \\vdots \\\\ u_n \\end{bmatrix} $$\n\nIt follows that when doing a Matrix-Vector product, the matrix is on the left $\\mathbf{A}\\mathbf{u}$.\n\n### Order of operations\n\nSimilarly, when we say \"we apply a linear transformation $\\mathbf{A}$ to $\\mathbf{B}$\", we mean $\\mathbf{A}\\mathbf{B}$.\n\n### Determinant\n\nI think of the determinant of $\\mathbf{A}$ as the scaling factor of the linear transformation represented by $\\mathbf{A}$.\n\nThis explains why a matrix whose determinant is 0 is not invertible: it \"collapses\", and two images might have the same original input $\\mathbf{x}$\n\n## Proofs\n\nLater!\n\n## Algorithms\n### Gaussian Elimination\n\nGaussian elimination — or row reduction – is an algorithm for solving systems of linear equations, based on the following operations:\n\n- Scaling a row\n- Swapping two rows\n- Adding a multiple of one row to another\n\n### LU decomposition (or LU factorization)\n\nComputing the determinant of a Matrix is not trivial at first glance.\n\nBut consider the following fact:\n\n- we know how to easily compute the [determinant of a triangular matrix](/notes/linear-algebra.html#determinant-of-a-triangular-matrix)\n- we know how to easily compute the [determinant of a product of matrices](/notes/linear-algebra.html#basic-identities)\n\nKnowing that, we want to find a representation of our original matrix $\\mathbf{A}$ that involves an Upper Triangular Matrix $\\mathbf{U}$, and one or more other matrices whose determinant is known or trivial to compute,  as $\\mathbf{P}\\mathbf{A} = \\mathbf{L}\\mathbf{U}$\n\nTo go from $\\mathbf{A}$ to $\\mathbf{U}$ we'll use [Gaussian Elimination](/notes/linear-algebra.html#gaussian-elimination), $\\mathbf{P}$ tracks our permutations (row swaps) and $\\mathbf{L}$ tracks our row operations (row additions).\n\nNow, because $\\mathbf{P}$ is [orthogonal](/notes/linear-algebra.html#orthogonality) (yes, since its the identity matrix with row swaps, when performing $\\mathbf{P}^\\top\\mathbf{P}$, the ones in the rows meet the ones in the columns at the diagonal, zeros everywhere else, so we get $\\mathbf{P}^\\top\\mathbf{P} = \\mathbf{I}$), we then have\n$$ \\mathbf{P}\\mathbf{A} = \\mathbf{L}\\mathbf{U} \\implies \\mathbf{A} = \\mathbf{P}^{-1}\\mathbf{L}\\mathbf{U} = \\mathbf{P}^\\top\\mathbf{L}\\mathbf{U} $$\n\nFinally, this means that\n$$ \\det(\\mathbf{A}) = \\det(\\mathbf{P}^\\top) \\cdot \\det(\\mathbf{L}) \\cdot \\det(\\mathbf{U}) = \\det(\\mathbf{P}) \\cdot \\det(\\mathbf{L}) \\cdot \\det(\\mathbf{U}) $$\n\nNow I'm not gonna prove that, but:\n\n- $\\det(\\mathbf{P}) = (-1)^{\\#swaps}$\n- $\\det(\\mathbf{L}) = 1$ (because when adding rows, we never modify the original identity's diagonal, so the diagonal is full of ones, and since L is a Lower Triangular Matrix, the determinant is 1)\n\nNow, if we just keep track of row swaps, we can easily compute $\\det(\\mathbf{A})$!\n\n## Notation\n\n- $x$: a scalar\n- $\\mathbf{x}$: a vector\n- $\\mathbf{X}$: a matrix\n- $x_i$: the $i^\\textrm{th}$ element of vector $\\mathbf{x}$\n- $x_{i,j}$: the element of matrix $\\mathbf{X}$ at row $i$ and column $j$\n- $\\mathbf{x}_{i, :}$: the $i^\\textrm{th}$ row-vector of $\\mathbf{X}$\n- $\\mathbf{x}_{:,j}$: the $j^\\textrm{th}$ column-vector of $\\mathbf{X}$\n- $\\operatorname{diag}(a_1, \\dots, a_n)$: a diagonal matrix\n- $\\mathbf{I}$: the indentity matrix\n- $\\mathbf{A}^{-1}$: the inverse of A\n\n",
    "supporting": [
      "linear-algebra_files"
    ],
    "filters": [],
    "includes": {}
  }
}