{
  "hash": "09225f6a7b81a4bfb98067312b960da2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Linear Algebra\nauthor: Theo POMIES\ndate: 2025-08-29\ndate-modified: 2025-09-04\ndescription: All things Linear Algebra, matrices, vectors etc.\ncategories: [math]\n---\n\n\n\n## Definitions & Formulas\n\n### Transpose\n$$ (\\mathbf{A}^\\top)_{i,j} = \\mathbf{A}_{j,i}  $$\n\n### Dot Product\n$$ \\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i} u_i v_i $$\n\n::: {#feae1e8a .cell execution_count=2}\n``` {.python .cell-code}\nu, v = torch.arange(3), torch.arange(3)  # torch.tensor([0, 1, 2])\ntorch.dot(u, v)  # u @ v\n```\n\n::: {.cell-output .cell-output-display execution_count=128}\n```\ntensor(5)\n```\n:::\n:::\n\n\n### Matrix-Vector Product\n$$ (\\mathbf{A}\\mathbf{u})_{i} = \\mathbf{a}_{i,:} \\cdot \\mathbf{u} = \\sum_{j} a_{i,j} u_j  $$\n\n::: {#f0fb4b8a .cell execution_count=3}\n``` {.python .cell-code}\nA, u = torch.arange(6).reshape(3, 2), torch.arange(2)\nA @ u\n```\n\n::: {.cell-output .cell-output-display execution_count=129}\n```\ntensor([1, 3, 5])\n```\n:::\n:::\n\n\n### Matrix Multiplication\n$$ (\\mathbf{A}\\mathbf{B})_{i,j} = \\mathbf{a}_{i,:} \\cdot \\mathbf{b}_{:, j} = \\sum_{k} a_{i,k} b_{k,j} $$\n\n::: {#4812c6d0 .cell execution_count=4}\n``` {.python .cell-code}\nU, V = torch.arange(6).reshape(3, 2), torch.arange(6).reshape(2, 3)\nU @ V\n```\n\n::: {.cell-output .cell-output-display execution_count=130}\n```\ntensor([[ 3,  4,  5],\n        [ 9, 14, 19],\n        [15, 24, 33]])\n```\n:::\n:::\n\n\n#### Properties\n\n* Non-commutativity: most of the time $\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}$\n* Distributivity: $\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{B}\\mathbf{C}$ and $(\\mathbf{A} + \\mathbf{B})\\mathbf{C} = \\mathbf{A}\\mathbf{C} + \\mathbf{B}\\mathbf{C}$\n* Associativity: $\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}$\n* Transpose: $(\\mathbf{A}\\mathbf{B})^\\top = \\mathbf{B}^\\top\\mathbf{A}^\\top$\n\n### Inverse of a Square Matrix\n$$ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I},\\quad \\mathbf{A} \\in \\mathbb{R}^{n \\times n} $$\nNote: Might not exist if $\\mathbf{A}$ is not invertible, that is $\\det(\\mathbf{A}) = 0$\n\n::: {#b4a54a72 .cell execution_count=5}\n``` {.python .cell-code}\nA = torch.randn(3, 3, dtype=torch.double)\nA_i = A.inverse()\nI = torch.eye(3, dtype=torch.double)\ntorch.allclose(A @ A_i, I), torch.allclose(A_i @ A, I)\n```\n\n::: {.cell-output .cell-output-display execution_count=131}\n```\n(True, True)\n```\n:::\n:::\n\n\n### Determinant of a Matrix\n#### Basic identities\n$$ \\det(\\mathbf{A}^\\top) = \\det(\\mathbf{A}) $$\n$$ \\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A})\\det(\\mathbf{B}) $$\n\n#### Determinant of Diagonal Matrix\n$$ A = \\operatorname{diag}(a_1, \\dots, a_n), \\quad \\det(A) = \\prod_{i=1}^n a_i $$\n\n::: {#77fbe668 .cell execution_count=6}\n``` {.python .cell-code}\n# using I as a mask\nI = torch.eye(3, dtype=torch.float32)\nA = torch.arange(1, 10, dtype=torch.float32).reshape(3, 3) * I  # A = diag(1, 5, 9)\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n```\n\n::: {.cell-output .cell-output-display execution_count=132}\n```\n(tensor(45.), tensor(True))\n```\n:::\n:::\n\n\n#### Determinant of a Triangular Matrix\n$$ \\det(T) = \\prod_{i} t_{ii}, \\quad \\text{where $T$ is triangular} $$\n\n::: {#a0f7eda9 .cell execution_count=7}\n``` {.python .cell-code}\nA = torch.triu(torch.arange(1, 10, dtype=torch.float32).reshape(3, 3))\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n```\n\n::: {.cell-output .cell-output-display execution_count=133}\n```\n(tensor(45.), tensor(True))\n```\n:::\n:::\n\n\n### Orthogonality\n$$ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}\\: (= \\mathbf{A}\\mathbf{A}^\\top) $$\n\nInterstingly,\n\n$$ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^{-1} = \\mathbf{A}^\\top $$\n\n### Elementary Matrix\n\nAn elementary matrix is a matrix obtained from the application of a [**single elementary row operation**](/notes/linear-algebra#elementary-matrix) to the identity matrix $\\mathbf{I}$.\n\nBecause of **associativity**, the product of an elementary matrix and another matrix is the same as applying the row operation to the other matrix.\n\n#### Scaling a row\n\nMultiplying row $i$ by $m$\n\n$$\\mathbf{D}_i(m) = \\operatorname{diag}(1, \\dots, 1, d_i = m, 1, \\dots, 1)$$\n\n##### Properties\n\n* $\\mathbf{D}_i(m) = \\mathbf{D}_i(\\dfrac{1}{m})^{-1} = \\mathbf{D}_i(m)^\\top$\n* $\\operatorname{det}(\\mathbf{D}_i(m)) = m$ — obviously because it's a diagonal matrix full of $1$'s and a single $m$\n\n#### Adding a multiple of one row to another\n\nAdding $m$ times row $j$ to row $i$\n\n$$\\mathbf{L}_{ij}(m) =\n\\begin{bmatrix}\n1 &&&&&& \\\\\n& \\ddots &&&&& \\\\\n&& 1 &&&& \\\\\n&&& \\ddots &&& \\\\\n&& l_{i,j} = m && 1 && \\\\\n&&&&& \\ddots & \\\\\n&&&&&& 1\n\\end{bmatrix}$$\n\n##### Properties\n\n* $\\mathbf{L}_{ij}(m)^{-1} = \\mathbf{L}_{ij}(-m)$ — the inverse of adding a row multiple to another, is *subtracting* the same row multiple\n* $\\mathbf{L}_{ij}(m)$ and $\\mathbf{L}_{ij}(m)^{-1}$ are triangular matrices\n* So $\\operatorname{det}(\\mathbf{L}_{ij}(m)) = 1 \\iff \\operatorname{det}(\\mathbf{L}_{ij}(m)\\mathbf{A}) = \\operatorname{det}(\\mathbf{A})$\n\n#### Swapping (switching) rows\n\nSwapping rows $i$ and $j$\n\n$$\\mathbf{T}_{ij} =\n\\begin{bmatrix}\n1 &&&&&& \\\\\n& \\ddots &&&&& \\\\\n&& 0 && t_{j,i} = 1 && \\\\\n&&& \\ddots &&& \\\\\n&& t_{i,j} = 1 && 0 && \\\\\n&&&&& \\ddots & \\\\\n&&&&&& 1\n\\end{bmatrix}$$\n\n##### Properties\n\n* $\\mathbf{T}_{ij}^\\top = \\mathbf{T}_{ij}^{-1} = \\mathbf{T}_{ij}$\n* $\\operatorname{det}(\\mathbf{T}_{ij}) = -1\\:^{(*)} \\iff \\operatorname{det}(\\mathbf{T}_{ij}\\mathbf{A}) = -\\operatorname{det}(\\mathbf{A})$\n\n$^{(*)}$ Consider that swapping two rows can be expressed as applying consecutive additions/subtractions as follows\n$$\\begin{aligned}\n\\mathbf{T}_{ij} & = \\mathbf{D}_{i}(-1)\\mathbf{L}_{ij}(-1)\\mathbf{L}_{ji}(1)\\mathbf{L}_{ij}(-1) \\\\\n\\iff \\operatorname{det}(\\mathbf{T}_{ij}) & = \\operatorname{det}(\\mathbf{D}_{i}(-1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ij}(-1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ji}(1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ij}(-1)) \\\\\n\\iff \\operatorname{det}(\\mathbf{T}_{ij}) & = -1 \\\\\n\\end{aligned}$$\n\n## Notes\n### Vectors\n\nIn linear algebra, vectors are column vectors by default, so\n$$ \\mathbf{u} \\in \\mathbb{R}^{n \\times 1}, \\quad \\mathbf{u} = \\begin{bmatrix} u_1 \\\\ \\vdots \\\\ u_n \\end{bmatrix} $$\n\nIt follows that when doing a Matrix-Vector product, the matrix is on the left $\\mathbf{A}\\mathbf{u}$.\n\n### Order of operations\n\nSimilarly, when we say \"we apply a linear transformation $\\mathbf{A}$ to $\\mathbf{B}$\", we mean $\\mathbf{A}\\mathbf{B}$.\n\n### Determinant\n\nI think of the determinant of $\\mathbf{A}$ as the scaling factor of the linear transformation represented by $\\mathbf{A}$.\n\nThis explains why a matrix whose determinant is 0 is not invertible: it \"collapses\", and two images might have the same original input $\\mathbf{x}$\n\n## Proofs\n\nLater!\n\n## Algorithms\n### Gaussian Elimination\n\nGaussian elimination — or row reduction – is an algorithm for solving systems of linear equations, based on the following **elementary row operations**:\n\n* Scaling a row: $\\mathbf{R}_i \\gets k\\mathbf{R}_i$, where $k \\neq 0$\n* Adding a multiple of one row to another: $\\mathbf{R}_i \\gets \\mathbf{R}_i + k\\mathbf{R}_j$, where $i \\neq j$\n* Swapping two rows: $\\mathbf{R}_i \\leftrightarrow \\mathbf{R}_j$\n\n### Laplace Expansion\n\n### LU decomposition (or LU factorization)\n\nComputing the determinant of a Matrix is not trivial at first glance.\n\nBut consider the following facts:\n\n- we know how to easily compute the [determinant of a product of matrices](/notes/linear-algebra.html#basic-identities)\n- we know how to easily compute the [determinant of a elementary matrices](/notes/linear-algebra.html#elementary-matrices)\n\nKnowing that, we want to find a representation of our original matrix $\\mathbf{A}$ that involves an Upper Triangular Matrix $\\mathbf{U}$, and one or more other matrices whose determinant is known or trivial to compute,  as $\\mathbf{P}\\mathbf{A} = \\mathbf{L}\\mathbf{U}$\n\nTo go from $\\mathbf{A}$ to $\\mathbf{U}$ we'll use [Gaussian Elimination](/notes/linear-algebra.html#gaussian-elimination), $\\mathbf{P}$ tracks our permutations (row swaps) and $\\mathbf{L}$ tracks our row operations (row additions).\n\nNow, because $\\mathbf{P}$ is [orthogonal](/notes/linear-algebra.html#orthogonality), we have\n$$ \\mathbf{P}\\mathbf{A} = \\mathbf{L}\\mathbf{U} \\implies \\mathbf{A} = \\mathbf{P}^{-1}\\mathbf{L}\\mathbf{U} = \\mathbf{P}^\\top\\mathbf{L}\\mathbf{U} $$\n\nFinally, this means that\n$$ \\det(\\mathbf{A}) = \\det(\\mathbf{P}^\\top) \\cdot \\det(\\mathbf{L}) \\cdot \\det(\\mathbf{U}) = \\det(\\mathbf{P}) \\cdot \\det(\\mathbf{L}) \\cdot \\det(\\mathbf{U}) $$\n\n* $\\det(\\mathbf{P}) = (-1)^{\\#swaps}$\n* $\\det(\\mathbf{L}) = 1$ — product of \"multiplied row additions\" elementary matrices\n\nNow, if we just keep track of row swaps, we can easily compute $\\det(\\mathbf{A})$!\n\n## Notation\n\n* $x$: a scalar\n* $\\mathbf{x}$: a vector\n* $\\mathbf{X}$: a matrix\n* $x_i$: the $i^\\textrm{th}$ element of vector $\\mathbf{x}$\n* $x_{i,j}$: the element of matrix $\\mathbf{X}$ at row $i$ and column $j$\n* $\\mathbf{x}_{i, :}$: the $i^\\textrm{th}$ row-vector of $\\mathbf{X}$\n* $\\mathbf{x}_{:,j}$: the $j^\\textrm{th}$ column-vector of $\\mathbf{X}$\n* $\\operatorname{diag}(a_1, \\dots, a_n)$: a diagonal matrix\n* $\\mathbf{I}$: the indentity matrix\n* $(\\cdot)^\\top$: the transpose of a vector or matrix\n* $\\mathbf{A}^{-1}$: the inverse of a matrix\n\n",
    "supporting": [
      "linear-algebra_files"
    ],
    "filters": [],
    "includes": {}
  }
}