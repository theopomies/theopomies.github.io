{
  "hash": "fb480abd14b9edd680c203d742d75562",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Linear Algebra\nauthor: Theo POMIES\ndate: 2025-08-29\ndate-modified: 2025-09-04\ndescription: All things Linear Algebra, matrices, vectors etc.\ncategories: [math]\n---\n\n\n\n## Definitions & Formulas\n### Vector Space {#sec-vector-space}\nA **vector space** (over a field, like $\\mathbb{R}$, which provides the **scalars**) is a **set** of **vectors** equipped with **vector addition** and **scalar multiplication** that satisfies the following conditions:\n\n1. **Closure under addition:** $\\mathbf{u} + \\mathbf{v}$ is in the space for any $\\mathbf{u}, \\mathbf{v}$ in the space.\n2. **Closure under scalar multiplication:** $\\alpha \\mathbf{u}$ is in the space for any scalar $\\alpha$.\n3. **Associativity of addition:** $\\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}$.\n4. **Commutativity of addition:** $\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}$.\n5. **Additive identity:** there exists $\\mathbf{0}$ such that $\\mathbf{u} + \\mathbf{0} = \\mathbf{u}$.\n6. **Additive inverse:** for each $\\mathbf{u}$, there exists $-\\mathbf{u}$ such that $\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}$.\n7. **Compatibility of scalar multiplication:** $\\alpha(\\beta \\mathbf{u}) = (\\alpha\\beta) \\mathbf{u}$.\n8. **Identity of scalar multiplication:** $1 \\cdot \\mathbf{u} = \\mathbf{u}$.\n9. **Distributivity over vector addition:** $\\alpha(\\mathbf{u} + \\mathbf{v}) = \\alpha \\mathbf{u} + \\alpha \\mathbf{v}$.\n10. **Distributivity over scalar addition:** $(\\alpha + \\beta)\\mathbf{u} = \\alpha \\mathbf{u} + \\beta \\mathbf{u}$.\n\n::: {.callout-note}\nThis might be useful later, but for all intents and purposes in these notes, the field will be $\\mathbb{R}$, vectors will be in $\\mathbb{R}^n$, and addition and scalar multiplication will be element-wise.\n:::\n\n### Transpose\n$$ (\\mathbf{A}^\\top)_{i,j} = \\mathbf{A}_{j,i}  $$\n\n### Dot Product\n$$ \\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i} u_i v_i $$\n\n::: {#29da284b .cell execution_count=2}\n``` {.python .cell-code}\nu, v = torch.arange(3), torch.arange(3)  # torch.tensor([0, 1, 2])\ntorch.dot(u, v)  # u @ v\n```\n\n::: {.cell-output .cell-output-display execution_count=79}\n```\ntensor(5)\n```\n:::\n:::\n\n\n::: {.callout-note}\nThe dot product has a geometric meaning.\n\n$$\\mathbf{v}\\cdot\\mathbf{w} = \\|\\mathbf{v}\\|\\|\\mathbf{w}\\|\\cos{\\theta}$$\n$$\\iff \\theta = \\arccos\\left(\\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}\\right).$$\n:::\n\n::: {.callout-note}\nIn linear algebra, vectors are column vectors by default, so\n$$ \\mathbf{u} \\in \\mathbb{R}^{n \\times 1}, \\quad \\mathbf{u} = \\begin{bmatrix} u_1 \\\\ \\vdots \\\\ u_n \\end{bmatrix} $$\n\nIt follows that when doing a Matrix-Vector product, the matrix is on the left $\\mathbf{A}\\mathbf{u}$.\n:::\n\n### Matrix-Vector Product\n$$ (\\mathbf{A}\\mathbf{u})_{i} = \\mathbf{a}_{i,:} \\cdot \\mathbf{u} = \\sum_{j} a_{i,j} u_j  $$\n\n::: {#f0e8dd77 .cell execution_count=3}\n``` {.python .cell-code}\nA, u = torch.arange(6).reshape(3, 2), torch.arange(2)\nA @ u\n```\n\n::: {.cell-output .cell-output-display execution_count=80}\n```\ntensor([1, 3, 5])\n```\n:::\n:::\n\n\n::: {.callout-note}\nWhen we say \"we apply a linear transformation $\\mathbf{A}$ to $\\mathbf{B}$\", we mean $\\mathbf{A}\\mathbf{B}$.\n:::\n\n\n### Matrix Multiplication\n$$ (\\mathbf{A}\\mathbf{B})_{i,j} = \\mathbf{a}_{i,:} \\cdot \\mathbf{b}_{:, j} = \\sum_{k} a_{i,k} b_{k,j} $$\n\n::: {#29291e7e .cell execution_count=4}\n``` {.python .cell-code}\nU, V = torch.arange(6).reshape(3, 2), torch.arange(6).reshape(2, 3)\nU @ V\n```\n\n::: {.cell-output .cell-output-display execution_count=81}\n```\ntensor([[ 3,  4,  5],\n        [ 9, 14, 19],\n        [15, 24, 33]])\n```\n:::\n:::\n\n\n#### Properties\n\n* Non-commutativity: most of the time $\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}$\n* Distributivity: $\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{B}\\mathbf{C}$ and $(\\mathbf{A} + \\mathbf{B})\\mathbf{C} = \\mathbf{A}\\mathbf{C} + \\mathbf{B}\\mathbf{C}$\n* Associativity: $\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}$\n* Transpose: $(\\mathbf{A}\\mathbf{B})^\\top = \\mathbf{B}^\\top\\mathbf{A}^\\top$\n\n### Inverse of a Square Matrix\n$$ \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I},\\quad \\mathbf{A} \\in \\mathbb{R}^{n \\times n} $$\n\n:::{.callout-note}\nMight not exist if $\\mathbf{A}$ is not invertible, that is $\\det(\\mathbf{A}) = 0$\n:::\n\n::: {#36d7e618 .cell execution_count=5}\n``` {.python .cell-code}\nA = torch.randn(3, 3, dtype=torch.double)\nA_i = A.inverse()\nI = torch.eye(3, dtype=torch.double)\ntorch.allclose(A @ A_i, I), torch.allclose(A_i @ A, I)\n```\n\n::: {.cell-output .cell-output-display execution_count=82}\n```\n(True, True)\n```\n:::\n:::\n\n\n### Determinant of a Matrix\nI think of the determinant of $\\mathbf{A}$ as the scaling factor of the linear transformation represented by $\\mathbf{A}$.\n\nThis explains why a matrix whose determinant is 0 is not invertible: it \"collapses\", and two images might have the same original input $\\mathbf{x}$\n\n#### Basic identities {#sec-matrix-basic-identities}\n$$ \\det(\\mathbf{A}^\\top) = \\det(\\mathbf{A}) $$\n$$ \\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A})\\det(\\mathbf{B}) $$\n\n#### Determinant of Diagonal Matrix\n$$ A = \\operatorname{diag}(a_1, \\dots, a_n), \\quad \\det(A) = \\prod_{i=1}^n a_i $$\n\n::: {#de55ba43 .cell execution_count=6}\n``` {.python .cell-code}\n# using I as a mask\nI = torch.eye(3, dtype=torch.float32)\nA = torch.arange(1, 10, dtype=torch.float32).reshape(3, 3) * I  # A = diag(1, 5, 9)\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n```\n\n::: {.cell-output .cell-output-display execution_count=83}\n```\n(tensor(45.), tensor(True))\n```\n:::\n:::\n\n\n#### Determinant of a Triangular Matrix\n$$ \\det(T) = \\prod_{i} t_{ii}, \\quad \\text{where $T$ is triangular} $$\n\n::: {#b84cae4e .cell execution_count=7}\n``` {.python .cell-code}\nA = torch.triu(torch.arange(1, 10, dtype=torch.float32).reshape(3, 3))\ntorch.det(A), torch.det(A) == torch.diag(A).prod()\n```\n\n::: {.cell-output .cell-output-display execution_count=84}\n```\n(tensor(45.), tensor(True))\n```\n:::\n:::\n\n\n### Orthogonality {#sec-orthogonality}\n$$ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}\\: (= \\mathbf{A}\\mathbf{A}^\\top) $$\n\nInterstingly,\n\n$$ \\mathbf{A} \\, \\textnormal{is orthogonal} \\iff \\mathbf{A}^{-1} = \\mathbf{A}^\\top $$\n\n### Elementary Matrix {#sec-elementary-matrix}\n\nAn elementary matrix is a matrix obtained from the application of a **single elementary row operation** to the identity matrix $\\mathbf{I}$.\n\nBecause of **associativity**, the product of an elementary matrix and another matrix is the same as applying the row operation to the other matrix.\n\n#### Scaling a row\n\nMultiplying row $i$ by $m$\n\n$$\\mathbf{D}_i(m) = \\operatorname{diag}(1, \\dots, 1, d_i = m, 1, \\dots, 1)$$\n\n##### Properties\n\n* $\\mathbf{D}_i(m) = \\mathbf{D}_i(\\dfrac{1}{m})^{-1} = \\mathbf{D}_i(m)^\\top$\n* $\\operatorname{det}(\\mathbf{D}_i(m)) = m$ — obviously because it's a diagonal matrix full of $1$'s and a single $m$\n\n#### Adding a multiple of one row to another\n\nAdding $m$ times row $j$ to row $i$\n\n$$\\mathbf{L}_{ij}(m) =\n\\begin{bmatrix}\n1 &&&&&& \\\\\n& \\ddots &&&&& \\\\\n&& 1 &&&& \\\\\n&&& \\ddots &&& \\\\\n&& l_{i,j} = m && 1 && \\\\\n&&&&& \\ddots & \\\\\n&&&&&& 1\n\\end{bmatrix}$$\n\n##### Properties\n\n* $\\mathbf{L}_{ij}(m)^{-1} = \\mathbf{L}_{ij}(-m)$ — the inverse of adding a row multiple to another, is *subtracting* the same row multiple\n* $\\mathbf{L}_{ij}(m)$ and $\\mathbf{L}_{ij}(m)^{-1}$ are triangular matrices\n* So $\\operatorname{det}(\\mathbf{L}_{ij}(m)) = 1 \\iff \\operatorname{det}(\\mathbf{L}_{ij}(m)\\mathbf{A}) = \\operatorname{det}(\\mathbf{A})$\n\n#### Swapping (switching) rows\n\nSwapping rows $i$ and $j$\n\n$$\\mathbf{T}_{ij} =\n\\begin{bmatrix}\n1 &&&&&& \\\\\n& \\ddots &&&&& \\\\\n&& 0 && t_{j,i} = 1 && \\\\\n&&& \\ddots &&& \\\\\n&& t_{i,j} = 1 && 0 && \\\\\n&&&&& \\ddots & \\\\\n&&&&&& 1\n\\end{bmatrix}$$\n\n##### Properties\n\n* $\\mathbf{T}_{ij}^\\top = \\mathbf{T}_{ij}^{-1} = \\mathbf{T}_{ij}$\n* $\\operatorname{det}(\\mathbf{T}_{ij}) = -1\\:^{(*)} \\iff \\operatorname{det}(\\mathbf{T}_{ij}\\mathbf{A}) = -\\operatorname{det}(\\mathbf{A})$\n\n$^{(*)}$ Consider that swapping two rows can be expressed as applying consecutive additions/subtractions as follows\n$$\\begin{aligned}\n\\mathbf{T}_{ij} & = \\mathbf{D}_{i}(-1)\\mathbf{L}_{ij}(-1)\\mathbf{L}_{ji}(1)\\mathbf{L}_{ij}(-1) \\\\\n\\iff \\operatorname{det}(\\mathbf{T}_{ij}) & = \\operatorname{det}(\\mathbf{D}_{i}(-1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ij}(-1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ji}(1)) \\cdot \\operatorname{det}(\\mathbf{L}_{ij}(-1)) \\\\\n\\iff \\operatorname{det}(\\mathbf{T}_{ij}) & = -1 \\\\\n\\end{aligned}$$\n\n### Linear Dependence\nWe say of $n$ vectors — $\\mathbf{v}_1, \\dots, \\mathbf{v}_n$ that they are **linearly dependent** if there exists scalars $a_1, \\dots, a_n$ **not all equal to 0** satisfying\n$$\\sum_{i=0}^n a_i\\mathbf{v}_i = 0$$\n\nIf the only solution is $a_i = 0$ for $i$ in $0, \\dots, n$, they are **linearly independent**.\n\n::: {.callout-note}\nLinear dependence means that some vectors could be expressed as a weighted sum (**linear combination**) of others, hence carrying redudant information/operation.\n:::\n\n### Rank\nThe **rank** of a matrix is the size of the **largest** subset of linearly independent columns among its columns.\nEquivalently, it is the **dimension of the column space**.\n\n::: {.callout-note}\nThis reflects that if some columns are linearly dependent, they are redundant: they do not contribute a new direction.\nThus, the matrix can be expressed with fewer independent columns, and the image (output space) of the matrix has correspondingly fewer dimensions.\n:::\n\n### Basis\nA **basis** (not *the*, since a vector space can have many different bases) of a **[vector space @sec-vector-space]** is a linearly independent subset that **spans** the space.\n\nAll bases of the same vector space have the same size = the **dimension**\n\n::: {.callout-note}\nThat means: a basis is a set of vectors that you can combine (with weighted sums) to form **any** vector in the space — and none of them are redundant.\n:::\n\n### Dimension\nThe **dimension** of a vector space is the number of vectors in a basis of that space.\n\n::: {.callout-note}\nSince every basis of a vector space has the same size, the dimension is well-defined.\nIt tells you “how many independent directions” the space has.\n:::\n\n### Eigendecomposition\nAn **eigenvector** is a vector whose direction (eg. \"line\") is unchanged by a linear transformation (represented by a matrix). That means that when applying the linear transformation $\\mathbf{A}$ to an eigeinvector $\\mathbf{v}$ it simply gets scaled by a constant scalar factor, the **eigenvalue** $\\lambda$. Represented by this **eigenequation**.\n$$\\begin{aligned}\n\\mathbf{A}\\mathbf{v} & = \\lambda\\mathbf{v} \\\\\n\\iff (\\mathbf{A} - \\lambda\\mathbf{I})\\mathbf{v} & = \\mathbf{0} \\label{eq:1}\n\\end{aligned}$$ {#eq-eigenequation}\n\nThis [equation @eq-eigenequation] has a non-zero solution $\\mathbf{v}$ only if $\\operatorname{det}(\\mathbf{A} - \\lambda\\mathbf{I}) = 0$ — called the **characteristic equation** of $\\mathbf{A}$. The non-zero values of $\\lambda$ that satisfy this equation are the **egeinvalues** of $\\mathbf{A}$.\n\nThe **eigenvectors** $\\mathbf{v}_{\\lambda=n}$ corresponding to each eigenvalue can be found by plugging the values of $\\lambda$ in [equation @eq-eigenequation].\n\n## Proofs\n\nLater!\n\n## Algorithms\n### Gaussian Elimination {#sec-gaussian-elimination}\n\nGaussian elimination — or row reduction – is an algorithm for solving systems of linear equations, based on the following **elementary row operations**:\n\n* Scaling a row: $\\mathbf{R}_i \\gets k\\mathbf{R}_i$, where $k \\neq 0$\n* Adding a multiple of one row to another: $\\mathbf{R}_i \\gets \\mathbf{R}_i + k\\mathbf{R}_j$, where $i \\neq j$\n* Swapping two rows: $\\mathbf{R}_i \\leftrightarrow \\mathbf{R}_j$\n\n### Laplace Expansion\n\n### LU decomposition (or LU factorization)\n\nComputing the determinant of a Matrix is not trivial at first glance.\n\nWe know how to easily compute:\n- the [determinant of a product of matrices @sec-matrix-basic-identities]\n- the [determinant of a elementary matrices @sec-elementary-matrix]\n\nKnowing that, we want to find a representation of our original matrix $\\mathbf{A}$ that involves an Upper Triangular Matrix $\\mathbf{U}$, and one or more other matrices whose determinant is known or trivial to compute,  as $\\mathbf{P}\\mathbf{A} = \\mathbf{L}\\mathbf{U}$\n\nTo go from $\\mathbf{A}$ to $\\mathbf{U}$ we'll use [Gaussian Elimination @sec-gaussian-elimination], $\\mathbf{P}$ tracks our permutations (row swaps) and $\\mathbf{L}$ tracks our row operations (row additions).\n\nNow, because $\\mathbf{P}$ is [orthogonal @sec-orthogonality], we have\n$$ \\mathbf{P}\\mathbf{A} = \\mathbf{L}\\mathbf{U} \\implies \\mathbf{A} = \\mathbf{P}^{-1}\\mathbf{L}\\mathbf{U} = \\mathbf{P}^\\top\\mathbf{L}\\mathbf{U} $$\n\nFinally, this means that\n$$ \\det(\\mathbf{A}) = \\det(\\mathbf{P}^\\top) \\cdot \\det(\\mathbf{L}) \\cdot \\det(\\mathbf{U}) = \\det(\\mathbf{P}) \\cdot \\det(\\mathbf{L}) \\cdot \\det(\\mathbf{U}) $$\n\n* $\\det(\\mathbf{P}) = (-1)^{\\#swaps}$\n* $\\det(\\mathbf{L}) = 1$ — product of \"multiplied row additions\" elementary matrices\n\nNow, if we just keep track of row swaps, we can easily compute $\\det(\\mathbf{A})$!\n\n## Notation\n\n* $x$: a scalar\n* $\\mathbf{x}$: a vector\n* $\\mathbf{X}$: a matrix\n* $x_i$: the $i^\\textrm{th}$ element of vector $\\mathbf{x}$\n* $x_{i,j}$: the element of matrix $\\mathbf{X}$ at row $i$ and column $j$\n* $\\mathbf{x}_{i, :}$: the $i^\\textrm{th}$ row-vector of $\\mathbf{X}$\n* $\\mathbf{x}_{:,j}$: the $j^\\textrm{th}$ column-vector of $\\mathbf{X}$\n* $\\operatorname{diag}(a_1, \\dots, a_n)$: a diagonal matrix\n* $\\mathbf{I}$: the indentity matrix\n* $\\mathbf{0}$: the zero vector / zero matrix — depending on context\n* $(\\cdot)^\\top$: the transpose of a vector or matrix\n* $\\mathbf{A}^{-1}$: the inverse of a matrix\n\n",
    "supporting": [
      "linear-algebra_files"
    ],
    "filters": [],
    "includes": {}
  }
}