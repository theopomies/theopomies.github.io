---
title: Distributed Training
author: Theo POMIES
date: 2025-09-02
date-modified: 2025-09-10
description: Distributed training study notes and algorithms.
categories: [mle, python]
---

```{python}
# | echo: false
import torch
import torch.distributed as dist

rank = 0
```

## Functions / Methods
### Basics (Point-Point)
`send` and `recv` to send or receive a tensor synchronously — from/to a single rank.

And their async counterparts, `isend` and `irecv`.

```{python}
# | eval: false
rank = dist.get_rank()
tensor = torch.arange(3) if rank == 0 else torch.zeros(3)

print(f"Before: {tensor}")

if rank == 0:
    request = dist.isend(tensor, 1)
    ...
    # can do something else, like more sends for example!
    ...
    request.wait() # now block until it's been fulfilled
elif rank == 1:
    dist.recv(tensor, 0) # recv is synchronous, so it will block until tensor is fully received

print(f"After: {tensor}")
```
```{python}
# | echo: false
tensor = torch.arange(3)
zeros = torch.zeros(3)

print("========== rank 0 ==========")
print(f"Before: {tensor}")
print(f"After: {tensor}\n")

print("========== rank 1 ==========")
print(f"Before: {zeros}")
print(f"After: {tensor}")
```

### Collective Operations

Collective operations allow communication (data-transfer) from All->Point, Point->All and All->All.

#### Point->All
##### Broadcast

Broadcast (`torch.distributed.broadcast(tensor, src, ...)`) allows a rank to _broadcast_ a tensor to the whole group.

```{python}
# | eval: false
tensor = torch.arange(3) if rank == 0 else torch.zeros(3)

print(f"Before: {tensor}")

dist.broadcast(tensor, src=0)

print(f"After: {tensor})
```
```{python}
# | echo: false
tensor = torch.arange(3)
zeros = torch.zeros(3)

print("========== rank 0 ==========")
print(f"Before: {tensor}")
print(f"After: {tensor}\n")

print("========== rank 1 ==========")
print(f"Before: {zeros}")
print(f"After: {tensor}\n")

print("========== rank 2 ==========")
print(f"Before: {zeros}")
print(f"After: {tensor}")
```

##### Scatter

Scatter (`torch.distributed.scatter(tensor, scatter_list, src, ...)`) allows us to _scatter_ — split and broadcast different chunks of — a tensor from a rank to the whole group.

```{python}
# | eval: false
tensor = torch.zeros(3)
scatter_list = [torch.arange(3 * i, 3 * i + 3) if rank == 0 else torch.zeros(3) for i in range(world_size)]

print(f"Scatter list: {scatter_list}")
print(f"Before: {tensor}")

dist.scatter(tensor, scatter_list, src=0)

print(f"After: {tensor}")
```
```{python}
# | echo: false
tensor = torch.zeros(3)
scatter_list = [torch.arange(3 * i, 3 * i + 3) for i in range(2)]
scatter_list_zeros = [torch.zeros(3) for _ in range(2)]

print("========== rank 0 ==========")
print(f"Scatter list: {scatter_list}")
print(f"Before: {zeros}")
print(f"After: {scatter_list[0]}\n")

print("========== rank 1 ==========")
print(f"Scatter list: {scatter_list_zeros}")
print(f"Before: {zeros}")
print(f"After: {scatter_list[1]}")
```

#### All->Point
##### Reduce

Reduce (`torch.distributed.reduce(tensor, dst, op, ...)`) performs a reduction operation (N->1, eg. sum, max, min, prod, ...) and the `dst` rank receives the result.

```{python}
# | eval: false
tensor = torch.arange(3) + rank * 3

print(f"Before: {tensor}")

dist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)

print(f"After: {tensor}")
```
```{python}
# | echo: false
tensors = [ torch.arange(3) + rank * 3 for rank in range(2)]

print("========== rank 0 ==========")
print(f"Before: {tensors[0]}")
print(f"After: {tensors[0] + tensors[1]}\n")

print("========== rank 1 ==========")
print(f"Before: {tensors[1]}")
print(f"After: {tensors[1]}")
```

##### Gather

Gather (`torch.distributed.gather(tensor, gather_list, dst, ...)`) _gathers_ — pulls — a tensor, of the same size, from every rank and stores them in a list in a single rank.

```{python}
# | eval: false
tensor = torch.arange(3) + rank * 3
gather_list = [torch.zeros(3) for _ in range(world_size)]

print(f"Before: {tensor}")
print(f"Before: {gather_list}")

dist.gather(tensor, gather_list, dst=0)

print(f"After: {gather_list}")
```
```{python}
# | echo: false
tensors = [ torch.arange(3) + rank * 3 for rank in range(2)]
gather_lists = [
    [torch.arange(3) + rank * 3 for rank in range(2)],
    [torch.zeros(3) for _ in range(2)],
]

print("========== rank 0 ==========")
print(f"Before: {tensors[0]}")
print(f"Before: {gather_lists[1]}")
print(f"Before: {tensors[1]}\n")

print("========== rank 1 ==========")
print(f"Before: {gather_lists[1]}")
print(f"After: {gather_lists[0]}")
print(f"After: {gather_lists[1]}")
```

#### All->All
##### All-Reduce

All-Reduce (`torch.distributed.all_reduce(tensor, op, ...)`) performs a _reduction_ operation, like `reduce`, but every rank receives the result — rather than a single one with `reduce`. Think of it as `reduce` + `broadcast` — though it is optimized by techniques like ring-reduce.

```{python}
# | eval: false
tensor = torch.arange(3) + rank * 3

print(f"Before: {tensor}")

dist.all_reduce(tensor, op=dist.ReduceOp.SUM)

print(f"After: {tensor}")
```
```{python}
# | echo: false
tensors = [ torch.arange(3) + rank * 3 for rank in range(2)]

print("========== rank 0 ==========")
print(f"Before: {tensors[0]}")
print(f"After: {tensors[0] + tensors[1]}\n")

print("========== rank 1 ==========")
print(f"Before: {tensors[1]}")
print(f"After: {tensors[0] + tensors[1]}")
```

##### All-Gather

All-Gather (`torch.distributed.all_gather(tensor, gather_list, ...)`) _gathers_ — pulls — a tensor, of the same size, from every rank and stores them in a list in _every_ rank. Think of it as running `gather` on all ranks.

```{python}
# | eval: false
tensor = torch.arange(3) + rank * 3
gather_list = [torch.zeros(3) for _ in range(world_size)]

print(f"Before: {tensor}")
print(f"Before: {gather_list}")

dist.all_gather(tensor, gather_list)

print(f"After: {gather_list}")
```
```{python}
# | echo: false
tensors = [torch.arange(3) + rank * 3 for rank in range(2)]
gather_lists = [
    [torch.arange(3) + rank * 3 for rank in range(2)],
    [torch.zeros(3) for _ in range(2)],
]

print("========== rank 2 ==========")
print(f"Before: {tensors[0]}")
print(f"Before: {gather_lists[1]}")
print(f"After: {gather_lists[0]}\n")

print("========== rank 1 ==========")
print(f"Before: {tensors[1]}")
print(f"Before: {gather_lists[1]}")
print(f"After: {gather_lists[0]}")
```

##### Reduce-Scatter

Reduce-Scatter (`torch.distributed.reduce_scatter(output_tensor, input_list, op, ...)`) performs a _reduction_ operation — like other `reduce` functions — and _scatters_ the resulting tensor. Think of it like `reduce` + `scatter`.

:::{.callout-note}
It needs `len(input_list) == world_size` and every tensor in `input_list` to have the same shape of `output_tensor`.
:::

```{python}
# | eval: false
tensor = torch.zeros(2)
scatter_list = [torch.tensor([(rank + 1) * i for i in range(1, 3)]) ** (j + 1) for j in range(3)]

print(f"Before: {tensor}")
print(f"Before: {scatter_list}")

dist.reduce_scatter(tensor, scatter_list, op=dist.ReduceOp.SUM)

print(f"After: {tensor}")
```
```{python}
# | echo: false
world_size = 3
zeros = torch.zeros(2)
scatter_lists = [[torch.tensor([(rank + 1) * i for i in range(1, 3)]) ** (j + 1) for j in range(3)] for rank in range(world_size)]
tensors = [sum(scatter_lists[i][j] for i in range(world_size)) for j in range(world_size)]

print("========== rank 0 ==========")
print(f"Before: {zeros}")
print(f"Before: {scatter_lists[0]}")
print(f"After: {tensors[0]}\n")

print("========== rank 1 ==========")
print(f"Before: {zeros}")
print(f"Before: {scatter_lists[1]}")
print(f"After: {tensors[1]}\n")

print("========== rank 2 ==========")
print(f"Before: {zeros}")
print(f"Before: {scatter_lists[2]}")
print(f"After: {tensors[2]}")
```

#### Other
##### Barrier

Barrier (`torch.distributed.barrier(...)`) synchronizes all ranks (processes), waiting for all of them to reach the barrier. (think of `.join()` for threads or processes)

## Algorithms / Techniques
### Types of parallelism

The goal of parallelism is to maximize throughput / cluster utilization.

* **Data Parallelism** (**DP**): Each rank has a replica of the model — they're **replicants** — and receives a different mini-batch. After [Gradient Accumulation] (optional), average the gradients (`all_reduce`).
* **Pipeline Parallelism** (**PP**): The model is split along the layers. Each rank has 1+ layer of the model, and we orchestrate sequential forward/backward passes along the ranks. This is **inter-layer** parallelism.
* **Tensor Parallelism** (**TP**): The model's layers are split across ranks. We need more complex orchestration since a single tensor's values are on different ranks. This is **intra-layer** parallelism.
* **Expert Parallelism** (**EP**): A specific type of **TP** where we only split the experts of an **MoE** across ranks.

::: {.callout-important}
ZeRO/FSDP is not a **parallelism** strategy, but a **modelling** strategy.

Parallelism = optimizing throughput
Modelling $\approx$ optimizing memory

Both can (and should) be combined.
:::

### DDP — Distributed Data Parallelism

Distributed Data Parallelism is a type of parallelism where each _rank_ loads a copy — replica — of the model, after each optimizer step they always all have the same parameters, they are **replicants**. Each rank then trains on a _different_ mini-batch (hence the importance of [data sharding](/notes/distributed-training.qmd#data-sharding)). We then average the gradients (`all_reduce` sum + division by world_size), perform a step of gradient descent, rinse and repeat. If we _can_ use this, we should, it has the least amount of overhead, but it requires that the model + optimizer states all fit in the device's VRAM.

:::{.callout-note}
The difference between DDP and DP is that DDP uses processes to avoid the GIL and DP uses threads. Do not use DP, only DDP.
:::

```python
class SimpleDataParaellism():
    def __init__(self, model):
        self.model = model

        for param in model.parameters():
            rank_0_params = param.data.clone()
            dist.broadcast(rank_0_params, src=0)
            assert torch.equal(param.data, rank_0_params), "Parameters mismatch at initialization"

    def sync_grad(self):
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.SUM)
            param.grad /= dist.get_world_size()
```

### Data Sharding

Data Sharding is the process of sharding — splitting — the dataset / dataloader so that each rank only pulls their own unique mini-batches of the training data. This avoids duplicates and is more commucation / memory efficient that duplicating the same full dataset on every rank. To do this with torch, setup the `DataLoader` with `sampler=[instance of DistributedSampler]`.

### ZeRO

**Ze**ro **R**edudency **O**ptimizer is a modeling strategy involving sharding states and parameters during training as a mean of optimizing peak memory. The core idea is that the states and/or parameters are sharded, retrieved only when necessary for some computation, updated information is shared, then anything we do not use anymore is discarded.

## Notes

## Terminology

* **device**: Hardware unit — GPU `"cuda:0"`, CPU `"cpu"` etc. **that's where tensors and computations live**
* **node**: Phyisical machine/server (or VPS whatever) that has 1+ devices
* **process**: Python process/worker, executing a copy of the code/script — often on a single device (GPU)
* **rank**: ID of a process — often that maps to a single device. **rank** without qualifiers is **global rank**
* **world**: Set of all processes part of our current distributed job
* **global** rank, world rank: rank across all processes/nodes. **note**: collective operations take the **global rank** (or just **rank**) as input for `src`/`dst`
* **local rank**: rank within a single node (**node** _not_ group). **note**: `device` takes the **local rank** `"cuda:{local_rank}"`
* **group**: subset of processes (1+ nodes) that we've grouped for sub-communications. **note**: we still use **global rank** for intra-group communication.
