<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Theo Pomies</title>
<link>https://theopomies.com/notes/</link>
<atom:link href="https://theopomies.com/notes/index.xml" rel="self" type="application/rss+xml"/>
<description>All my notes, formulas and proofs for me to reference later. These are evolving documents, so they might at times be incomplete</description>
<generator>quarto-1.8.24</generator>
<lastBuildDate>Tue, 02 Sep 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Distributed Training</title>
  <dc:creator>Theo POMIES</dc:creator>
  <link>https://theopomies.com/notes/distributed-training.html</link>
  <description><![CDATA[ 





<!-- TODO: DiLoCo, Streaming DiLoCo -->
<section id="functions-methods" class="level2">
<h2 class="anchored" data-anchor-id="functions-methods">Functions / Methods</h2>
<section id="basics-point-point" class="level3">
<h3 class="anchored" data-anchor-id="basics-point-point">Basics (Point-Point)</h3>
<p><code>send</code> and <code>recv</code> to send or receive a tensor synchronously — from/to a single rank.</p>
<p>And their async counterparts, <code>isend</code> and <code>irecv</code>.</p>
<div id="3f2754a1" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">rank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dist.get_rank()</span>
<span id="cb1-2">tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> rank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> torch.zeros(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Before: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> rank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb1-7">    request <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dist.isend(tensor, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-8">    ...</span>
<span id="cb1-9">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># can do something else, like more sends for example!</span></span>
<span id="cb1-10">    ...</span>
<span id="cb1-11">    request.wait() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># now block until it's been fulfilled</span></span>
<span id="cb1-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> rank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb1-13">    dist.recv(tensor, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># recv is synchronous, so it will block until tensor is fully received</span></span>
<span id="cb1-14"></span>
<span id="cb1-15"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"After: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
</div>
<div id="6b1d11d0" class="cell" data-execution_count="3">
<div class="cell-output cell-output-stdout">
<pre><code>========== rank 0 ==========
Before: tensor([0, 1, 2])
After: tensor([0, 1, 2])

========== rank 1 ==========
Before: tensor([0., 0., 0.])
After: tensor([0, 1, 2])</code></pre>
</div>
</div>
</section>
<section id="collective-operations" class="level3">
<h3 class="anchored" data-anchor-id="collective-operations">Collective Operations</h3>
<p>Collective operations allow communication (data-transfer) from All-&gt;Point, Point-&gt;All and All-&gt;All.</p>
<section id="point-all" class="level4">
<h4 class="anchored" data-anchor-id="point-all">Point-&gt;All</h4>
<section id="broadcast" class="level5">
<h5 class="anchored" data-anchor-id="broadcast">Broadcast</h5>
<p>Broadcast (<code>torch.distributed.broadcast(tensor, src, ...)</code>) allows a rank to <em>broadcast</em> a tensor to the whole group.</p>
<div id="40389502" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> rank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> torch.zeros(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb3-2"></span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Before: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb3-4"></span>
<span id="cb3-5">dist.broadcast(tensor, src<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb3-6"></span>
<span id="cb3-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"After: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)</span></span></code></pre></div></div>
</div>
<div id="880c6513" class="cell" data-execution_count="5">
<div class="cell-output cell-output-stdout">
<pre><code>========== rank 0 ==========
Before: tensor([0, 1, 2])
After: tensor([0, 1, 2])

========== rank 1 ==========
Before: tensor([0., 0., 0.])
After: tensor([0, 1, 2])

========== rank 2 ==========
Before: tensor([0., 0., 0.])
After: tensor([0, 1, 2])</code></pre>
</div>
</div>
</section>
<section id="scatter" class="level5">
<h5 class="anchored" data-anchor-id="scatter">Scatter</h5>
<p>Scatter (<code>torch.distributed.scatter(tensor, scatter_list, src, ...)</code>) allows us to <em>scatter</em> — split and broadcast different chunks of — a tensor from a rank to the whole group.</p>
<div id="d191e8e8" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.zeros(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb5-2">scatter_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> i, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> rank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> torch.zeros(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(world_size)]</span>
<span id="cb5-3"></span>
<span id="cb5-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Scatter list: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>scatter_list<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb5-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Before: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb5-6"></span>
<span id="cb5-7">dist.scatter(tensor, scatter_list, src<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb5-8"></span>
<span id="cb5-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"After: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
</div>
<div id="7e1bb653" class="cell" data-execution_count="7">
<div class="cell-output cell-output-stdout">
<pre><code>========== rank 0 ==========
Scatter list: [tensor([0, 1, 2]), tensor([3, 4, 5])]
Before: tensor([0., 0., 0.])
After: tensor([0, 1, 2])

========== rank 1 ==========
Scatter list: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]
Before: tensor([0., 0., 0.])
After: tensor([3, 4, 5])</code></pre>
</div>
</div>
</section>
</section>
<section id="all-point" class="level4">
<h4 class="anchored" data-anchor-id="all-point">All-&gt;Point</h4>
<section id="reduce" class="level5">
<h5 class="anchored" data-anchor-id="reduce">Reduce</h5>
<p>Reduce (<code>torch.distributed.reduce(tensor, dst, op, ...)</code>) performs a reduction operation (N-&gt;1, eg. sum, max, min, prod, …) and the <code>dst</code> rank receives the result.</p>
<div id="569607fe" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> rank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb7-2"></span>
<span id="cb7-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Before: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb7-4"></span>
<span id="cb7-5">dist.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">reduce</span>(tensor, dst<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, op<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dist.ReduceOp.SUM)</span>
<span id="cb7-6"></span>
<span id="cb7-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"After: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
</div>
<div id="d6f3a09a" class="cell" data-execution_count="9">
<div class="cell-output cell-output-stdout">
<pre><code>========== rank 0 ==========
Before: tensor([0, 1, 2])
After: tensor([3, 5, 7])

========== rank 1 ==========
Before: tensor([3, 4, 5])
After: tensor([3, 4, 5])</code></pre>
</div>
</div>
</section>
<section id="gather" class="level5">
<h5 class="anchored" data-anchor-id="gather">Gather</h5>
<p>Gather (<code>torch.distributed.gather(tensor, gather_list, dst, ...)</code>) <em>gathers</em> — pulls — a tensor, of the same size, from every rank and stores them in a list in a single rank.</p>
<div id="b4e60d0c" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> rank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb9-2">gather_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [torch.zeros(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(world_size)]</span>
<span id="cb9-3"></span>
<span id="cb9-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Before: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb9-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Before: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>gather_list<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb9-6"></span>
<span id="cb9-7">dist.gather(tensor, gather_list, dst<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb9-8"></span>
<span id="cb9-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"After: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>gather_list<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
</div>
<div id="6eb516c4" class="cell" data-execution_count="11">
<div class="cell-output cell-output-stdout">
<pre><code>========== rank 0 ==========
Before: tensor([0, 1, 2])
Before: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]
Before: tensor([3, 4, 5])

========== rank 1 ==========
Before: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]
After: [tensor([0, 1, 2]), tensor([3, 4, 5])]
After: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]</code></pre>
</div>
</div>
</section>
</section>
<section id="all-all" class="level4">
<h4 class="anchored" data-anchor-id="all-all">All-&gt;All</h4>
<section id="all-reduce" class="level5">
<h5 class="anchored" data-anchor-id="all-reduce">All-Reduce</h5>
<p>All-Reduce (<code>torch.distributed.all_reduce(tensor, op, ...)</code>) performs a <em>reduction</em> operation, like <code>reduce</code>, but every rank receives the result — rather than a single one with <code>reduce</code>. Think of it as <code>reduce</code> + <code>broadcast</code> — though it is optimized by techniques like ring-reduce.</p>
<div id="6b58596c" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> rank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb11-2"></span>
<span id="cb11-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Before: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb11-4"></span>
<span id="cb11-5">dist.all_reduce(tensor, op<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dist.ReduceOp.SUM)</span>
<span id="cb11-6"></span>
<span id="cb11-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"After: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
</div>
<div id="fb107516" class="cell" data-execution_count="13">
<div class="cell-output cell-output-stdout">
<pre><code>========== rank 0 ==========
Before: tensor([0, 1, 2])
After: tensor([3, 5, 7])

========== rank 1 ==========
Before: tensor([3, 4, 5])
After: tensor([3, 5, 7])</code></pre>
</div>
</div>
</section>
<section id="all-gather" class="level5">
<h5 class="anchored" data-anchor-id="all-gather">All-Gather</h5>
<p>All-Gather (<code>torch.distributed.all_gather(tensor, gather_list, ...)</code>) <em>gathers</em> — pulls — a tensor, of the same size, from every rank and stores them in a list in <em>every</em> rank. Think of it as running <code>gather</code> on all ranks.</p>
<div id="142dc5ac" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> rank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb13-2">gather_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [torch.zeros(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(world_size)]</span>
<span id="cb13-3"></span>
<span id="cb13-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Before: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb13-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Before: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>gather_list<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb13-6"></span>
<span id="cb13-7">dist.all_gather(tensor, gather_list)</span>
<span id="cb13-8"></span>
<span id="cb13-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"After: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>gather_list<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
</div>
<div id="1ca801b7" class="cell" data-execution_count="15">
<div class="cell-output cell-output-stdout">
<pre><code>========== rank 2 ==========
Before: tensor([0, 1, 2])
Before: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]
After: [tensor([0, 1, 2]), tensor([3, 4, 5])]

========== rank 1 ==========
Before: tensor([3, 4, 5])
Before: [tensor([0., 0., 0.]), tensor([0., 0., 0.])]
After: [tensor([0, 1, 2]), tensor([3, 4, 5])]</code></pre>
</div>
</div>
</section>
<section id="reduce-scatter" class="level5">
<h5 class="anchored" data-anchor-id="reduce-scatter">Reduce-Scatter</h5>
<p>Reduce-Scatter (<code>torch.distributed.reduce_scatter(output_tensor, input_list, op, ...)</code>) performs a <em>reduction</em> operation — like other <code>reduce</code> functions — and <em>scatters</em> the resulting tensor. Think of it like <code>reduce</code> + <code>scatter</code>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>It needs <code>len(input_list) == world_size</code> and every tensor in <code>input_list</code> to have the same shape of <code>output_tensor</code>.</p>
</div>
</div>
<div id="e65ffe56" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.zeros(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb15-2">scatter_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [torch.tensor([(rank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> i <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> (j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)]</span>
<span id="cb15-3"></span>
<span id="cb15-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Before: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb15-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Before: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>scatter_list<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb15-6"></span>
<span id="cb15-7">dist.reduce_scatter(tensor, scatter_list, op<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dist.ReduceOp.SUM)</span>
<span id="cb15-8"></span>
<span id="cb15-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"After: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>tensor<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
</div>
<div id="08b1f7ef" class="cell" data-execution_count="17">
<div class="cell-output cell-output-stdout">
<pre><code>========== rank 0 ==========
Before: tensor([0., 0.])
Before: [tensor([1, 2]), tensor([1, 4]), tensor([1, 8])]
After: tensor([ 6, 12])

========== rank 1 ==========
Before: tensor([0., 0.])
Before: [tensor([2, 4]), tensor([ 4, 16]), tensor([ 8, 64])]
After: tensor([14, 56])

========== rank 2 ==========
Before: tensor([0., 0.])
Before: [tensor([3, 6]), tensor([ 9, 36]), tensor([ 27, 216])]
After: tensor([ 36, 288])</code></pre>
</div>
</div>
</section>
</section>
<section id="other" class="level4">
<h4 class="anchored" data-anchor-id="other">Other</h4>
<section id="barrier" class="level5">
<h5 class="anchored" data-anchor-id="barrier">Barrier</h5>
<p>Barrier (<code>torch.distributed.barrier(...)</code>) synchronizes all ranks (processes), waiting for all of them to reach the barrier. (think of <code>.join()</code> for threads or processes)</p>
</section>
</section>
</section>
</section>
<section id="algorithms-techniques" class="level2">
<h2 class="anchored" data-anchor-id="algorithms-techniques">Algorithms / Techniques</h2>
<section id="data-sharding" class="level3">
<h3 class="anchored" data-anchor-id="data-sharding">Data Sharding</h3>
<p>Data Sharding is the process of sharding — splitting — the dataset / dataloader so that each rank only pulls their own unique mini-batches of the training data. This avoids duplicates and is more commucation / memory efficient that duplicating the same full dataset on every rank. To do this with torch, setup the <code>DataLoader</code> with <code>sampler=[instance of DistributedSampler]</code>.</p>
</section>
<section id="types-of-parallelism" class="level3">
<h3 class="anchored" data-anchor-id="types-of-parallelism">Types of parallelism</h3>
<p>The goal of parallelism is to maximize throughput and cluster utilization.</p>
<!--TODO: gradient accumulation somewhere -->
<!--TODO: activation checkpointing -->
<!--TODO: PP/TP/EP sections + link to journal 09-23 and 09-24 -->
<ul>
<li><strong>Data Parallelism</strong> (<strong>DP</strong>): Each rank has a replica of the model — they’re <strong>replicants</strong> — and receives a different mini-batch. After optional [Gradient Accumulation] , gradients are averaged across ranks (<code>all_reduce</code>).</li>
<li><strong>Pipeline Parallelism</strong> (<strong>PP</strong>): The model is split along the layers. Each rank has 1+ consecutive layers of the model, and we orchestrate sequential forward/backward passes along the ranks. This is <strong>inter-layer</strong> parallelism.</li>
<li><strong>Tensor Parallelism</strong> (<strong>TP</strong>): The model’s layers themselves are split across ranks. We need more complex orchestration since a single tensor’s values are scattered across different ranks. This is <strong>intra-layer</strong> parallelism.</li>
<li><strong>Expert Parallelism</strong> (<strong>EP</strong>): A specific type of <strong>TP</strong> where we only split the experts of an <strong>MoE</strong> across ranks.</li>
</ul>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>ZeRO/FSDP is not a parallelism strategy in the strict sense, but a memory-optimization strategy.</strong> It’s a highly memory-efficient form DP.</p>
<ul>
<li><strong>Parallelism</strong> = distributing <em>computation</em> to increase throughput.</li>
<li><strong>Memory optimization</strong> (eg. ZeRO/FSDP) = sharding model states (parameters, gradients, optimizer states) across ranks so the model fits in memory, while each rank still computes the full forward and backward pass.</li>
</ul>
<p>Thus:</p>
<ul>
<li>With <strong>ZeRO/FSDP</strong>, every rank executes the full network computation but stores only a shard of the model states.</li>
<li>With <strong>TP/EP/PP</strong>, computation itself is partitioned across ranks, and the combined work reconstructs the whole forward/backward pass.</li>
</ul>
<p>These approaches are complementary and usually combined in large-scale training.</p>
</div>
</div>
</section>
<section id="sec-data-parallelism" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-parallelism">DDP — Distributed Data Parallelism</h3>
<p>Distributed Data Parallelism is a type of parallelism where each <em>rank</em> loads a copy — replica — of the model, after each optimizer step they always all have the same parameters, they are <strong>replicants</strong>. Each rank then trains on a <em>different</em> mini-batch (hence the importance of <a href="../notes/distributed-training.html#data-sharding">data sharding</a>). We then average the gradients (<code>all_reduce</code> sum + division by world_size or avg operation if available), perform a step of gradient descent, rinse and repeat. If we <em>can</em> use this, we should, it has the least amount of overhead, but it requires that the model + optimizer states all fit in the device’s VRAM.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The difference between DDP and DP is that DDP uses processes to avoid the GIL and DP uses threads. Do not use DP, only DDP.</p>
</div>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> SimpleDataParaellism():</span>
<span id="cb17-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, model):</span>
<span id="cb17-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model</span>
<span id="cb17-4"></span>
<span id="cb17-5">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> param <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> model.parameters():</span>
<span id="cb17-6">            rank_0_params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> param.data.clone()</span>
<span id="cb17-7">            dist.broadcast(rank_0_params, src<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb17-8">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> torch.equal(param.data, rank_0_params), <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Parameters mismatch at initialization"</span></span>
<span id="cb17-9"></span>
<span id="cb17-10">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> sync_grad(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb17-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> param <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> model.parameters():</span>
<span id="cb17-12">            dist.all_reduce(param.grad, op<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dist.ReduceOp.AVG) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># only available on NCCL backend</span></span>
<span id="cb17-13">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># eq.</span></span>
<span id="cb17-14">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># dist.all_reduce(param.grad, op=dist.ReduceOp.SUM)</span></span>
<span id="cb17-15">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># param.grad /= dist.get_world_size()</span></span></code></pre></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The above is a Toy implementation, in reality you do <strong>not</strong> waste time and resources by doing a single <code>all_reduce</code> at the end. This leaves GPUs idle. You interleave computations and communications</p>
</div>
</div>
</section>
<section id="sec-zero-fsdp" class="level3">
<h3 class="anchored" data-anchor-id="sec-zero-fsdp">ZeRO / FSDP</h3>
<p><strong>Ze</strong>ro <strong>R</strong>edudency <strong>O</strong>ptimizer (ZeRO) by DeepSpeed is a modeling strategy involving sharding states and parameters during training as a mean of optimizing peak memory. The core idea is that the optimizer states, gradients and/or model parameters are sharded, retrieved only when necessary for some computation, then anything we do not use anymore is discarded.</p>
<p><strong>F</strong>ully <strong>S</strong>harded <strong>D</strong>ata <strong>P</strong>arallelism (FSDP) is PyTorch’s implementation of ZeRo.</p>
<p><a href="https://huggingface.co/papers/1910.02054" target="_blank">Paper</a> <a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/" target="_blank">Article</a> <a href="https://huggingface.co/papers/2304.11277" target="_blank">FSDP Paper</a> <a href="https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html" target="_blank">FSDP Doc</a></p>
<section id="zero-1" class="level4">
<h4 class="anchored" data-anchor-id="zero-1">ZeRO-1</h4>
<p>ZeRO stage 1 (aka. <img src="https://latex.codecogs.com/png.latex?P_%7Bos%7D">) is the sharding/partitioning of <strong>optimizer states</strong> only. 4x memory reduction, communication volume of the same order as DP (gradient all-reduce dominates).</p>
<p><strong>Forward pass</strong></p>
<ul>
<li>Same as DP: each rank stores the full model parameters and runs the full forward pass.</li>
</ul>
<p><strong>Backward pass</strong></p>
<ul>
<li>Same as DP: each rank computes all gradients locally.</li>
<li>Same as DP: gradients are averaged across ranks via <code>all_reduce</code>.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Can be a <code>reduce_scatter</code> too</p>
</div>
</div>
<p><strong>Optimizer step</strong></p>
<ul>
<li>Each rank holds the full parameters and full averaged gradients.</li>
<li>Each rank updates <strong>only the parameter shard</strong> corresponding to its shard of the optimizer state.</li>
<li>Updated parameter shards are then exchanged (<code>all_gather</code>) so all ranks end up with the full updated model.</li>
</ul>
</section>
<section id="zero-2" class="level4">
<h4 class="anchored" data-anchor-id="zero-2">ZeRO-2</h4>
<p>ZeRO stage 2 (aka. <img src="https://latex.codecogs.com/png.latex?P_%7Bos%7D%20+%20P_g">) is the sharding/partitioning of optimizer states <strong>and</strong> <strong>gradient</strong>. 8x memory reduction, communication volume of the same order as DP and ZeRO-1.</p>
<p><strong>Forward pass</strong></p>
<ul>
<li>Same as DP: each rank stores the full model parameters and runs the full forward pass.</li>
</ul>
<p><strong>Backward pass</strong></p>
<ul>
<li>Each rank computes gradients locally, so gradients are temporarily materialized on every rank. This means ZeRO-2 has the same <em>peak</em> memory as ZeRO-1, but 8x lower <em>persistent</em> memory.</li>
<li>Gradients are averaged and sharded across ranks (<code>reduce_scatter</code>) — think averaging + sending to each rank the shard of the gradients that corresponds <strong>exactly</strong> to its optimizer state</li>
</ul>
<p><strong>Optimizer step</strong></p>
<ul>
<li>Each rank holds the full parameters.</li>
<li>Each rank holds <strong>only the averaged gradients</strong> corresponding to its shard of the optimizer state.</li>
<li>Each rank updates <strong>only the parameter shard</strong> corresponding to its shard of the optimizer state.</li>
<li>Updated parameter shards are then exchanged (<code>all_gather</code>) so all ranks end up with the full updated model.</li>
</ul>
</section>
<section id="zero-3" class="level4">
<h4 class="anchored" data-anchor-id="zero-3">ZeRO-3</h4>
<p>ZeRO stage 3 (aka. <img src="https://latex.codecogs.com/png.latex?P_%7Bos%7D%20+%20P_g%20+%20P_p">) is the sharding/partitioning of optimizer states and gradient <strong>and</strong> <strong>model parameters</strong>. Memory reduction scales linearly with our parallelism degree, larger communication overhead (≈50% more than DP/ZeRO-1/2) — (need to <code>all_gather</code> and <code>reduce_scatter</code> parameters before and after every computation requiring them).</p>
<p>(Assuming FP16 params and FP32 optimizer states)</p>
<p><strong>Forward pass</strong></p>
<ul>
<li>Each rank stores its shard of the model parameters.</li>
<li>Whenever a parameter is needed for computation, it is materialized (<code>all_gather</code> from its shard)</li>
<li>The computation is done</li>
<li>The local param is released/flushed (<code>del</code>/<code>=None</code>) on every rank but the one owning it</li>
</ul>
<p><strong>Backward pass</strong></p>
<ul>
<li>Each rank runs the backward pass for its full model replica, but parameters must be all-gathered on demand.</li>
<li>Gradients are produced during backprop, then immediately reduce-scattered so only the owning rank keeps the shard.</li>
</ul>
<p><strong>Optimizer step</strong></p>
<ul>
<li>Each rank holds <strong>only the parameters’ shard</strong> corresponding to its shard of the gradients and optimizer state.</li>
<li>Each rank holds <strong>only the averaged gradients’ shard</strong> corresponding to its shard of the parameters and optimizer state.</li>
<li>Each rank updates <strong>only the parameter shard</strong> corresponding to its shard of the optimizer state.</li>
<li>Updated parameter shards are then exchanged (<code>all_gather</code>) so all ranks end up with the full updated model.</li>
</ul>
</section>
</section>
</section>
<section id="terminology" class="level2">
<h2 class="anchored" data-anchor-id="terminology">Terminology</h2>
<ul>
<li><strong>device</strong>: Hardware unit — GPU <code>"cuda:0"</code>, CPU <code>"cpu"</code> etc. <strong>that’s where tensors and computations live</strong></li>
<li><strong>node</strong>: Phyisical machine/server (or VPS whatever) that has 1+ devices</li>
<li><strong>process</strong>: Python process/worker, executing a copy of the code/script — often on a single device (GPU)</li>
<li><strong>rank</strong>: ID of a process — often that maps to a single device. <strong>rank</strong> without qualifiers is <strong>global rank</strong></li>
<li><strong>world</strong>: Set of all processes part of our current distributed job</li>
<li><strong>global</strong> rank, world rank: rank across all processes/nodes. <strong>note</strong>: collective operations take the <strong>global rank</strong> (or just <strong>rank</strong>) as input for <code>src</code>/<code>dst</code></li>
<li><strong>local rank</strong>: rank within a single node (<strong>node</strong> <em>not</em> group). <strong>note</strong>: <code>device</code> takes the <strong>local rank</strong> <code>"cuda:{local_rank}"</code></li>
<li><strong>group</strong>: subset of processes (1+ nodes) that we’ve grouped for sub-communications. <strong>note</strong>: we still use <strong>global rank</strong> for intra-group communication.</li>
</ul>
</section>
<section id="resources-references-bookmarks" class="level2">
<h2 class="anchored" data-anchor-id="resources-references-bookmarks">Resources / References / Bookmarks</h2>
<ul>
<li><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook" target="_blank">HF UltraScale Playbook</a></li>
<li><a href="https://docs.pytorch.org/docs/stable/distributed.html" target="_blank">torch.distributed doc</a>, RTFM</li>
</ul>


</section>

 ]]></description>
  <category>mle</category>
  <category>python</category>
  <guid>https://theopomies.com/notes/distributed-training.html</guid>
  <pubDate>Tue, 02 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Probability and Information Theory</title>
  <dc:creator>Theo POMIES</dc:creator>
  <link>https://theopomies.com/notes/probability.html</link>
  <description><![CDATA[ 





<section id="definitions-formulas" class="level2">
<h2 class="anchored" data-anchor-id="definitions-formulas">Definitions &amp; Formulas</h2>
<section id="outcomes-and-events" class="level3">
<h3 class="anchored" data-anchor-id="outcomes-and-events">Outcomes and Events</h3>
<p>When studying probability, we are performing experiments, random trials or observations. The set of all possible <em>outcomes</em> of this experiment is <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7B%5COmega%7D"> (or <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BS%7D">). eg. When rolling a die, <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7B%5COmega%7D%20=%20%5C%7B1,2,3,4,5,6%5C%7D">.</p>
<p>We can group these outcomes into <em>events</em> — <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BE%7D%20%5Csubseteq%20%5Cmathcal%7B%5COmega%7D">. eg. The event <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BE%7D%20=%20%5C%7B">die shows an even number<img src="https://latex.codecogs.com/png.latex?%5C%7D%20=%20%5C%7B2,%204,%206%5C%7D">. Whenever the outcome <img src="https://latex.codecogs.com/png.latex?z"> of the random experiment satisfies <img src="https://latex.codecogs.com/png.latex?z%20%5Cin%20%5Cmathcal%7BE%7D">, the event <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BE%7D"> has occurred. Multiple events can occur from the same outcome, say we have <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D%20=%20%5C%7B3,%206%5C%7D"> “the result is divisible by 3” and <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BB%7D%20=%20%5C%7B2,%204,%206%5C%7D">. <img src="https://latex.codecogs.com/png.latex?z%20=%206"> satisfies both <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BB%7D">.</p>
</section>
<section id="probability-function" class="level3">
<h3 class="anchored" data-anchor-id="probability-function">Probability function</h3>
<p>The <strong>probability function</strong> maps events onto a real value <img src="https://latex.codecogs.com/png.latex?P%5Ccolon%20%5Cmathcal%7BE%7D%20%5Csubseteq%20%5Cmathcal%7B%5COmega%7D%20%5Cto%20%5B0,%201%5D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(%5Cmathcal%7BE%7D)"> is the <em>probability associated with event <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BE%7D"></em>.</p>
<section id="properties" class="level4">
<h4 class="anchored" data-anchor-id="properties">Properties</h4>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(%5Cmathcal%7BE%7D)%20%5Cgeq%200"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(%5Cmathcal%7B%5COmega%7D)%20=%201,%20%5Coperatorname%7BP%7D(%5Cmathcal%7B%5Cemptyset%7D)%20=%200"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(%5Cmathcal%7BA%7D%20%5Ccup%20%5Cmathcal%7BB%7D)%20=%20%5Coperatorname%7BP%7D(%5Cmathcal%7BA%7D)%20+%20%5Coperatorname%7BP%7D(%5Cmathcal%7BB%7D)%20-%20%5Coperatorname%7BP%7D(%5Cmathcal%7BA%7D%20%5Ccap%20%5Cmathcal%7BB%7D)"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(%5Cbigcup_%7Bi=1%7D%5E%7B%5Cinfty%7D%20%5Cmathcal%7BA%7D_i)%20=%20%5Csum_%7Bi=1%7D%5E%7B%5Cinfty%7D%20%5Coperatorname%7BP%7D(%5Cmathcal%7BA%7D_i),%20%5Cquad%20%5Cmathcal%7BA%7D_i%20%5Ccap%20%5Cmathcal%7BA%7D_j%20=%20%5Cemptyset%5C:%20%5Ctext%7Bfor%20all%7D%5C:%20i%20%5Cneq%20j"> (= if all events <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D_i"> are <em>mutually exclusive</em>)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(%5Cmathcal%7BA%7D%20%5Ccap%20%5Cmathcal%7BB%7D)%20=%20%5Coperatorname%7BP%7D(%5Cmathcal%7BA%7D%20%5Cmid%20%5Cmathcal%7BB%7D)%5Coperatorname%7BP%7D(%5Cmathcal%7BB%7D)"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(%5Cmathcal%7BA%7D%20%5Ccap%20%5Cmathcal%7BB%7D)%20=%20%5Coperatorname%7BP%7D(%5Cmathcal%7BA%7D)%5Coperatorname%7BP%7D(%5Cmathcal%7BB%7D)%20%5Ciff%20%5Cmathcal%7BA%7D%20%5Cperp%20%5Cmathcal%7BB%7D"> (eg. 2 fair dice rolls)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D%20%5Cperp%20%5Cmathcal%7BB%7D%20%5Ciff%20%5Coperatorname%7BP%7D(%5Cmathcal%7BA%7D%20%5Cmid%20%5Cmathcal%7BB%7D)%20=%20%5Coperatorname%7BP%7D(%5Cmathcal%7BA%7D)"></li>
</ul>
</section>
</section>
<section id="sec-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="sec-random-variables">Random Variables</h3>
<p>A <strong>random variable</strong> <img src="https://latex.codecogs.com/png.latex?X"> is a measurable function (mapping) <img src="https://latex.codecogs.com/png.latex?X%20%5Ccolon%20%5Cmathcal%7B%5COmega%7D%20%5Cto%20%5Cmathcal%7BE%7D"> from a sample space <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7B%5COmega%7D"> as a set of possible outcomes to a measurable space <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BE%7D">.</p>
<p>The probability that <img src="https://latex.codecogs.com/png.latex?X"> takes on a value in a measurable set <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BS%7D%20%5Cin%20%5Cmathcal%7BE%7D"> is written as <img src="https://latex.codecogs.com/png.latex?%0A%5Coperatorname%7BP%7D(X%20%5Cin%20%5Cmathcal%7BS%7D)%20=%20%5Coperatorname%7BP%7D(%5C%7B%5Comega%20%5Cin%20%5Cmathcal%7B%5COmega%7D%20%5Cmid%20X(%5Comega)%20%5Cin%20%5Cmathcal%7BS%7D%5C%7D)%0A"></p>
<p>The probability that <img src="https://latex.codecogs.com/png.latex?X"> takes a <em>discrete</em> value <img src="https://latex.codecogs.com/png.latex?v">, denoted <img src="https://latex.codecogs.com/png.latex?X%20=%20v">, is <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(X=v)">.</p>
<p>Expressions like <img src="https://latex.codecogs.com/png.latex?X%20=%20v"> or <img src="https://latex.codecogs.com/png.latex?X%20%5Cgeq%20v"> define <strong>events</strong>, i.e., subsets of <img src="https://latex.codecogs.com/png.latex?%5COmega"> whose probability can be measured.</p>
<p>Random variables allow us to go from outcomes to values, like <img src="https://latex.codecogs.com/png.latex?X(%5Comega)%20=%20%5Comega">, the random variable that associates to each die its value (identity function). This is also an example of a <em>discrete</em> random variable.</p>
<p>When <img src="https://latex.codecogs.com/png.latex?X"> is <em>continuous</em>, it doesn’t make sense to have events like <img src="https://latex.codecogs.com/png.latex?X%20=%20v"> (and <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(X%20=%20v)%20=%200">); rather we use <img src="https://latex.codecogs.com/png.latex?v%20%5Cleq%20X%20%5Cleq%20w"> and probability <strong>densities</strong>. An example would be the height of a population. Probabilities are described via a <strong>probability density function</strong> <img src="https://latex.codecogs.com/png.latex?p_X(x)">, with <img src="https://latex.codecogs.com/png.latex?%0A%5Coperatorname%7BP%7D(v%20%5Cle%20X%20%5Cle%20w)%20=%20%5Cint_v%5Ew%20p_X(x)%5C,dx%0A"></p>
<p>We denote the probability distribution of <img src="https://latex.codecogs.com/png.latex?X"> as <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(X)"> (strictly speaking <img src="https://latex.codecogs.com/png.latex?P_X">, but we often write <img src="https://latex.codecogs.com/png.latex?P(X)"> for convenience).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>When the measurable space <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BE%7D"> is multi-dimensional, like <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Em">, we call the random variable <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Em"> a <strong>random vector</strong>.</p>
</div>
</div>
<section id="multiple-random-variables" class="level4">
<h4 class="anchored" data-anchor-id="multiple-random-variables">Multiple Random Variables</h4>
<p><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(A%20=%20a,%20B%20=%20b)"> is the <strong>joint probability</strong> of <img src="https://latex.codecogs.com/png.latex?A%20=%20a"> <strong>and</strong> <img src="https://latex.codecogs.com/png.latex?B%20=%20b"> (it’s the intersection of the events <img src="https://latex.codecogs.com/png.latex?A%20=%20a"> and <img src="https://latex.codecogs.com/png.latex?B%20=%20b">). Equivalently it’s <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(%5C%7BA%20=%20a%5C%7D%20%5Ccap%20%5C%7BB%20=%20b%5C%7D)">, with an overloaded notation, the <strong>joint probability distribution</strong> becomes <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(A,%20B)"></p>
<p>Obviously <img src="https://latex.codecogs.com/png.latex?%20%5Coperatorname%7BP%7D(A%20=%20a,%20B%20=%20b)%20%5Cleq%20%5Coperatorname%7BP%7D(A=a)%20%5Cquad%20%5Ctext%7Band%7D%20%5Cquad%20%5Coperatorname%7BP%7D(A%20=%20a,%20B%20=%20b)%20%5Cleq%20%5Coperatorname%7BP%7D(B=b)%20"></p>
<p>Also, we can <strong>marginalize</strong> <img src="https://latex.codecogs.com/png.latex?%20%5Coperatorname%7BP%7D(A%20=%20a)%20=%20%5Csum_v%20%5Coperatorname%7BP%7D(A%20=%20a,%20B%20=%20v)%20"></p>
<p>Because <img src="https://latex.codecogs.com/png.latex?A%20=%20a"> and <img src="https://latex.codecogs.com/png.latex?B%20=%20b"> are events, <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%5Coperatorname%7BP%7D(A%20=%20a,%20B%20=%20b)%20&amp;%20=%20%5Coperatorname%7BP%7D(A%20=%20a%20%5Cmid%20B%20=%20b)%5Coperatorname%7BP%7D(B%20=%20b)%20%5C%5C%0A%5Ciff%20%5Coperatorname%7BP%7D(A%20=%20a%20%5Cmid%20B%20=%20b)%20&amp;%20=%20%5Coperatorname%7BP%7D(A%20=%20a,%20B%20=%20b)/%5Coperatorname%7BP%7D(B%20=%20b)%0A%5Cend%7Baligned%7D"></p>
</section>
</section>
<section id="bayes-theorem" class="level3">
<h3 class="anchored" data-anchor-id="bayes-theorem">Bayes’ Theorem</h3>
<p>From the properties and definitions above, we can derive the following formula</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Coverbrace%7B%5Coperatorname%7BP%7D(A%20%5Cmid%20B)%7D%5E%7B%5Ctext%7Bposterior%20probability%7D%7D%20=%20%5Cdfrac%7B%5Coverbrace%7B%5Coperatorname%7BP%7D(B%20%5Cmid%20A)%7D%5E%7B%5Ctext%7Blikelihood%7D%7D%5Coverbrace%7B%5Coperatorname%7BP%7D(A)%7D%5E%7B%5Ctext%7Bprior%7D%7D%7D%7B%5Cunderbrace%7B%5Coperatorname%7BP%7D(B)%7D_%7B%5Ctext%7Bobservation%7D%7D%7D%20"></p>
<ul>
<li>prior/hypothesis: our estimate or current belief about the probability of <img src="https://latex.codecogs.com/png.latex?A"></li>
<li>observation/marginal likelihood/evidence: the evidence or observations we’ve made regarding <img src="https://latex.codecogs.com/png.latex?B"></li>
<li>likelihood: a measure of how compatible our hypothesis is with our observation</li>
</ul>
<p>A simplified version is <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(A%20%5Cmid%20B)%20%5Cpropto%20%5Coperatorname%7BP%7D(B%20%5Cmid%20A)%5Coperatorname%7BP%7D(A)"></p>
</section>
<section id="expectation" class="level3">
<h3 class="anchored" data-anchor-id="expectation">Expectation</h3>
<p>The <strong>expectation</strong> (or <strong>expected value</strong>) is the weighted average of the values of <img src="https://latex.codecogs.com/png.latex?X">.</p>
<p>Discrete case:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Coperatorname%7BE%7D%5BX%5D%20=%20%5Coperatorname%7BE%7D_%7BX%20%5Csim%20P%7D%5BX%5D%20=%20%5Csum_x%20x%5Coperatorname%7BP%7D(X=x)%20"></p>
<p>Continuous case:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BE%7D%5BX%5D%20=%20%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D%20x%20p(x)%20%5C;dx%20"></p>
<p>To follow mathematical notation, sometimes we use <img src="https://latex.codecogs.com/png.latex?%5Cmu"> to denote this average.</p>
<section id="properties-1" class="level4">
<h4 class="anchored" data-anchor-id="properties-1">Properties</h4>
<ul>
<li>Linearity: <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BE%7D%5B%5Calpha%20A%20+%20B%5D%20=%20%5Calpha%20%5Coperatorname%7BE%7D%5BA%5D%20+%20%5Coperatorname%7BE%7D%5BB%5D"></li>
<li>Equality: <img src="https://latex.codecogs.com/png.latex?X%20=%20Y%20%5C;%20%5Ctext%7Ba.s.%7D%20%5Cimplies%20%5Coperatorname%7BE%7D%5BX%5D%20=%20%5Coperatorname%7BE%7D%5BY%5D"></li>
<li>Constants: <img src="https://latex.codecogs.com/png.latex?X%20=%20c%20%5Cimplies%20%5Coperatorname%7BE%7D%5BX%5D%20=%20c"></li>
<li>Tower property: <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BE%7D%5B%5Coperatorname%7BE%7D%5BX%5D%5D%20=%20%5Coperatorname%7BE%7D%5BX%5D"></li>
</ul>
</section>
<section id="expectation-of-a-random-vector" class="level4">
<h4 class="anchored" data-anchor-id="expectation-of-a-random-vector">Expectation of a Random Vector</h4>
<p>For a vector-valued random variable — ie. the <strong>random vector</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En">, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Cmu%7D%20=%20%5Coperatorname%7BE%7D_%7B%5Cmathbf%7BX%7D%20%5Csim%20P%7D%5B%5Cmathbf%7BX%7D%5D"> with <img src="https://latex.codecogs.com/png.latex?%5Cmu_i%20=%20%5Coperatorname%7BE%7D_%7B%5Cmathbf%7BX%7D%20%5Csim%20P%7D%5Bx_i%5D"> — the expectation of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> is a vector of the expectations of each element <img src="https://latex.codecogs.com/png.latex?x_i"> of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">.</p>
</section>
</section>
<section id="sec-variance" class="level3">
<h3 class="anchored" data-anchor-id="sec-variance">Variance</h3>
<p>The <strong>variance</strong> is a measure of dispersion, it quantifies <em>how much values deviate from their expectation, on average</em>. The variance is the expectation of the squared difference between the values and the expected value.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Coperatorname%7BVar%7D(X)%20=%20%5Coperatorname%7BE%7D%5B(X%20-%20%5Coperatorname%7BE%7D%5BX%5D)%5E2%5D%20=%20%5Coperatorname%7BE%7D%5BX%5E2%5D%20-%20(%5Coperatorname%7BE%7D%5BX%5D)%5E2%20"></p>
<p>Because</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Coperatorname%7BE%7D%5BX%5E2%20-%202X%5Coperatorname%7BE%7D%5BX%5D%20+%20%5Coperatorname%7BE%7D%5BX%5D%5E2%5D%20=%20%5Coperatorname%7BE%7D%5BX%5E2%5D%20-%202(%5Coperatorname%7BE%7D%5BX%5D)%5E2%20+%20(%5Coperatorname%7BE%7D%5BX%5D)%5E2%20"></p>
<section id="sec-var-random-vector" class="level4">
<h4 class="anchored" data-anchor-id="sec-var-random-vector">Variance of a Random Vector</h4>
<p>For a random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">, we store the pairwise <strong>variances</strong> of elements and <strong>covariances</strong> in a <strong>covariance matrix</strong> (aka. auto-covariance matrix or variance matrix) noted <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CSigma%7D"> or <img src="https://latex.codecogs.com/png.latex?K_%7B%5Cmathbf%7Bx%7D%5Cmathbf%7Bx%7D%7D"> or <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCov%7D_%7B%5Cmathbf%7Bx%7D%20%5Csim%20P%7D">, defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbf%7B%5CSigma%7D%20=%20%5Coperatorname%7BE%7D_%7B%5Cmathbf%7BX%7D%20%5Csim%20P%7D%5B(%5Cmathbf%7BX%7D%20-%20%5Cmathbf%7B%5Cmu%7D)(%5Cmathbf%7BX%7D%20-%20%5Cmathbf%7B%5Cmu%7D)%5E%5Ctop%5D%20"> <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%5Cmathbf%7B%5CSigma%7D%20=%20K_%7B%5Cmathbf%7BX%7D%5Cmathbf%7BY%7D%7D%0A&amp;%20=%20%5Coperatorname%7BE%7D_%7B%5Cmathbf%7BX%7D%20%5Csim%20P%7D%5B(%5Cmathbf%7BX%7D%20-%20%5Cmathbf%7B%5Cmu%7D)(%5Cmathbf%7BX%7D%20-%20%5Cmathbf%7B%5Cmu%7D)%5E%5Ctop%5D%20%5C%5C%0A&amp;%20=%20%5Coperatorname%7BE%7D%5B%5Cmathbf%7BX%7D%5Cmathbf%7BX%7D%5E%5Ctop%5D%20-%20%5Coperatorname%7BE%7D%5B%5Cmathbf%7BX%7D%5D%5Coperatorname%7BE%7D%5B%5Cmathbf%7BX%7D%5D%5E%5Ctop%0A%5Cend%7Baligned%7D"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Each entry <img src="https://latex.codecogs.com/png.latex?%5CSigma_%7Bi,%20j%7D%20=%20%5Coperatorname%7BCov%7D(X_i,%20X_j)"> — see covariance), and by definition, for diagonal entries <img src="https://latex.codecogs.com/png.latex?%5CSigma_%7Bi,%20i%7D%20=%20%5Coperatorname%7BCov%7D(X_i,%20X_i)%20=%20%5Coperatorname%7BVar%7D(X_i)"></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>We have the following property when applying a linear transformation represented by the appropriately dimensioned matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Coperatorname%7BCov%7D(%5Cmathbf%7BAX%7D,%20%5Cmathbf%7BAX%7D)%20&amp;%20=%20%5Coperatorname%7BE%7D%5B(%5Cmathbf%7BAX%7D%20-%20%5Coperatorname%7BE%7D%5B%5Cmathbf%7BAX%7D%5D)(%5Cmathbf%7BAX%7D%20-%20%5Coperatorname%7BE%7D%5B%5Cmathbf%7BAX%7D%5D)%5E%5Ctop%5D%20%5C%5C%0A&amp;%20=%20%5Coperatorname%7BE%7D%5B%5Cmathbf%7BAX%7D(%5Cmathbf%7BAX%7D)%5E%5Ctop%5D%20-%20%5Coperatorname%7BE%7D%5B%5Cmathbf%7BAX%7D%5D%5Coperatorname%7BE%7D%5B(%5Cmathbf%7BAX%7D)%5E%5Ctop%5D%20%5C%5C%0A&amp;%20=%20%5Coperatorname%7BE%7D%5B%5Cmathbf%7BAX%7D%5Cmathbf%7BX%7D%5E%5Ctop%5Cmathbf%7BA%7D%5E%5Ctop%5D%20-%20%5Coperatorname%7BE%7D%5B%5Cmathbf%7BAX%7D%5D%5Coperatorname%7BE%7D%5B%5Cmathbf%7BX%7D%5E%5Ctop%5Cmathbf%7BA%7D%5E%5Ctop%5D%20%5C%5C%0A&amp;%20=%20%5Cmathbf%7BA%7D%5Coperatorname%7BE%7D%5B%5Cmathbf%7BX%7D%5Cmathbf%7BX%7D%5E%5Ctop%5D%5Cmathbf%7BA%7D%5E%5Ctop%20-%20%5Cmathbf%7BA%7D%5Coperatorname%7BE%7D%5B%5Cmathbf%7BX%7D%5D%5Coperatorname%7BE%7D%5B%5Cmathbf%7BX%7D%5E%5Ctop%5D%5Cmathbf%7BA%7D%5E%5Ctop%20%5C%5C%0A&amp;%20=%20%5Cmathbf%7BA%7D(%5Coperatorname%7BE%7D%5B%5Cmathbf%7BX%7D%5Cmathbf%7BX%7D%5E%5Ctop%5D%20-%20%5Coperatorname%7BE%7D%5B%5Cmathbf%7BX%7D%5D%5Coperatorname%7BE%7D%5B%5Cmathbf%7BX%7D%5E%5Ctop%5D)%5Cmathbf%7BA%7D%5E%5Ctop%20%5C%5C%0A&amp;%20=%20%5Cmathbf%7BA%7D%5Cmathbf%7B%5CSigma%7D%5Cmathbf%7BA%7D%5E%5Ctop%0A%5Cend%7Baligned%7D%0A"></p>
<!---TODO--->
<p>As a result of the [linearity of expectation]</p>
</div>
</div>
</section>
</section>
<section id="standard-deviation" class="level3">
<h3 class="anchored" data-anchor-id="standard-deviation">Standard deviation</h3>
<p>Because the variance is a squared difference, we can take its square root to get the <strong>standard deviation</strong> which has the benefit of being in the same unit as our random variable.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Coperatorname%7BVar%7D(X)%20=%20%5Csigma%5E2_X%20%5Ciff%20%5Csigma_X%20=%20%5Csqrt%7B%5Coperatorname%7BVar%7D(X)%7D%20"></p>
</section>
<section id="sec-covariance" class="level3">
<h3 class="anchored" data-anchor-id="sec-covariance">Covariance</h3>
<p><strong>Covariance</strong> is a measure of the joint variability of two random variables.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Coperatorname%7BCov%7D(X,%20Y)%20=%20%5Coperatorname%7BE%7D%5B(X%20-%20%5Coperatorname%7BE%7D%5BX%5D)(Y%20-%20%5Coperatorname%7BE%7D%5BY%5D)%5D%20"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCov%7D(X,%20X)%20=%20%5Coperatorname%7BE%7D%5B(X%20-%20%5Coperatorname%7BE%7D%5BX%5D)%5E2%5D%20=%20%5Coperatorname%7BVar%7D(X)"></p>
</div>
</div>
<section id="sec-covariance-matrix" class="level4">
<h4 class="anchored" data-anchor-id="sec-covariance-matrix">Covariance Matrix of two Random Vectors</h4>
<p>For random vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Em">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BY%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En">, the <strong>covariance matrix</strong> is a matrix <img src="https://latex.codecogs.com/png.latex?K_%7B%5Cmathbf%7BX%7D%5Cmathbf%7BY%7D%7D"> defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0AK_%7B%5Cmathbf%7BX%7D%5Cmathbf%7BY%7D%7D%20&amp;%20=%20%5Coperatorname%7BE%7D%5B(%5Cmathbf%7BX%7D%20-%20%5Coperatorname%7BE%7D%5B%5Cmathbf%7BX%7D%5D)(%5Cmathbf%7BY%7D%20-%20%5Coperatorname%7BE%7D%5B%5Cmathbf%7BY%7D%5D)%5E%5Ctop%5D%20%5C%5C%0A&amp;%20=%20%5Coperatorname%7BE%7D%5B%5Cmathbf%7BX%7D%5Cmathbf%7BY%7D%5E%5Ctop%5D%20-%20%5Coperatorname%7BE%7D%5B%5Cmathbf%7BX%7D%5D%5Coperatorname%7BE%7D%5B%5Cmathbf%7BY%7D%5D%5E%5Ctop%0A%5Cend%7Baligned%7D"></p>
<p>We have <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCov%7D(X_i,%20Y_j)%20=%20K_%7BX_iY_j%7D%20=%20%5Coperatorname%7BE%7D%5B(X_i%20-%20%5Coperatorname%7BE%7D%5BX_i%5D)(Y_j%20-%20%5Coperatorname%7BE%7D%5BY_j%5D)%5D"> found at index <img src="https://latex.codecogs.com/png.latex?(i,%20j)"> in <img src="https://latex.codecogs.com/png.latex?K_%7B%5Cmathbf%7BX%7D%5Cmathbf%7BY%7D%7D"></p>
<p>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%20=%20%5Cmathbf%7BY%7D"> this is the auto-covariance matrix or variance matrix of this random vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"></p>
<p>If <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D%20%5Cneq%20%5Cmathbf%7BY%7D"> this is the <strong>cross-covariance matrix</strong> of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BY%7D"></p>
<!--- TODO: correlation, KL-Divergence --->
</section>
</section>
</section>
<section id="proofs" class="level2">
<h2 class="anchored" data-anchor-id="proofs">Proofs</h2>
<p>Later!</p>
</section>
<section id="notation" class="level2">
<h2 class="anchored" data-anchor-id="notation">Notation</h2>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D">: a set</li>
<li><img src="https://latex.codecogs.com/png.latex?%5C%7Ba,%20b,%20c%5C%7D">: a set, with its elements</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cemptyset">: the empty set</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D%20%5Csubset%20%5Cmathcal%7BB%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D%20%5Csubsetneq%20%5Cmathcal%7BB%7D">: <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> is a proper/strict subset of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BB%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D%20%5Csubseteq%20%5Cmathcal%7BB%7D">: <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> is a subest of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BB%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D%20%5Ccap%20%5Cmathcal%7BB%7D">: the intersection of sets <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BB%7D"> — “<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> <strong>and</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BB%7D">”</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D%20%5Ccup%20%5Cmathcal%7BB%7D">: the union of sets <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BB%7D"> — “<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> <strong>or</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BB%7D">”</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D%20%5Csetminus%20%5Cmathcal%7BB%7D">: set subtraction of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BB%7D"> from <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D">, elements from <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> but not in <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BB%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BS%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7B%5COmega%7D">: the sample space / universe (the set of all possible outcomes)</li>
<li><img src="https://latex.codecogs.com/png.latex?%7C%5Cmathcal%7BX%7D%7C">: the cardinality of set <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D"> (its number of events)</li>
<li><img src="https://latex.codecogs.com/png.latex?X">: a random variable</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">: a random vector</li>
<li><img src="https://latex.codecogs.com/png.latex?P">: a probability distribution</li>
<li><img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20P">: the random variable <img src="https://latex.codecogs.com/png.latex?X"> follows the probability distribution <img src="https://latex.codecogs.com/png.latex?P"></li>
<li><img src="https://latex.codecogs.com/png.latex?a%20%5Cpropto%20b">: <img src="https://latex.codecogs.com/png.latex?a"> is proportional to <img src="https://latex.codecogs.com/png.latex?b">, eg. <img src="https://latex.codecogs.com/png.latex?a%20=%20kb"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(%5Ccdot)">: the probability function, maps events to their probability and random variables to their probability <em>distributions</em></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(X)">: depending on the context, a probability distribution <em>or</em> the probability of any <img src="https://latex.codecogs.com/png.latex?X=x">, meaning the formula is true for any value</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(X=x)">: the probability assigned to the event where random variable <img src="https://latex.codecogs.com/png.latex?X"> takes value <img src="https://latex.codecogs.com/png.latex?x"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D(X%20%5Cmid%20Y)">: the conditional probability distribution of <img src="https://latex.codecogs.com/png.latex?X"> given <img src="https://latex.codecogs.com/png.latex?Y"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bp%7D(%5Ccdot)">: a probability density function (PDF) associated with distribution <img src="https://latex.codecogs.com/png.latex?P"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BE%7D%5BX%5D">: expectation of a random variable <img src="https://latex.codecogs.com/png.latex?X"></li>
<li><img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20Y">: random variables <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are independent</li>
<li><img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20Y%20%5Cmid%20Z">: random variables <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are conditionally independent given <img src="https://latex.codecogs.com/png.latex?Z"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Csigma_X">: standard deviation of random variable <img src="https://latex.codecogs.com/png.latex?X"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BVar%7D(X)">: variance of random variable <img src="https://latex.codecogs.com/png.latex?X">, equal to <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2_X"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCov%7D(X,%20Y)">: covariance of random variables <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7B%5Crho%7D(X,%20Y)">: the Pearson correlation coefficient between <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">, equals <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Coperatorname%7BCov%7D(X,%20Y)%7D%7B%5Csigma_X%20%5Csigma_Y%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BH%7D(X)">: entropy of random variable <img src="https://latex.codecogs.com/png.latex?X"></li>
<li><img src="https://latex.codecogs.com/png.latex?D_%7B%5Coperatorname%7BKL%7D%7D(P%5C%7CQ)">: the KL-divergence (or relative entropy) from distribution <img src="https://latex.codecogs.com/png.latex?Q"> to distribution <img src="https://latex.codecogs.com/png.latex?P"></li>
</ul>


</section>

 ]]></description>
  <category>math</category>
  <guid>https://theopomies.com/notes/probability.html</guid>
  <pubDate>Tue, 02 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Calculus</title>
  <dc:creator>Theo POMIES</dc:creator>
  <link>https://theopomies.com/notes/calculus.html</link>
  <description><![CDATA[ 





<section id="definitions-formulas" class="level2">
<h2 class="anchored" data-anchor-id="definitions-formulas">Definitions &amp; Formulas</h2>
<section id="differentiation" class="level3">
<h3 class="anchored" data-anchor-id="differentiation">Differentiation</h3>
<p>As the name states, we analyze differences in the output(s) of a function based on differences in the input(s) <img src="https://latex.codecogs.com/png.latex?%20%5Cdfrac%7Bdy%7D%7Bdx%7D%20=%20%5Clim_%7Bh%20%5Crightarrow%200%7D%20%5Cdfrac%7Bf(x%20+%20h)%20-%20f(x)%7D%7Bh%7D%20"></p>
<p>A function is said to be differentiable at <img src="https://latex.codecogs.com/png.latex?x"> if this limit exists, and differentiable on an interval if it exists at any <img src="https://latex.codecogs.com/png.latex?x"> in this interval.</p>
<section id="common-derivatives" class="level4">
<h4 class="anchored" data-anchor-id="common-derivatives">Common Derivatives</h4>
<p>From that we can get common derivatives</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cbegin%7Baligned%7D%0A%20%20%20%20%5Cdfrac%7Bd%7D%7Bdx%7DC%20&amp;%20=%200%20&amp;&amp;%20%5Ctext%7Bfor%20any%20constant%20C%7D%20%5C%5C%0A%20%20%20%20%5Cdfrac%7Bd%7D%7Bdx%7Dx%5En%20&amp;%20=%20nx%5E%7Bn%20-%201%7D%20&amp;&amp;%20%5Ctext%7Bfor%20n%7D%20%5Cneq%200%20%5C%5C%0A%20%20%20%20%5Cdfrac%7Bd%7D%7Bdx%7De%5Ex%20&amp;%20=%20e%5Ex%20%5C%5C%0A%20%20%20%20%5Cdfrac%7Bd%7D%7Bdx%7D%5Cln%20x%20&amp;%20=%20x%5E%7B-1%7D%20%5C%5C%0A%20%20%20%20%5Cdfrac%7Bd%7D%7Bdx%7Da%5Ex%20&amp;%20=%20%5Cln(a)a%5Ex%20%5C%5C%0A%20%20%20%20%5Cdfrac%7Bd%7D%7Bdx%7D%5Ccos%20x%20&amp;%20=%20-%5Csin%20x%20%5C%5C%0A%20%20%20%20%5Cdfrac%7Bd%7D%7Bdx%7D%5Csin%20x%20&amp;%20=%20%5Ccos%20x%20%5C%5C%0A%5Cend%7Baligned%7D%20"></p>
<p>From these it becomes trivial to derive <img src="https://latex.codecogs.com/png.latex?%5Ctan">, <img src="https://latex.codecogs.com/png.latex?%5Csec">, <img src="https://latex.codecogs.com/png.latex?%5Ccsc"> and <img src="https://latex.codecogs.com/png.latex?%5Ccot">.</p>
</section>
<section id="derivation-rules" class="level4">
<h4 class="anchored" data-anchor-id="derivation-rules">Derivation Rules</h4>
<p>Finally a composition of differentiable functions is also differentiable, so we have the following rules that allow us to derive almost any function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cbegin%7Baligned%7D%0A%20%20%20%20%5Cdfrac%7Bd%7D%7Bdx%7DCf(x)%20&amp;%20=%20C%5Cdfrac%7Bd%7D%7Bdx%7Df(x)%20&amp;&amp;%20%5Ctext%7BConstant%20multiple%20rule%7D%20%5C%5C%0A%20%20%20%20%5Cdfrac%7Bd%7D%7Bdx%7D%5Bf(x)%20+%20g(x)%5D%20&amp;%20=%20%5Cdfrac%7Bd%7D%7Bdx%7Df(x)%20+%20%5Cdfrac%7Bd%7D%7Bdx%7Dg(x)%20&amp;&amp;%20%5Ctext%7BSum%20rule%7D%20%5C%5C%0A%20%20%20%20%5Cdfrac%7Bd%7D%7Bdx%7D%5Bf(x)g(x)%5D%20&amp;%20=%20%5Cdfrac%7Bd%7D%7Bdx%7Df(x)g(x)%20+%20f(x)%5Cdfrac%7Bd%7D%7Bdx%7Dg(x)%20&amp;&amp;%20%5Ctext%7BProduct%20rule%7D%20%5C%5C%0A%20%20%20%20%5Cdfrac%7Bdy%7D%7Bdx%7D%20&amp;%20=%20%5Cdfrac%7Bdy%7D%7Bdz%7D%5Cdfrac%7Bdz%7D%7Bdx%7D%20=%20%5Cdfrac%7B%5Cfrac%7Bdy%7D%7Bdz%7D%7D%7B%5Cfrac%7Bdx%7D%7Bdz%7D%7D%20&amp;&amp;%20%5Ctext%7BChain%20rule%7D%20%5C%5C%0A%5Cend%7Baligned%7D%20"></p>
<p>From these we can easily find a Quotient Rule, a Power Rule and a Reciprocal Rule:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cbegin%7Baligned%7D%0A%20%20%20%20%5Cdfrac%7Bd%7D%7Bdx%7D%5Cdfrac%7Bf(x)%7D%7Bg(x)%7D%20&amp;%20=%20%5Cdfrac%7B%5Cfrac%7Bd%7D%7Bdx%7Df(x)g(x)%20-%20f(x)%5Cfrac%7Bd%7D%7Bdx%7Dg(x)%7D%7Bg(x)%5E2%7D%20&amp;&amp;%20%5Ctext%7BQuotient%20rule%7D%20%5C%5C%0A%20%20%20%20%5Cdfrac%7Bd%7D%7Bdx%7Df(x)%5En%20&amp;%20=%20nf(x)%5E%7Bn-1%7D%5Cdfrac%7Bd%7D%7Bdx%7Df(x)%20&amp;&amp;%20%5Ctext%7BPower%20rule%7D%20%5C%5C%0A%20%20%20%20%5Cdfrac%7Bdy%7D%7Bdx%7D%5Cdfrac%7B1%7D%7Bf(x)%7D%20&amp;%20=%20-%5Cdfrac%7B%5Cfrac%7Bd%7D%7Bdx%7Df(x)%7D%7Bf(x)%5E2%7D%20&amp;&amp;%20%5Ctext%7BReciprocal%20rule%7D%20%5C%5C%0A%5Cend%7Baligned%7D%20"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Because of the definition of derivative as a rate of change, this is possible <img src="https://latex.codecogs.com/png.latex?%5Cdfrac%7Bdy%7D%7Bdx%7D%20=%20%5Cdfrac%7B1%7D%7B%5Cfrac%7Bdx%7D%7Bdy%7D%7D"></p>
</div>
</div>
</section>
</section>
<section id="multivariate-calculus" class="level3">
<h3 class="anchored" data-anchor-id="multivariate-calculus">Multivariate Calculus</h3>
<section id="sec-gradient" class="level4">
<h4 class="anchored" data-anchor-id="sec-gradient">Gradient</h4>
<p>Very similar to univariate calculus, but now our function takes a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En"> as input and returns a scalar <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathbb%7BR%7D">.</p>
<p>To paraphrase D2L because their explanation is perfect:</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?y%20=%20f(x_1,%20x_2,%20%5Cldots,%20x_n)"> be a function with <img src="https://latex.codecogs.com/png.latex?n"> variables. The <strong>partial derivative</strong> of <img src="https://latex.codecogs.com/png.latex?y"> with respect to its <img src="https://latex.codecogs.com/png.latex?i%5E%5Ctextrm%7Bth%7D"> parameter <img src="https://latex.codecogs.com/png.latex?x_i"> is</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cdfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x_i%7D%20=%20%5Clim_%7Bh%20%5Crightarrow%200%7D%20%5Cfrac%7Bf(x_1,%20%5Cldots,%20x_%7Bi-1%7D,%20x_i+h,%20x_%7Bi+1%7D,%20%5Cldots,%20x_n)%20-%20f(x_1,%20%5Cldots,%20x_i,%20%5Cldots,%20x_n)%7D%7Bh%7D."></p>
<p>For <img src="https://latex.codecogs.com/png.latex?f%20%5Ccolon%20%5Cmathbb%7BR%7D%5En%20%5Cto%20%5Cmathbb%7BR%7D">, we collect/concatenate all the partial derivatives to obtain the <strong>gradient</strong> of the output <img src="https://latex.codecogs.com/png.latex?y%20=%20f(%5Cmathbf%7Bx%7D)"> with respect to the input <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> <img src="https://latex.codecogs.com/png.latex?%20%5Cnabla_%7B%5Cmathbf%7Bx%7D%7Df(%5Cmathbf%7Bx%7D)%20=%20%5Cnabla_%7B%5Cmathbf%7Bx%7D%7Dy%20=%5Cbegin%7Bbmatrix%7D%20%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x_1%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x_n%7D%20%5Cend%7Bbmatrix%7D%20"> sometimes written <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20f(%5Cmathbf%7Bx%7D)"> or <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20y"> when not ambiguous.</p>
</section>
<section id="sec-jacobian" class="level4">
<h4 class="anchored" data-anchor-id="sec-jacobian">Jacobian</h4>
<p>The <strong>Jacobian</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BJ%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D"> is a generalization of the gradient to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%20=%20f(%5Cmathbf%7Bx%7D)"> with <img src="https://latex.codecogs.com/png.latex?f%20%5Ccolon%20%5Cmathbb%7BR%7D%5En%20%5Cto%20%5Cmathbb%7BR%7D%5Em">, where <img src="https://latex.codecogs.com/png.latex?j_%7Bi,j%7D%20=%20%5Cdfrac%7B%5Cpartial%20y_i%7D%7B%5Cpartial%5Cmathbf%7Bx%7D_j%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20%5Cvdots%20%5C%5C%20x_n%20%5Cend%7Bbmatrix%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%20=%20%5Cbegin%7Bbmatrix%7D%20y_1%20%5C%5C%20%5Cvdots%20%5C%5C%20y_m%20%5Cend%7Bbmatrix%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Em">.</p>
<p>Explicitly <img src="https://latex.codecogs.com/png.latex?%20%5Cdisplaystyle%7B%20%5Cmathbf%7BJ%7D%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cdfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7B1%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cdfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20x_%7Bn%7D%7D%20%5Cend%7Bbmatrix%7D%0A=%20%5Cbegin%7Bbmatrix%7D%20%5Cnabla%5E%7B%5Ctop%7Dy_%7B1%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cnabla%5E%7B%5Ctop%7Dy_%7Bm%7D%20%5Cend%7Bbmatrix%7D%0A=%20%5Cbegin%7Bbmatrix%7D%0A%5Cdfrac%7B%5Cpartial%20y_%7B1%7D%7D%7B%5Cpartial%20x_%7B1%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cdfrac%7B%5Cpartial%20y_%7B1%7D%7D%7B%5Cpartial%20x_%7Bn%7D%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A%5Cdfrac%7B%5Cpartial%20y_%7Bm%7D%7D%7B%5Cpartial%20x_%7B1%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cdfrac%7B%5Cpartial%20y_%7Bm%7D%7D%7B%5Cpartial%20x_%7Bn%7D%7D%0A%5Cend%7Bbmatrix%7D%7D%20"></p>
</section>
<section id="handy-rules" class="level4">
<h4 class="anchored" data-anchor-id="handy-rules">Handy Rules</h4>
<p>The following rules come straight from D2L:</p>
<ul>
<li>For all <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D"> we have <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Cmathbf%7Bx%7D%7D%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D%20=%20%5Cmathbf%7BA%7D%5E%5Ctop"> and <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Cmathbf%7Bx%7D%7D%20%5Cmathbf%7Bx%7D%5E%5Ctop%20%5Cmathbf%7BA%7D%20%20=%20%5Cmathbf%7BA%7D">.</li>
<li>For square matrices <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20n%7D"> we have that <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Cmathbf%7Bx%7D%7D%20%5Cmathbf%7Bx%7D%5E%5Ctop%20%5Cmathbf%7BA%7D%20%5Cmathbf%7Bx%7D%20%20=%20(%5Cmathbf%7BA%7D%20+%20%5Cmathbf%7BA%7D%5E%5Ctop)%5Cmathbf%7Bx%7D"> and in particular <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Cmathbf%7Bx%7D%7D%20%5C%7C%5Cmathbf%7Bx%7D%20%5C%7C%5E2%20=%20%5Cnabla_%7B%5Cmathbf%7Bx%7D%7D%20%5Cmathbf%7Bx%7D%5E%5Ctop%20%5Cmathbf%7Bx%7D%20=%202%5Cmathbf%7Bx%7D">.</li>
</ul>
<p>Then the chain rule states that</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x_%7Bi%7D%7D%20=%20%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20u_%7B1%7D%7D%20%5Cfrac%7B%5Cpartial%20u_%7B1%7D%7D%7B%5Cpartial%20x_%7Bi%7D%7D%20+%20%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20u_%7B2%7D%7D%20%5Cfrac%7B%5Cpartial%20u_%7B2%7D%7D%7B%5Cpartial%20x_%7Bi%7D%7D%20+%20%5Cldots%20+%20%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20u_%7Bm%7D%7D%20%5Cfrac%7B%5Cpartial%20u_%7Bm%7D%7D%7B%5Cpartial%20x_%7Bi%7D%7D%20%5C%20%5Ctextrm%7B%20and%20so%20%7D%20%5C%20%5Cnabla_%7B%5Cmathbf%7Bx%7D%7D%20y%20=%20%20%5Cmathbf%7BA%7D%20%5Cnabla_%7B%5Cmathbf%7Bu%7D%7D%20y,"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20m%7D"> is a <em>matrix</em> that contains the derivative of vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> with respect to vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">.</p>
</section>
</section>
<section id="integrals" class="level3">
<h3 class="anchored" data-anchor-id="integrals">Integrals</h3>
<p>Integrals are</p>
<ul>
<li>a way to compute the <strong>signed</strong> area under a curve</li>
<li>antiderivatives</li>
<li>a way of adding up tiny bits</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cint_a%5Eb%20f(x)%5C,dx%20"></p>
<section id="intuition-as-sum-of-rectangles" class="level4">
<h4 class="anchored" data-anchor-id="intuition-as-sum-of-rectangles">Intuition as sum of rectangles</h4>
<p>The integral <img src="https://latex.codecogs.com/png.latex?%5Cint_a%5Eb%20f(x)%5C,dx"> is the <em>limit of sums of tiny rectangular areas</em>.</p>
<p>If we cut the interval <img src="https://latex.codecogs.com/png.latex?%5Ba,b%5D"> into <img src="https://latex.codecogs.com/png.latex?n"> equal chunks of width <img src="https://latex.codecogs.com/png.latex?%5CDelta%20x%20=%20%5Cfrac%7Bb-a%7D%7Bn%7D,"> then the total area is approximated by <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bk=1%7D%5E%7Bn%7D%20f(a%20+%20k%5CDelta%20x)%5C,%5CDelta%20x."></p>
<p>As we make the chunks thinner (<img src="https://latex.codecogs.com/png.latex?n%20%5Cto%20%5Cinfty">, so <img src="https://latex.codecogs.com/png.latex?%5CDelta%20x%20%5Cto%200">), this sum becomes exact:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cint_a%5Eb%20f(x)%5C,dx%20=%20%5Clim_%7Bn%5Cto%5Cinfty%7D%20%5Csum_%7Bk=1%7D%5E%7Bn%7D%20f(a%20+%20k%5CDelta%20x)%5C,%5CDelta%20x."></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cint_a%5Eb%20f(x)%5C,dx"> is a <strong>definite integral</strong> of <img src="https://latex.codecogs.com/png.latex?f(x)"> from <img src="https://latex.codecogs.com/png.latex?%5Ba,b%5D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cint%20f(x)%5C,dx"> is an <strong>indefinite integral</strong>.</p>
</section>
</section>
<section id="fundamental-theorem-of-calculus" class="level3">
<h3 class="anchored" data-anchor-id="fundamental-theorem-of-calculus">Fundamental Theorem of Calculus</h3>
<p>The fundamental theorem of calculus links differentiation (derivatives) and integration (integrals).</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cint_a%5Eb%20f(x)%5C,dx%20=%20F(b)%20-%20F(a)"></p>
<p>Where</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cdfrac%7Bd%7D%7Bdx%7DF(x)%20=%20f(x)%20"></p>
<section id="intuition" class="level4">
<h4 class="anchored" data-anchor-id="intuition">Intuition</h4>
<p>Say we have a function <img src="https://latex.codecogs.com/png.latex?A(x)"> being the area under the curve of <img src="https://latex.codecogs.com/png.latex?f(x)"> between <img src="https://latex.codecogs.com/png.latex?0"> and <img src="https://latex.codecogs.com/png.latex?x">.</p>
<p>to find the area under the curve between <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?x+h">, we could compute <img src="https://latex.codecogs.com/png.latex?%20A(x+h)%20-%20A(x)%20%5Capprox%20f(x)h%20"> <img src="https://latex.codecogs.com/png.latex?%20%5Ciff%20%5Cdfrac%7BA(x+h)%20-%20A(x)%7D%7Bh%7D%20%5Capprox%20f(x)%20"> <img src="https://latex.codecogs.com/png.latex?%20%5Ciff%20%5Clim_%7Bh%20%5Cto%200%7D%20%5Cdfrac%7BA(x+h)%20-%20A(x)%7D%7Bh%7D%20=%20f(x)%20"> <img src="https://latex.codecogs.com/png.latex?%20%5Ciff%20%5Cdfrac%7Bd%7D%7Bdx%7DA(x)%20=%20f(x)%20"></p>
</section>
<section id="dx" class="level4">
<h4 class="anchored" data-anchor-id="dx">dx</h4>
<p><img src="https://latex.codecogs.com/png.latex?dx"> = the differential of <img src="https://latex.codecogs.com/png.latex?x"></p>
<p>A single symbol that means “infinitesimal change in <img src="https://latex.codecogs.com/png.latex?x">.”</p>
<p>In derivatives, it appears in a ratio (<img src="https://latex.codecogs.com/png.latex?%5Cdfrac%7Bdy%7D%7Bdx%7D">). We are differentiating <img src="https://latex.codecogs.com/png.latex?f(x)"> <strong>wrt <img src="https://latex.codecogs.com/png.latex?x"></strong>.</p>
<p>In integrals, it appears as a piece being added up (<img src="https://latex.codecogs.com/png.latex?f(x)%5C,dx">). We are integrating <img src="https://latex.codecogs.com/png.latex?f(x)"> <strong>wrt <img src="https://latex.codecogs.com/png.latex?x"></strong>.</p>
</section>
</section>
<section id="lhôpitals-rule" class="level3">
<h3 class="anchored" data-anchor-id="lhôpitals-rule">L’Hôpital’s Rule</h3>
<p>If <img src="https://latex.codecogs.com/png.latex?%5Clim_%7Bx%20%5Cto%20c%7D%20f(x)%20=%20%5Clim_%7Bx%20%5Cto%20c%7D%20g(x)%20=%200"> or <img src="https://latex.codecogs.com/png.latex?%5Cpm%20%5Cinfty">, and <img src="https://latex.codecogs.com/png.latex?%5Clim_%7Bx%20%5Cto%20c%7D%20%5Cdfrac%7B%5Cfrac%7Bd%7D%7Bdx%7Df(x)%7D%7B%5Cfrac%7Bd%7D%7Bdx%7Dg(x)%7D"> exists, then</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clim_%7Bx%20%5Cto%20c%7D%20%5Cdfrac%7Bf(x)%7D%7Bg(x)%7D%20=%20%5Clim_%7Bx%20%5Cto%20c%7D%20%5Cdfrac%7B%5Cfrac%7Bd%7D%7Bdx%7Df(x)%7D%7B%5Cfrac%7Bd%7D%7Bdx%7Dg(x)%7D"></p>
</section>
<section id="implicit-differentiation" class="level3">
<h3 class="anchored" data-anchor-id="implicit-differentiation">Implicit Differentiation</h3>
<p>We can use <img src="https://latex.codecogs.com/png.latex?a%20=%20b%20%5Cimplies%20%5Cdfrac%7Bd%7D%7Bdx%7Da%20=%20%5Cdfrac%7Bd%7D%7Bdx%7Db"> to compute derivatives of <strong>relations</strong> (linked variables eg. the equation for a circle centered at the origin and of radius 5: <img src="https://latex.codecogs.com/png.latex?x%5E2%20+%20y%5E2%20=%2025">)</p>
<p>Or even to compute the derivative of <img src="https://latex.codecogs.com/png.latex?ln(x)">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0Ay%20=%20ln(x)%20&amp;%20%5Ciff%20e%5Ey%20=%20x%20%5Cimplies%20%5Cdfrac%7Bd%7D%7Bdx%7D%20e%5Ey%20=%20%5Cdfrac%7Bd%7D%7Bdx%7D%20x%20%5C%5C%0A&amp;%20%5Ciff%20%5Cdfrac%7Bd%7D%7Bdx%7D%20e%5Ey%20=%201%20%5C%5C%0A&amp;%20%5Ciff%20e%5Ey%5Cdfrac%7Bd%7D%7Bdx%7Dy%20=%201%20%5C%5C%0A&amp;%20%5Ciff%20%5Cdfrac%7Bd%7D%7Bdx%7Dy%20=%20%5Cdfrac%7B1%7D%7Be%5Ey%7D%20%5C%5C%0A&amp;%20%5Ciff%20%5Cdfrac%7Bd%7D%7Bdx%7Dy%20=%20%5Cdfrac%7B1%7D%7Be%5E%7Bln(x)%7D%7D%20%5C%5C%0A&amp;%20%5Ciff%20%5Cdfrac%7Bd%7D%7Bdx%7Dy%20=%20%5Cdfrac%7B1%7D%7Bx%7D%0A%5Cend%7Baligned%7D"></p>
</section>
</section>
<section id="proofs" class="level2">
<h2 class="anchored" data-anchor-id="proofs">Proofs</h2>
<p>Later!</p>
</section>
<section id="notation" class="level2">
<h2 class="anchored" data-anchor-id="notation">Notation</h2>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?f(%5Ccdot)">: a function</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cdfrac%7Bdy%7D%7Bdx%7D">: derivative of <img src="https://latex.codecogs.com/png.latex?y"> with respect to <img src="https://latex.codecogs.com/png.latex?x"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cdfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x%7D">: partial derivative of <img src="https://latex.codecogs.com/png.latex?y"> with respect to <img src="https://latex.codecogs.com/png.latex?x"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cnabla_%7B%5Cmathbf%7Bx%7D%7D%20y">: gradient of <img src="https://latex.codecogs.com/png.latex?y"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BJ%7D_f%20(%5Cmathbf%7Bx%7D),%20%5Cdfrac%7B%5Cpartial%20%5Cmathbf%7By%7D%7D%7B%5Cpartial%20%5Cmathbf%7Bx%7D%7D">: Jacobian of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%20=%20f(%5Cmathbf%7Bx%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cint_a%5Eb%20f(x)%20%5C;dx">: definite integral of <img src="https://latex.codecogs.com/png.latex?f"> from <img src="https://latex.codecogs.com/png.latex?a"> to <img src="https://latex.codecogs.com/png.latex?b"> with respect to <img src="https://latex.codecogs.com/png.latex?x"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cint%20f(x)%20%5C;dx">: indefinite integral of <img src="https://latex.codecogs.com/png.latex?f"> with respect to <img src="https://latex.codecogs.com/png.latex?x"></li>
</ul>


</section>

 ]]></description>
  <category>math</category>
  <guid>https://theopomies.com/notes/calculus.html</guid>
  <pubDate>Tue, 02 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>PyTorch</title>
  <dc:creator>Theo POMIES</dc:creator>
  <link>https://theopomies.com/notes/pytorch.html</link>
  <description><![CDATA[ 





<section id="functions-methods-classes" class="level2">
<h2 class="anchored" data-anchor-id="functions-methods-classes">Functions / Methods / Classes</h2>
</section>
<section id="notes" class="level2">
<h2 class="anchored" data-anchor-id="notes">Notes</h2>
<p>Later!</p>
</section>
<section id="terminology" class="level2">
<h2 class="anchored" data-anchor-id="terminology">Terminology</h2>


</section>

 ]]></description>
  <category>python</category>
  <category>mle</category>
  <guid>https://theopomies.com/notes/pytorch.html</guid>
  <pubDate>Tue, 02 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Linear Algebra</title>
  <dc:creator>Theo POMIES</dc:creator>
  <link>https://theopomies.com/notes/linear-algebra.html</link>
  <description><![CDATA[ 





<section id="definitions-formulas" class="level2">
<h2 class="anchored" data-anchor-id="definitions-formulas">Definitions &amp; Formulas</h2>
<section id="sec-vector-space" class="level3">
<h3 class="anchored" data-anchor-id="sec-vector-space">Vector Space</h3>
<p>A <strong>vector space</strong> (over a field, like <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D">, which provides the <strong>scalars</strong>) is a <strong>set</strong> of <strong>vectors</strong> equipped with <strong>vector addition</strong> and <strong>scalar multiplication</strong> that satisfies the following conditions:</p>
<ol type="1">
<li><strong>Closure under addition:</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D"> is in the space for any <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D,%20%5Cmathbf%7Bv%7D"> in the space.</li>
<li><strong>Closure under scalar multiplication:</strong> <img src="https://latex.codecogs.com/png.latex?%5Calpha%20%5Cmathbf%7Bu%7D"> is in the space for any scalar <img src="https://latex.codecogs.com/png.latex?%5Calpha">.</li>
<li><strong>Associativity of addition:</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20(%5Cmathbf%7Bv%7D%20+%20%5Cmathbf%7Bw%7D)%20=%20(%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D)%20+%20%5Cmathbf%7Bw%7D">.</li>
<li><strong>Commutativity of addition:</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D%20=%20%5Cmathbf%7Bv%7D%20+%20%5Cmathbf%7Bu%7D">.</li>
<li><strong>Additive identity:</strong> there exists <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7B0%7D%20=%20%5Cmathbf%7Bu%7D">.</li>
<li><strong>Additive inverse:</strong> for each <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D">, there exists <img src="https://latex.codecogs.com/png.latex?-%5Cmathbf%7Bu%7D"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20+%20(-%5Cmathbf%7Bu%7D)%20=%20%5Cmathbf%7B0%7D">.</li>
<li><strong>Compatibility of scalar multiplication:</strong> <img src="https://latex.codecogs.com/png.latex?%5Calpha(%5Cbeta%20%5Cmathbf%7Bu%7D)%20=%20(%5Calpha%5Cbeta)%20%5Cmathbf%7Bu%7D">.</li>
<li><strong>Identity of scalar multiplication:</strong> <img src="https://latex.codecogs.com/png.latex?1%20%5Ccdot%20%5Cmathbf%7Bu%7D%20=%20%5Cmathbf%7Bu%7D">.</li>
<li><strong>Distributivity over vector addition:</strong> <img src="https://latex.codecogs.com/png.latex?%5Calpha(%5Cmathbf%7Bu%7D%20+%20%5Cmathbf%7Bv%7D)%20=%20%5Calpha%20%5Cmathbf%7Bu%7D%20+%20%5Calpha%20%5Cmathbf%7Bv%7D">.</li>
<li><strong>Distributivity over scalar addition:</strong> <img src="https://latex.codecogs.com/png.latex?(%5Calpha%20+%20%5Cbeta)%5Cmathbf%7Bu%7D%20=%20%5Calpha%20%5Cmathbf%7Bu%7D%20+%20%5Cbeta%20%5Cmathbf%7Bu%7D">.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This might be useful later, but for all intents and purposes in these notes, the field will be <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D">, vectors will be in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En">, and addition and scalar multiplication will be element-wise.</p>
</div>
</div>
</section>
<section id="transpose" class="level3">
<h3 class="anchored" data-anchor-id="transpose">Transpose</h3>
<p><img src="https://latex.codecogs.com/png.latex?%20(%5Cmathbf%7BA%7D%5E%5Ctop)_%7Bi,j%7D%20=%20%5Cmathbf%7BA%7D_%7Bj,i%7D%20%20"></p>
</section>
<section id="dot-product" class="level3">
<h3 class="anchored" data-anchor-id="dot-product">Dot Product</h3>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbf%7Bu%7D%20%5Ccdot%20%5Cmathbf%7Bv%7D%20=%20%5Csum_%7Bi%7D%20u_i%20v_i%20"></p>
<div id="4416df6f" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">u, v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>), torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># torch.tensor([0, 1, 2])</span></span>
<span id="cb1-2">torch.dot(u, v)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># u @ v</span></span></code></pre></div></div>
<div class="cell-output cell-output-display" data-execution_count="119">
<pre><code>tensor(5)</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The dot product has a geometric meaning.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D%5Ccdot%5Cmathbf%7Bw%7D%20=%20%5C%7C%5Cmathbf%7Bv%7D%5C%7C%5C%7C%5Cmathbf%7Bw%7D%5C%7C%5Ccos%7B%5Ctheta%7D"> <img src="https://latex.codecogs.com/png.latex?%5Ciff%20%5Ctheta%20=%20%5Carccos%5Cleft(%5Cfrac%7B%5Cmathbf%7Bv%7D%5Ccdot%5Cmathbf%7Bw%7D%7D%7B%5C%7C%5Cmathbf%7Bv%7D%5C%7C%5C%7C%5Cmathbf%7Bw%7D%5C%7C%7D%5Cright)."></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In linear algebra, vectors are column vectors by default, so <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbf%7Bu%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%201%7D,%20%5Cquad%20%5Cmathbf%7Bu%7D%20=%20%5Cbegin%7Bbmatrix%7D%20u_1%20%5C%5C%20%5Cvdots%20%5C%5C%20u_n%20%5Cend%7Bbmatrix%7D%20"></p>
</div>
</div>
</section>
<section id="matrix-vector-product" class="level3">
<h3 class="anchored" data-anchor-id="matrix-vector-product">Matrix-Vector Product</h3>
<p><img src="https://latex.codecogs.com/png.latex?%20(%5Cmathbf%7BA%7D%5Cmathbf%7Bu%7D)_%7Bi%7D%20=%20%5Cmathbf%7Ba%7D_%7Bi,:%7D%20%5Ccdot%20%5Cmathbf%7Bu%7D%20=%20%5Csum_%7Bj%7D%20a_%7Bi,j%7D%20u_j%20%20"></p>
<div id="9d859f8a" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">A, u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>).reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb3-2">A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> u</span></code></pre></div></div>
<div class="cell-output cell-output-display" data-execution_count="120">
<pre><code>tensor([1, 3, 5])</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>When we say “we apply a linear transformation <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D">”, we mean <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5Cmathbf%7Bv%7D">.</p>
</div>
</div>
</section>
<section id="matrix-multiplication" class="level3">
<h3 class="anchored" data-anchor-id="matrix-multiplication">Matrix Multiplication</h3>
<p><img src="https://latex.codecogs.com/png.latex?%20(%5Cmathbf%7BA%7D%5Cmathbf%7BB%7D)_%7Bi,j%7D%20=%20%5Cmathbf%7Ba%7D_%7Bi,:%7D%20%5Ccdot%20%5Cmathbf%7Bb%7D_%7B:,%20j%7D%20=%20%5Csum_%7Bk%7D%20a_%7Bi,k%7D%20b_%7Bk,j%7D%20"></p>
<div id="0312b10e" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">U, V <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>).reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>).reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb5-2">U <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> V</span></code></pre></div></div>
<div class="cell-output cell-output-display" data-execution_count="121">
<pre><code>tensor([[ 3,  4,  5],
        [ 9, 14, 19],
        [15, 24, 33]])</code></pre>
</div>
</div>
<section id="properties" class="level4">
<h4 class="anchored" data-anchor-id="properties">Properties</h4>
<ul>
<li>Non-commutativity: most of the time <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5Cmathbf%7BB%7D%20%5Cneq%20%5Cmathbf%7BB%7D%5Cmathbf%7BA%7D"></li>
<li>Distributivity: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D(%5Cmathbf%7BB%7D%20+%20%5Cmathbf%7BC%7D)%20=%20%5Cmathbf%7BA%7D%5Cmathbf%7BB%7D%20+%20%5Cmathbf%7BB%7D%5Cmathbf%7BC%7D"> and <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BA%7D%20+%20%5Cmathbf%7BB%7D)%5Cmathbf%7BC%7D%20=%20%5Cmathbf%7BA%7D%5Cmathbf%7BC%7D%20+%20%5Cmathbf%7BB%7D%5Cmathbf%7BC%7D"></li>
<li>Associativity: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D(%5Cmathbf%7BB%7D%5Cmathbf%7BC%7D)%20=%20(%5Cmathbf%7BA%7D%5Cmathbf%7BB%7D)%5Cmathbf%7BC%7D"></li>
<li>Transpose: <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BA%7D%5Cmathbf%7BB%7D)%5E%5Ctop%20=%20%5Cmathbf%7BB%7D%5E%5Ctop%5Cmathbf%7BA%7D%5E%5Ctop"></li>
</ul>
</section>
</section>
<section id="sec-outer-product" class="level3">
<h3 class="anchored" data-anchor-id="sec-outer-product">Outer Product</h3>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbf%7Bu%7D%20%5Cotimes%20%5Cmathbf%7Bv%7D%0A=%20%5Cbegin%7Bbmatrix%7D%0Au_1v_1%20&amp;%20u_1v_2%20&amp;%20%5Cdots%20&amp;%20u_1v_n%20%5C%5C%0Au_2v_1%20&amp;%20u_2v_2%20&amp;%20%5Cdots%20&amp;%20u_2v_n%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0Au_mv_1%20&amp;%20u_mv_2%20&amp;%20%5Cdots%20&amp;%20u_mv_n%20%5C%5C%0A%5Cend%7Bbmatrix%7D%0A=%20%5Cmathbf%7Bu%7D%5Cmathbf%7Bv%7D%5E%5Ctop"></p>
<div id="f79784fe" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">u, v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>), torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb7-2">torch.outer(u, v), torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span>(torch.outer(u, v) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> u.reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> v.reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span></code></pre></div></div>
<div class="cell-output cell-output-display" data-execution_count="122">
<pre><code>(tensor([[0, 0, 0],
         [0, 1, 2],
         [0, 2, 4]]),
 tensor(True))</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The outer product of two column vectors is the matrix whose entries are all products of an element in the first vector with an element in the second vector.</p>
</div>
</div>
</section>
<section id="inverse-of-a-square-matrix" class="level3">
<h3 class="anchored" data-anchor-id="inverse-of-a-square-matrix">Inverse of a Square Matrix</h3>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BA%7D%5E%7B-1%7D%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BI%7D,%5Cquad%20%5Cmathbf%7BA%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20n%7D%20"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Might not exist if <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> is not invertible, that is <img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BA%7D)%20=%200"></p>
</div>
</div>
<div id="0ca507ca" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.double)</span>
<span id="cb9-2">A_i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> A.inverse()</span>
<span id="cb9-3">I <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.eye(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.double)</span>
<span id="cb9-4">torch.allclose(A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> A_i, I), torch.allclose(A_i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> A, I)</span></code></pre></div></div>
<div class="cell-output cell-output-display" data-execution_count="123">
<pre><code>(True, True)</code></pre>
</div>
</div>
</section>
<section id="sec-determinant" class="level3">
<h3 class="anchored" data-anchor-id="sec-determinant">Determinant of a Matrix</h3>
<p>I think of the determinant of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> as the scaling factor of the linear transformation represented by <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>This explains why a matrix whose determinant is 0 is not invertible: it “collapses”, and two images might have the same original input <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"></p>
<section id="sec-det-identities" class="level4">
<h4 class="anchored" data-anchor-id="sec-det-identities">Basic identities</h4>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cdet(%5Cmathbf%7BA%7D%5E%5Ctop)%20=%20%5Cdet(%5Cmathbf%7BA%7D)%20"> <img src="https://latex.codecogs.com/png.latex?%20%5Cdet(%5Cmathbf%7BA%7D%5Cmathbf%7BB%7D)%20=%20%5Cdet(%5Cmathbf%7BA%7D)%5Cdet(%5Cmathbf%7BB%7D)%20"> <img src="https://latex.codecogs.com/png.latex?%20%5Cdet(%5Cmathbf%7BA%7D%5E%7B-1%7D)%20=%20%5Cdfrac%7B1%7D%7B%5Cdet(%5Cmathbf%7BA%7D)%7D%20%5Ciff%20%5Cdet(%5Cmathbf%7BA%7D)%20%5Cneq%200%20"></p>
</section>
<section id="sec-det-diag-matrix" class="level4">
<h4 class="anchored" data-anchor-id="sec-det-diag-matrix">Determinant of Diagonal Matrix</h4>
<p><img src="https://latex.codecogs.com/png.latex?%20A%20=%20%5Coperatorname%7Bdiag%7D(a_1,%20%5Cdots,%20a_n),%20%5Cquad%20%5Cdet(A)%20=%20%5Cprod_%7Bi=1%7D%5En%20a_i%20"></p>
<div id="55e0717a" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># using I as a mask</span></span>
<span id="cb11-2">I <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.eye(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32)</span>
<span id="cb11-3">A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32).reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> I  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># A = diag(1, 5, 9)</span></span>
<span id="cb11-4">torch.det(A), torch.det(A) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> torch.diag(A).prod()</span></code></pre></div></div>
<div class="cell-output cell-output-display" data-execution_count="124">
<pre><code>(tensor(45.), tensor(True))</code></pre>
</div>
</div>
</section>
<section id="determinant-of-a-triangular-matrix" class="level4">
<h4 class="anchored" data-anchor-id="determinant-of-a-triangular-matrix">Determinant of a Triangular Matrix</h4>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cdet(T)%20=%20%5Cprod_%7Bi%7D%20t_%7Bi,i%7D,%20%5Cquad%20%5Ctext%7Bwhere%20$T$%20is%20triangular%7D%20"></p>
<div id="2339eb1c" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.triu(torch.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32).reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb13-2">torch.det(A), torch.det(A) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> torch.diag(A).prod()</span></code></pre></div></div>
<div class="cell-output cell-output-display" data-execution_count="125">
<pre><code>(tensor(45.), tensor(True))</code></pre>
</div>
</div>
</section>
</section>
<section id="sec-orthogonality" class="level3">
<h3 class="anchored" data-anchor-id="sec-orthogonality">Orthogonality</h3>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbf%7BA%7D%20%5C,%20%5Ctextnormal%7Bis%20orthogonal%7D%20%5Ciff%20%5Cmathbf%7BA%7D%5E%5Ctop%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BI%7D%5C:%20(=%20%5Cmathbf%7BA%7D%5Cmathbf%7BA%7D%5E%5Ctop)%20"></p>
<p>Interstingly,</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbf%7BA%7D%20%5C,%20%5Ctextnormal%7Bis%20orthogonal%7D%20%5Ciff%20%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BA%7D%5E%5Ctop%20"></p>
</section>
<section id="sec-elementary-matrix" class="level3">
<h3 class="anchored" data-anchor-id="sec-elementary-matrix">Elementary Matrix</h3>
<p>An elementary matrix is a matrix obtained from the application of a <strong>single elementary row operation</strong> to the identity matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D">.</p>
<p>Because of <strong>associativity</strong>, the product of an elementary matrix and another matrix is the same as applying the row operation to the other matrix.</p>
<section id="scaling-a-row" class="level4">
<h4 class="anchored" data-anchor-id="scaling-a-row">Scaling a row</h4>
<p>Multiplying row <img src="https://latex.codecogs.com/png.latex?i"> by <img src="https://latex.codecogs.com/png.latex?m"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BD%7D_i(m)%20=%20%5Coperatorname%7Bdiag%7D(1,%20%5Cdots,%201,%20d_i%20=%20m,%201,%20%5Cdots,%201)"></p>
<section id="properties-1" class="level5">
<h5 class="anchored" data-anchor-id="properties-1">Properties</h5>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BD%7D_i(m)%20=%20%5Cmathbf%7BD%7D_i(%5Cdfrac%7B1%7D%7Bm%7D)%5E%7B-1%7D%20=%20%5Cmathbf%7BD%7D_i(m)%5E%5Ctop"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bdet%7D(%5Cmathbf%7BD%7D_i(m))%20=%20m"> — obviously because it’s a diagonal matrix full of <img src="https://latex.codecogs.com/png.latex?1">’s and a single <img src="https://latex.codecogs.com/png.latex?m"></li>
</ul>
</section>
</section>
<section id="adding-a-multiple-of-one-row-to-another" class="level4">
<h4 class="anchored" data-anchor-id="adding-a-multiple-of-one-row-to-another">Adding a multiple of one row to another</h4>
<p>Adding <img src="https://latex.codecogs.com/png.latex?m"> times row <img src="https://latex.codecogs.com/png.latex?j"> to row <img src="https://latex.codecogs.com/png.latex?i"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BL%7D_%7Bij%7D(m)%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;&amp;&amp;&amp;&amp;&amp;%20%5C%5C%0A&amp;%20%5Cddots%20&amp;&amp;&amp;&amp;&amp;%20%5C%5C%0A&amp;&amp;%201%20&amp;&amp;&amp;&amp;%20%5C%5C%0A&amp;&amp;&amp;%20%5Cddots%20&amp;&amp;&amp;%20%5C%5C%0A&amp;&amp;%20l_%7Bi,j%7D%20=%20m%20&amp;&amp;%201%20&amp;&amp;%20%5C%5C%0A&amp;&amp;&amp;&amp;&amp;%20%5Cddots%20&amp;%20%5C%5C%0A&amp;&amp;&amp;&amp;&amp;&amp;%201%0A%5Cend%7Bbmatrix%7D"></p>
<section id="properties-2" class="level5">
<h5 class="anchored" data-anchor-id="properties-2">Properties</h5>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BL%7D_%7Bij%7D(m)%5E%7B-1%7D%20=%20%5Cmathbf%7BL%7D_%7Bij%7D(-m)"> — the inverse of adding a row multiple to another, is <em>subtracting</em> the same row multiple</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BL%7D_%7Bij%7D(m)"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BL%7D_%7Bij%7D(m)%5E%7B-1%7D"> are triangular matrices</li>
<li>So <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bdet%7D(%5Cmathbf%7BL%7D_%7Bij%7D(m))%20=%201%20%5Ciff%20%5Coperatorname%7Bdet%7D(%5Cmathbf%7BL%7D_%7Bij%7D(m)%5Cmathbf%7BA%7D)%20=%20%5Coperatorname%7Bdet%7D(%5Cmathbf%7BA%7D)"></li>
</ul>
</section>
</section>
<section id="swapping-switching-rows" class="level4">
<h4 class="anchored" data-anchor-id="swapping-switching-rows">Swapping (switching) rows</h4>
<p>Swapping rows <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BT%7D_%7Bij%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;&amp;&amp;&amp;&amp;&amp;%20%5C%5C%0A&amp;%20%5Cddots%20&amp;&amp;&amp;&amp;&amp;%20%5C%5C%0A&amp;&amp;%200%20&amp;&amp;%20t_%7Bj,i%7D%20=%201%20&amp;&amp;%20%5C%5C%0A&amp;&amp;&amp;%20%5Cddots%20&amp;&amp;&amp;%20%5C%5C%0A&amp;&amp;%20t_%7Bi,j%7D%20=%201%20&amp;&amp;%200%20&amp;&amp;%20%5C%5C%0A&amp;&amp;&amp;&amp;&amp;%20%5Cddots%20&amp;%20%5C%5C%0A&amp;&amp;&amp;&amp;&amp;&amp;%201%0A%5Cend%7Bbmatrix%7D"></p>
<section id="properties-3" class="level5">
<h5 class="anchored" data-anchor-id="properties-3">Properties</h5>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BT%7D_%7Bij%7D%5E%5Ctop%20=%20%5Cmathbf%7BT%7D_%7Bij%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BT%7D_%7Bij%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bdet%7D(%5Cmathbf%7BT%7D_%7Bij%7D)%20=%20-1%5C:%5E%7B(*)%7D%20%5Ciff%20%5Coperatorname%7Bdet%7D(%5Cmathbf%7BT%7D_%7Bij%7D%5Cmathbf%7BA%7D)%20=%20-%5Coperatorname%7Bdet%7D(%5Cmathbf%7BA%7D)"></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%5E%7B(*)%7D"> Consider that swapping two rows can be expressed as applying consecutive additions/subtractions as follows <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%5Cmathbf%7BT%7D_%7Bij%7D%20&amp;%20=%20%5Cmathbf%7BD%7D_%7Bi%7D(-1)%5Cmathbf%7BL%7D_%7Bij%7D(-1)%5Cmathbf%7BL%7D_%7Bji%7D(1)%5Cmathbf%7BL%7D_%7Bij%7D(-1)%20%5C%5C%0A%5Ciff%20%5Coperatorname%7Bdet%7D(%5Cmathbf%7BT%7D_%7Bij%7D)%20&amp;%20=%20%5Coperatorname%7Bdet%7D(%5Cmathbf%7BD%7D_%7Bi%7D(-1))%20%5Ccdot%20%5Coperatorname%7Bdet%7D(%5Cmathbf%7BL%7D_%7Bij%7D(-1))%20%5Ccdot%20%5Coperatorname%7Bdet%7D(%5Cmathbf%7BL%7D_%7Bji%7D(1))%20%5Ccdot%20%5Coperatorname%7Bdet%7D(%5Cmathbf%7BL%7D_%7Bij%7D(-1))%20%5C%5C%0A%5Ciff%20%5Coperatorname%7Bdet%7D(%5Cmathbf%7BT%7D_%7Bij%7D)%20&amp;%20=%20-1%20%5C%5C%0A%5Cend%7Baligned%7D"></p>
</section>
</section>
</section>
<section id="sec-linear-dependence" class="level3">
<h3 class="anchored" data-anchor-id="sec-linear-dependence">Linear Dependence</h3>
<p>We say of <img src="https://latex.codecogs.com/png.latex?n"> vectors — <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_1,%20%5Cdots,%20%5Cmathbf%7Bv%7D_n"> that they are <strong>linearly dependent</strong> if there exists scalars <img src="https://latex.codecogs.com/png.latex?a_1,%20%5Cdots,%20a_n"> <strong>not all equal to 0</strong> satisfying <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5En%20a_i%5Cmathbf%7Bv%7D_i%20=%200"></p>
<p>If the only solution is <img src="https://latex.codecogs.com/png.latex?a_i%20=%200"> for <img src="https://latex.codecogs.com/png.latex?i"> in <img src="https://latex.codecogs.com/png.latex?0,%20%5Cdots,%20n">, they are <strong>linearly independent</strong>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Linear dependence means that some vectors could be expressed as a weighted sum (<strong>linear combination</strong>) of others, hence carrying redudant information/operation.</p>
</div>
</div>
</section>
<section id="sec-rank" class="level3">
<h3 class="anchored" data-anchor-id="sec-rank">Rank</h3>
<p>The <strong>rank</strong> of a matrix is the size of the <strong>largest</strong> subset of linearly independent columns among its columns. Equivalently, it is the <strong>dimension of the column space</strong>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This reflects that if some columns are linearly dependent, they are redundant: they do not contribute a new direction. Thus, the matrix can be expressed with fewer independent columns, and the image (output space) of the matrix has correspondingly fewer dimensions.</p>
</div>
</div>
</section>
<section id="sec-basis" class="level3">
<h3 class="anchored" data-anchor-id="sec-basis">Basis</h3>
<p>A <strong>basis</strong> (not <em>the</em>, since a vector space can have many different bases) of a <strong>vector space</strong> is a linearly independent subset that <strong>spans</strong> the space.</p>
<p>All bases of the same vector space have the same size = the dimension</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>That means: a basis is a set of vectors that you can combine (with weighted sums) to form <strong>any</strong> vector in the space — and none of them are redundant.</p>
</div>
</div>
</section>
<section id="sec-dimension" class="level3">
<h3 class="anchored" data-anchor-id="sec-dimension">Dimension</h3>
<p>The <strong>dimension</strong> of a vector space is the number of vectors in a basis of that space.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Since every basis of a vector space has the same size, the dimension is well-defined. It tells you “how many independent directions” the space has.</p>
</div>
</div>
</section>
<section id="eigendecomposition" class="level3">
<h3 class="anchored" data-anchor-id="eigendecomposition">Eigendecomposition</h3>
<section id="sec-eigenv" class="level4">
<h4 class="anchored" data-anchor-id="sec-eigenv">Eigen{vectors, values}</h4>
<p>An <strong>eigenvector</strong> is a vector whose direction (eg. “line”) is unchanged by a linear transformation (represented by a matrix). That means that when applying the linear transformation <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> to an eigeinvector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> it simply gets scaled by a constant scalar factor, the <strong>eigenvalue</strong> <img src="https://latex.codecogs.com/png.latex?%5Clambda">. Represented by this <strong>eigenequation</strong>. <span id="eq-eigenequation"><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%5Cmathbf%7BA%7D%5Cmathbf%7Bv%7D%20&amp;%20=%20%5Clambda%5Cmathbf%7Bv%7D%20%5C%5C%0A%5Ciff%20(%5Cmathbf%7BA%7D%20-%20%5Clambda%5Cmathbf%7BI%7D)%5Cmathbf%7Bv%7D%20&amp;%20=%20%5Cmathbf%7B0%7D%20%5Clabel%7Beq:1%7D%0A%5Cend%7Baligned%7D%20%5Ctag%7B1%7D"></span></p>
<p>This equation&nbsp;1 has a non-zero solution <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D"> only if <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bdet%7D(%5Cmathbf%7BA%7D%20-%20%5Clambda%5Cmathbf%7BI%7D)%20=%200"> — called the <strong>characteristic equation</strong> of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">. The values of <img src="https://latex.codecogs.com/png.latex?%5Clambda"> that satisfy this equation are the <strong>egeinvalues</strong> of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">.</p>
<p>The <strong>eigenvectors</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bv%7D_%7B%5Clambda=n%7D"> corresponding to each eigenvalue can be found by plugging the values of <img src="https://latex.codecogs.com/png.latex?%5Clambda"> in equation&nbsp;1.</p>
</section>
<section id="decomposing-matrices" class="level4">
<h4 class="anchored" data-anchor-id="decomposing-matrices">Decomposing matrices</h4>
<p>By definition of eigenvectors, for a square matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, if it has a full set of linearly independent eigenvectors (ie. it’s <strong>diagonalizable</strong>), we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbf%7BA%7D%5Cmathbf%7BV%7D%20=%20%5Cmathbf%7BV%7D%5Cmathbf%7B%5CLambda%7D%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D"> a matrix whose columns are the the eigenvectors of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5CLambda%7D"> the diagonal of associated eigenvalues.</p>
<p>Because <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D"> is invertible, we can multiply <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BV%7D%5E%7B-1%7D"> by both sides:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BV%7D%5Cmathbf%7B%5CLambda%7D%5Cmathbf%7BV%7D%5E%7B-1%7D%20"></p>
<section id="properties-and-uses" class="level5">
<h5 class="anchored" data-anchor-id="properties-and-uses">Properties and uses</h5>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbf%7BA%7D%5En%20=%20%5Coverbrace%7B%5Cmathbf%7BA%7D%5Cdots%5Cmathbf%7BA%7D%7D%5E%7B%5Ctext%7Bn%20times%7D%7D%20=%20%5Coverbrace%7B(%5Cmathbf%7BV%7D%5Cmathbf%7B%5CLambda%7D%5Cmathbf%7BV%7D%5E%7B-1%7D)%5Cdots(%5Cmathbf%7BV%7D%5Cmathbf%7B%5CLambda%7D%5Cmathbf%7BV%7D%5E%7B-1%7D)%7D%5E%7B%5Ctext%7Bn%20times%7D%7D%20=%20%5Cmathbf%7BV%7D%5Coverbrace%7B%5Cmathbf%7B%5CLambda%7D%5Cdots%5Cmathbf%7B%5CLambda%7D%7D%5E%7B%5Ctext%7Bn%20times%7D%7D%5Cmathbf%7BV%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BV%7D%5Cmathbf%7B%5CLambda%7D%5En%5Cmathbf%7BV%7D%5E%7B-1%7D"></p>
<p>For negative powers: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E%7B-1%7D%20=%20%5Cmathbf%7BV%7D%5Cmathbf%7B%5CLambda%7D%5E%7B-1%7D%5Cmathbf%7BV%7D%5E%7B-1%7D"></p>
<p>as long as all diagonal entry <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bi,i%7D"> (ie. eigenvalues) are non-zero. Since <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%5Cdet(%5Cmathbf%7BA%7D)%20&amp;%20=%20%5Cdet(%5Cmathbf%7BV%7D)%5Ccdot%5Cdet(%5Cmathbf%7B%5CLambda%7D)%5Ccdot%5Cdet(%5Cmathbf%7BV%7D%5E%7B-1%7D)%20%5C%5C%0A&amp;%20=%20%5Cdet(%5Cmathbf%7BV%7D)%5Ccdot%5Cdet(%5Cmathbf%7B%5CLambda%7D)%5Ccdot%5Cdfrac%7B1%7D%7B%5Cdet(%5Cmathbf%7BV%7D)%7D%20%5C%5C%0A&amp;%20=%20%5Cdet(%5Cmathbf%7B%5CLambda%7D)%20=%20%5Cprod_%7Bi=1%7D%5En%20%5Clambda_i%0A%5Cend%7Baligned%7D"></p>
<p>(see determinant identities and determinant of diagonal matrix)</p>
<p>This reflects that a matrix is invertible if its determinant is 0.</p>
</section>
</section>
</section>
</section>
<section id="proofs" class="level2">
<h2 class="anchored" data-anchor-id="proofs">Proofs</h2>
<p>Later!</p>
</section>
<section id="algorithms" class="level2">
<h2 class="anchored" data-anchor-id="algorithms">Algorithms</h2>
<section id="sec-gaussian-elimination" class="level3">
<h3 class="anchored" data-anchor-id="sec-gaussian-elimination">Gaussian Elimination</h3>
<p>Gaussian elimination — or row reduction – is an algorithm for solving systems of linear equations, based on the following <strong>elementary row operations</strong>:</p>
<ul>
<li>Scaling a row: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D_i%20%5Cgets%20k%5Cmathbf%7BR%7D_i">, where <img src="https://latex.codecogs.com/png.latex?k%20%5Cneq%200"></li>
<li>Adding a multiple of one row to another: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D_i%20%5Cgets%20%5Cmathbf%7BR%7D_i%20+%20k%5Cmathbf%7BR%7D_j">, where <img src="https://latex.codecogs.com/png.latex?i%20%5Cneq%20j"></li>
<li>Swapping two rows: <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D_i%20%5Cleftrightarrow%20%5Cmathbf%7BR%7D_j"></li>
</ul>
</section>
<section id="laplace-expansion" class="level3">
<h3 class="anchored" data-anchor-id="laplace-expansion">Laplace Expansion</h3>
<p>todo</p>
</section>
<section id="lu-decomposition-or-lu-factorization" class="level3">
<h3 class="anchored" data-anchor-id="lu-decomposition-or-lu-factorization">LU decomposition (or LU factorization)</h3>
<p>Computing the determinant of a Matrix is not trivial at first glance.</p>
<p>We know how to easily compute:</p>
<ul>
<li>the determinant of a product of matrices</li>
<li>the determinant of a elementary matrices</li>
</ul>
<p>Knowing that, we want to find a representation of our original matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> that involves an Upper Triangular Matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BU%7D">, and one or more other matrices whose determinant is known or trivial to compute, as <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BL%7D%5Cmathbf%7BU%7D"></p>
<p>To go from <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D"> to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BU%7D"> we’ll use Gaussian Elimination, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D"> tracks our permutations (row swaps) and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BL%7D"> tracks our row operations (row additions).</p>
<p>Now, because <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BP%7D"> is orthogonal, we have <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbf%7BP%7D%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BL%7D%5Cmathbf%7BU%7D%20%5Cimplies%20%5Cmathbf%7BA%7D%20=%20%5Cmathbf%7BP%7D%5E%7B-1%7D%5Cmathbf%7BL%7D%5Cmathbf%7BU%7D%20=%20%5Cmathbf%7BP%7D%5E%5Ctop%5Cmathbf%7BL%7D%5Cmathbf%7BU%7D%20"></p>
<p>Finally, this means that <img src="https://latex.codecogs.com/png.latex?%20%5Cdet(%5Cmathbf%7BA%7D)%20=%20%5Cdet(%5Cmathbf%7BP%7D%5E%5Ctop)%20%5Ccdot%20%5Cdet(%5Cmathbf%7BL%7D)%20%5Ccdot%20%5Cdet(%5Cmathbf%7BU%7D)%20=%20%5Cdet(%5Cmathbf%7BP%7D)%20%5Ccdot%20%5Cdet(%5Cmathbf%7BL%7D)%20%5Ccdot%20%5Cdet(%5Cmathbf%7BU%7D)%20"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BP%7D)%20=%20(-1)%5E%7B%5C#swaps%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BL%7D)%20=%201"> — product of “multiplied row additions” elementary matrices</li>
</ul>
<p>Now, if we just keep track of row swaps, we can easily compute <img src="https://latex.codecogs.com/png.latex?%5Cdet(%5Cmathbf%7BA%7D)">!</p>
</section>
</section>
<section id="notation" class="level2">
<h2 class="anchored" data-anchor-id="notation">Notation</h2>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?x">: a scalar</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D">: a vector</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">: a matrix</li>
<li><img src="https://latex.codecogs.com/png.latex?x_i">: the <img src="https://latex.codecogs.com/png.latex?i%5E%5Ctextrm%7Bth%7D"> element of vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?x_%7Bi,j%7D">: the element of matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> at row <img src="https://latex.codecogs.com/png.latex?i"> and column <img src="https://latex.codecogs.com/png.latex?j"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_%7Bi,%20:%7D">: the <img src="https://latex.codecogs.com/png.latex?i%5E%5Ctextrm%7Bth%7D"> row-vector of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_%7B:,j%7D">: the <img src="https://latex.codecogs.com/png.latex?j%5E%5Ctextrm%7Bth%7D"> column-vector of <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bdiag%7D(a_1,%20%5Cdots,%20a_n)">: a diagonal matrix</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BI%7D">: the indentity matrix</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B0%7D">: the zero vector / zero matrix — depending on context</li>
<li><img src="https://latex.codecogs.com/png.latex?(%5Ccdot)%5E%5Ctop">: the transpose of a vector or matrix</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BA%7D%5E%7B-1%7D">: the inverse of a matrix</li>
<li><img src="https://latex.codecogs.com/png.latex?%5B%5Ccdot,%20%5Ccdot%5D">: concatenation</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Codot">: Hadamard (elementwise) product</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cotimes">: Outer product</li>
</ul>


</section>

 ]]></description>
  <category>math</category>
  <guid>https://theopomies.com/notes/linear-algebra.html</guid>
  <pubDate>Fri, 29 Aug 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>On Typescript</title>
  <dc:creator>Theo POMIES</dc:creator>
  <link>https://theopomies.com/notes/typescript.html</link>
  <description><![CDATA[ 





<section id="first-principles" class="level2">
<h2 class="anchored" data-anchor-id="first-principles">First Principles</h2>
<ul>
<li>Let your types be flexible</li>
<li>Let your constraints be rigid</li>
</ul>
</section>
<section id="tldr-rules" class="level2">
<h2 class="anchored" data-anchor-id="tldr-rules">TL;DR Rules</h2>
<ul>
<li>Use <code>unknown</code> instead of <code>any</code>, then use type narrowing to get the correct type.</li>
<li>Use <code>type</code> over <code>interface</code>, unless you <em>actually</em> need to reach for an interface or need to express objects/class inheritance.</li>
<li>Avoid using <code>as</code> to <em>assert</em> types, most of the time you actually want to <strong>narrow</strong> the type with checks (if/else).</li>
<li>Use <code>array.at(index)</code> instead of <code>array[index]</code> unless array is a tuple (fixed size array).</li>
<li><strong>NEVER</strong> use TS specifics (<code>enum</code>, <code>private</code> in constructor, etc.).</li>
</ul>
</section>
<section id="recommandations" class="level2">
<h2 class="anchored" data-anchor-id="recommandations">Recommandations</h2>
<ul>
<li>Use <code>satisfies</code> to check if an object fits a type but not erase the type.</li>
<li>Use <code>as const</code> whenever possible. (Immutable data, enum-like objects, etc.)</li>
<li>Define (and export) types where they are consumed, and import them from other files if needed.</li>
</ul>
</section>
<section id="explanations" class="level2">
<h2 class="anchored" data-anchor-id="explanations">Explanations</h2>
<section id="narrowing-over-using-as" class="level3">
<h3 class="anchored" data-anchor-id="narrowing-over-using-as">Narrowing over using <code>as</code></h3>
<p>Suppose you have a function that takes a number</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode ts code-with-copy"><code class="sourceCode typescript"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">double</span>(a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">number</span>) {</span>
<span id="cb1-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-3">}</span></code></pre></div></div>
<p>And you have a variable that could be a number or a string</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode ts code-with-copy"><code class="sourceCode typescript"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">getNumberOrString</span>()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">number</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">string</span> {</span>
<span id="cb2-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Math</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">random</span>() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">?</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"1"</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-3">}</span>
<span id="cb2-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">number</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">string</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">getNumberOrString</span>()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div></div>
<p>Typescript will allow you to use <code>as</code> to <em>assert</em> the variable to a number (this is one of the ways that TypeScript is not sound)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode ts code-with-copy"><code class="sourceCode typescript"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> result<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">number</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">double</span>(a <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">number</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div></div>
<p>But this not correct/sound at runtime!</p>
<p>The correct way to do this is to narrow the type with a check (if/else/early return).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode ts code-with-copy"><code class="sourceCode typescript"><span id="cb4-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> (<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">typeof</span> a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">===</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"number"</span>) {</span>
<span id="cb4-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">double</span>(a)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb4-3">}</span></code></pre></div></div>


</section>
</section>

 ]]></description>
  <category>typescript</category>
  <guid>https://theopomies.com/notes/typescript.html</guid>
  <pubDate>Fri, 14 Feb 2025 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
